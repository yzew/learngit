### 1.损失函数中用的什么评价指标？为什么这么选？

前置蒙版生成网络用的**二进制交叉熵**，因为蒙版图每个像素值为0或256，二进制交叉熵可以更好的解决二分类问题，来表达生成蒙版和真实蒙版分布的差异

监督学习网络采用**平均绝对误差MAE**，即L1损失来计算平均误差的大小，用L1损失而不是L2损失是为了减小生成图像的模糊

生成对抗网络中，判别器部分用的**均方误差MSE**，即**L2损失**来评价数据变化的程度，生成器同监督学习网络

### 2.商品图像分割没有开源数据集吗？为什么需要自建数据集？

针对商品图像的分割数据集较少，且存在前景过小、分割不精细等问题，因此本文参照很多图像分割论文扩大数据集的方法，针对部分类别的商品自建了小型数据集。

### 3.U2NET中为什么不都使用空洞卷积代替下采样？

因为空洞卷积会丢失边缘这种空间上的连续信息，而且没有下采样操作会增加计算量，而本文所用网络都是在个人笔记本上进行训练的，运算量受限。因此只在其中一部分用了空洞卷积来避免多次下采样造成的上下文信息丢失的问题。

### 4.是怎么通过输入额外的背景来代替三值图的？

用三值图作为先验信息的方法都是通过原图和三值图先在通道上进行拼接，再进行编解码操作。本文所用方法用背景图代替三值图，对原图、背景图、蒙版等同时进行特征提取后再对特征图进行有选择的拼接，更有效的结合输入信息的特征









## 一些疑问

### 生成对抗网络难训练

网络存在不收敛问题；网络不稳定；网络难训练；

举个例子，可能某一次训练的误差很小，在下一代训练时，又出现极限性的上升的很厉害，过几代又发现训练误差很小，震荡太严重

其次网络需要调才能出像样的结果，交替迭代次数的不同结果也不一样，比如每一次训练中，D网络训练两回，G网络训练一回，结果就不一样了。



### trimap图约束的网络

Deep image matting 输入是将原图和alpha图在通道上进行拼接，输入一个四通道图像。

然后进行卷积池化和上采样。最后经过卷积层构成的精细网络进一步优化。



### 你这个都同时提供背景了，不可以直接相减或者通过图像比对分割出前景吗

直接相减，一些部分的像素会变为0，背景为0的同时前景中很多像素也会变为0或负值，无法区分前景和背景。

图像比对，耗时太长，我做过一个两幅图像配准的程序，直方图，花了三十分钟才处理完一张图片。

像素比对，一样的不输出，不一样的输出。前景会不全。而且拍照的时候无法保证一样，我这个还结合了边缘信息，不仅仅依赖于背景



### 为什么不用传统方法？

传统方法也可以达到类似的效果。比如在grabcut的基础上，镂空不好的话，通过边缘检测算子，通过特征工程，找到一个合适的算子进行边缘检测之类的工作，进行形态学处理。但传统方法的话，你需要补充大量的相关知识，涉及各种复杂的计算和操作。

深度学习的出现，就是自动的帮你求出滤波器的值，学习一类知识可以解决多类问题，帮你简化这些人工计算和选择的过程。



### 生成对抗网络输入

和监督学习网络一样，粗糙的蒙版和I和B，然后输出好的F和不透明度



### 完全没用dropout???

要加的话也就加在残差模块里。因为层数很少就没加。

综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。





### RESBLKS？

增加解码网络深度，通过这些激活函数的组合变成更复杂的曲线，让他解码的更好。然后残差模块来避免网络深度增加出现的一些问题。



### 三值图和草图是如何作为约束解出那三个方程的？

圈定前景和背景的范围，然后预估和细分不确定部分。

通过图像内容和用户输入的先验信息来推测这三个未知量，具体都是原图和trimap在通道上就进行拼接



### 为什么背景图像能代替三值图作为先验信息？是如何发挥作用的？

背景图像是通过和原图像对比，来确定哪些属于背景，相当于在求解公式中提供了B，只需要求α了。

程序中是通过特征图的组合，经神经网络进行预测。



### 网络输入的M是如何发挥作用的？用处是什么？

M转换为灰度图，来忽略颜色信息，更加专注于运动信息的特征。主要用于视频，在图片中几乎没用



### **两层嵌套的U型结构**

为什么是两层？外面的U是一层，里面的RSU是一层，RSU里面每个还可以都换成RSU，变为三层。



### 损失函数图过拟合了？

train loss 不断下降，test loss趋于不变：说明网络过拟合;

#### 数据集太小

在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting）

这里参照原作者的数据，他默认是60epoch，数据集269加100个背景图片的扩大，2万七千张。类别几乎都是人像。我的，3255张，他的八分之一

#### Batch_Size过小

相对于正常数据集，如果**Batch_Size过小，训练数据就会非常难收敛，从而导致过拟合**。

增大Batch_Size,相对处理速度加快。增大Batch_Size,所需内存容量增加（epoch的次数需要增加以达到最好的结果）

batchsize=1，即每送入一个样本，执行一次梯度下降，称之为**SGD随机梯度下降法**，每次只操作一个实例。这种方法很适合在线学习的形式。但是单个样本的梯度随机性过强，**不易收敛到最优值**，且收敛速度过慢。

在线训练，就是每次只用一个样本来训练，可以理解为这个**mini-batch**的大小为1。

#### 其中的fg_c_loss为什么trainloss上升了？？？

占的比重较小，影响不大。和batch_size设的太小有关。也可能和数据集有关，对透明物体专训的训练中损失函数正常。

cd /d D:\BackgroundMatting\Background-Matting-master && tensorboard --logdir=./TB_Summary透明物体训练

#### 为什么trainloss比testloss高？？？

数据集问题。训练数据类别太多，各类别数据不均衡，挑选testloss是随机挑选的，没有每个类别按比例挑选。因此只是用来看大致的趋势了。。





### 基础概念看看

一层网络中，输入与权重相乘后再与偏置相加，经激活函数后输出

**反向传播**：通过损失函数计算损失，再通过计算损失函数对网络每一层的偏导数，对网络参数进行更新，来求得使损失函数取最小值的W和b

**卷积**：每个小区域的集合。卷积后得到图像的某些特征，如边缘特征。卷积生成特征图。和卷积核对应区域点乘然后求和。不同的卷积核参数可以过滤出不同的信息。随着卷积神经网络的训练，这些卷积核为了得到有用信息，在图像或feature map上的过滤工作会变得越来越好。这个过程是自动的，称作特征学习。特征学习自动适配新的任务：我们只需在新数据上训练一下自动找出新的过滤器就行了。



**池化**：下采样，比如最大值池化是在一个区域内选个数值最大的像素。对特征图进行压缩。就是减少参数、降维。通过池化增大感受野

**解码**：上采样，恢复图片大小，双线性插值上采样。双线性插值就是先计算出放大图像在原图像的位置，再在x轴和y轴根据线性关系分别计算其像素值

上下采样也可以通过空洞卷积代替，也可以增大感受野，但计算量也会相应增大。就是在卷积的时候，卷积核对应的区域更大，卷积核每个元素之间多了个间隔。



**BN层**：归一化，Batch Normalization，解决在训练过程中，中间层数据分布发生改变的问题，以防止梯度消失或爆炸、加快训练速度。是对每个通道进行归一化。具体是对每个通道求均值和方差，然后带入式子。



**感受野**

高分辨率尺度较小的感受野判别细节信息，分辨率低的尺度较大的感受野判别全局一致性

感受野就是一个卷积核能看到的信息。高分辨率里一个卷积核只能看到他覆盖的局部信息。下采样后虽然卷积核覆盖的面积不变，但看到的信息是浓缩之后的，感受野就更大了。

也可以通过空洞卷积，将卷积核看到的范围扩大，变相增大感受野。





*语义分割是像素级的分类问题。提到分类，就必须要有语义，必须要有大的感受野，这样你才能看到大的物体，而不是仅仅只能看到局部。那么提升感受野的方式，最直观最有效的就是下采样(编码)，而上采样(解码)只是为了把下采样的结果还原到原始的尺寸，现在主流网络大多是采用双线性插值的形式进行上采样。*

*降采样有以下几个目的:*

1. *降低显存和计算量，图小了占内存也就小了，运算量也少了。*
2. *增大感受野，使同样3*3的卷积能在更大的图像范围上进行特征提取。大感受野对分割任务很重要，小感受野是做不了多类分割的，而且分割出来的掩膜边界很粗糙！！*
3. *多出几条不同程度下采样的分支，可以很方便进行多尺度特征的融合。多级语义融合会让分类很准。*

*如果不需要考虑计算和显存，你当然可以不做下采样，用膨胀卷积也是可以增大感受野的。*



生成对抗网络不是无监督吗？输入？

输入就是无标签的，seg-gt对应输入的掩模图，没用。根据输入的粗糙掩模图(seg腐蚀膨胀)，生成器向原网络学习，减小他们之间的差距。





## 卷积不为整数时计算

例如输入的矩阵 H=W=5，卷积核的F=2，S=2，Padding=1。

经计算我们得到的N =（5 - 2 + 2*1）/ 2 +1 = 3.5 ，此时在Pytorch中是如何处理呢？

在卷积过程中会直接将最后一行以及最后一列给忽略掉，以保证N为整数，此时N = （5 - 2 + 2*1 - 1）/ 2 + 1 = 3







**梯度下降**

通过梯度下降，求损失函数最小值。梯度就是求导。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。

<img src="E:\MarkDown\picture\梯度下降.png" alt="梯度下降" style="zoom: 50%;" />