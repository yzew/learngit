HRNet.py
train_hrnet.py
用的是SE下的HRNet，最后报错IndexError: tuple index out of range，检查不出来



0.95794

0.97222 vit-base

0.9754 vit-base 最好模型

0.97937 简单跑了下vit_large_patch16_224

0.98254 完整训练vit_large_patch16_224.pth

0.98333 jx_vit_large_patch32_224_in21k-9046d2e7.pth，**在更大的in21k数据集上进行训练，且patch改为了32**

jx_vit_large_patch32_224_in21k-9046d2e7.pth

下一步：mae_pretrain_vit_large.pth，用MAE预训练下的vit_large。但结果很垃圾？？？
再下一步：mae_pretrain_vit_huge.pth，将ps改为1,且深度从32改回了2	4才能训练

再下一步：mae_finetuned_vit_huge.pth

再下一步：在自己数据上做MAE
VITAE

最后：SWA

/home/celia/wh//Image-Classification/validate.py

[ViTAE-Transformer-main](https://github.com/ViTAE-Transformer/ViTAE-Transformer/tree/main/Image-Classification#Usage)环境比较难装，不想折腾了





现在在试把bs调大，环境是VITPOSE，因为可以开fp16，bs可以设的更高

![image-20220901152421422](E:\MarkDown\picture\image-20220901152421422.png)

![image-20220901152356433](E:\MarkDown\picture\image-20220901152356433.png)

环境：pose_udp

1.将模型改为vit(默认是base)

备份下原来vit.py的参数

最新的：self, input_shape=[224, 224], patch_size=32, in_chans=3, num_classes=21843, num_features=1024,
            depth=24, num_heads=16, 

```python
# class VisionTransformer(nn.Module):
#     def __init__(
#             self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=768,
#             depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
#             norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
#         ):
#         super().__init__()
#         #-----------------------------------------------#
#         #   224, 224, 3 -> 196, 768
#         #-----------------------------------------------#
#         self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
#         num_patches         = (224 // patch_size) * (224 // patch_size)
#         self.num_features   = num_features
#         self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
#         self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

#         #--------------------------------------------------------------------------------------------------------------------#
#         #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
#         #
#         #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
#         #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
#         #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
#         #--------------------------------------------------------------------------------------------------------------------#
#         #   196, 768 -> 197, 768
#         self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
#         #--------------------------------------------------------------------------------------------------------------------#
#         #   为网络提取到的特征添加上位置信息。
#         #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
#         #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
#         #--------------------------------------------------------------------------------------------------------------------#
#         #   197, 768 -> 197, 768
#         self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
#         self.pos_drop       = nn.Dropout(p=drop_rate)

#         #-----------------------------------------------#
#         #   197, 768 -> 197, 768  12次
#         #-----------------------------------------------#
#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
#         self.blocks = nn.Sequential(
#             *[
#                 Block(
#                     dim         = num_features, 
#                     num_heads   = num_heads, 
#                     mlp_ratio   = mlp_ratio, 
#                     qkv_bias    = qkv_bias, 
#                     drop        = drop_rate,
#                     attn_drop   = attn_drop_rate, 
#                     drop_path   = dpr[i], 
#                     norm_layer  = norm_layer, 
#                     act_layer   = act_layer
#                 )for i in range(depth)
#             ]
#         )
#         self.norm = norm_layer(num_features)
#         self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()

#     def forward_features(self, x):
#         x = self.patch_embed(x)
#         cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
#         x = torch.cat((cls_token, x), dim=1)
        
#         cls_token_pe = self.pos_embed[:, 0:1, :]
#         img_token_pe = self.pos_embed[:, 1: , :]

#         img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
#         img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False)
#         img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
#         pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

#         x = self.pos_drop(x + pos_embed)
#         x = self.blocks(x)
#         x = self.norm(x)
#         return x[:, 0]

#     def forward(self, x):
#         x = self.forward_features(x)
#         x = self.head(x)
#         return x

#     def freeze_backbone(self):
#         backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
#         for module in backbone:
#             try:
#                 for param in module.parameters():
#                     param.requires_grad = False
#             except:
#                 module.requires_grad = False

#     def Unfreeze_backbone(self):
#         backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
#         for module in backbone:
#             try:
#                 for param in module.parameters():
#                     param.requires_grad = True
#             except:
#                 module.requires_grad = True

    
# def vit(input_shape=[224, 224], pretrained=False, num_classes=1000):
#     model = VisionTransformer(input_shape)
#     if pretrained:
#         model.load_state_dict(torch.load("model_data/vit-patch_16.pth"))

#     if num_classes!=1000:
#         model.head = nn.Linear(model.num_features, num_classes)
#     return model
```



2.改为vit_large_patch16_224

注意，模型在train.py的开头和vit.py的结尾两处都要改

![image-20220716201636754](E:\MarkDown\picture\image-20220716201636754.png)

预训练模型在这下载C:\Users\yzew\Desktop\deep-learning-for-image-processing-master\pytorch_classification\vision_transformer\vit_model.py

```python
class VisionTransformer(nn.Module):
    def __init__(
            self, input_shape=[224, 224], patch_size=16, in_chans=3, num_classes=1000, num_features=1024,
            depth=24, num_heads=16, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=GELU
        ):
        super().__init__()
        #-----------------------------------------------#
        #   224, 224, 3 -> 196, 768
        #-----------------------------------------------#
        self.patch_embed    = PatchEmbed(input_shape=input_shape, patch_size=patch_size, in_chans=in_chans, num_features=num_features)
        num_patches         = (224 // patch_size) * (224 // patch_size)
        self.num_features   = num_features
        self.new_feature_shape = [int(input_shape[0] // patch_size), int(input_shape[1] // patch_size)]
        self.old_feature_shape = [int(224 // patch_size), int(224 // patch_size)]

        #--------------------------------------------------------------------------------------------------------------------#
        #   classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。
        #
        #   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
        #   此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
        #   在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。
        #--------------------------------------------------------------------------------------------------------------------#
        #   196, 768 -> 197, 768
        self.cls_token      = nn.Parameter(torch.zeros(1, 1, num_features))
        #--------------------------------------------------------------------------------------------------------------------#
        #   为网络提取到的特征添加上位置信息。
        #   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
        #   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。
        #--------------------------------------------------------------------------------------------------------------------#
        #   197, 768 -> 197, 768
        self.pos_embed      = nn.Parameter(torch.zeros(1, num_patches + 1, num_features))
        self.pos_drop       = nn.Dropout(p=drop_rate)

        #-----------------------------------------------#
        #   197, 768 -> 197, 768  12次
        #-----------------------------------------------#
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.Sequential(
            *[
                Block(
                    dim         = num_features, 
                    num_heads   = num_heads, 
                    mlp_ratio   = mlp_ratio, 
                    qkv_bias    = qkv_bias, 
                    drop        = drop_rate,
                    attn_drop   = attn_drop_rate, 
                    drop_path   = dpr[i], 
                    norm_layer  = norm_layer, 
                    act_layer   = act_layer
                )for i in range(depth)
            ]
        )
        self.norm = norm_layer(num_features)
        self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        x = self.patch_embed(x)
        cls_token = self.cls_token.expand(x.shape[0], -1, -1) 
        x = torch.cat((cls_token, x), dim=1)
        
        cls_token_pe = self.pos_embed[:, 0:1, :]
        img_token_pe = self.pos_embed[:, 1: , :]

        img_token_pe = img_token_pe.view(1, *self.old_feature_shape, -1).permute(0, 3, 1, 2)
        img_token_pe = F.interpolate(img_token_pe, size=self.new_feature_shape, mode='bicubic', align_corners=False)
        img_token_pe = img_token_pe.permute(0, 2, 3, 1).flatten(1, 2)
        pos_embed = torch.cat([cls_token_pe, img_token_pe], dim=1)

        x = self.pos_drop(x + pos_embed)
        x = self.blocks(x)
        x = self.norm(x)
        return x[:, 0]

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def freeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = False
            except:
                module.requires_grad = False

    def Unfreeze_backbone(self):
        backbone = [self.patch_embed, self.cls_token, self.pos_embed, self.pos_drop, self.blocks[:8]]
        for module in backbone:
            try:
                for param in module.parameters():
                    param.requires_grad = True
            except:
                module.requires_grad = True

    
def vit(input_shape=[224, 224], pretrained=False, num_classes=1000):
    model = VisionTransformer(input_shape)
    if pretrained:
        model.load_state_dict(torch.load("model_data/vit_large_patch16_224.pth"))

    if num_classes!=1000:
        model.head = nn.Linear(model.num_features, num_classes)
    return model
```

2.2 vit_large_patch32_224_in21k

jx_vit_large_patch32_224_in21k-9046d2e7.pth，已下载

，更改了模型加载

[其他的下载地址，都没有huge的](https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-vitjx)

3,MAE

[预训练模型下载地址](https://github.com/facebookresearch/mae)

mae_pretrain_vit_large.pth，已下载
mae_pretrain_vit_huge.pth，已下载

![image-20220717165108894](E:\MarkDown\picture\image-20220717165108894.png)
![image-20220717165128328](E:\MarkDown\picture\image-20220717165128328.png)

报错是因为提供的预训练权重仅包括编码器权重，不包括解码器权重

见 [https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base_full.pth ](https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base_full.pth)。 其他两个文件 mae_pretrain_vit_large_full.pth 和 mae_pretrain_vit_huge_full.pth 也在同一个目录中。 请注意，这些模型会重建归一化像素。 

mae_pretrain_vit_base_full.pth，已下载
mae_pretrain_vit_large_full.pth，已下载
mae_pretrain_vit_huge_full.pth，已下载，未传输

可能也不行，因为这个MAE是预训练了编码器，这个解码器是恢复原图片的，在这里没用。我们需要的是MAE进行分类的model

因此又下载了MAE分类的Evaluation模型
mae_finetuned_vit_large.pth
mae_finetuned_vit_huge.pth



```python
OMP_NUM_THREADS=1 python -m torch.distributed.launch --nproc_per_node=8 main_finetune.py \
    --accum_iter 4 \
    --batch_size 32 \
    --model vit_large_patch16 \
    --finetune ${PRETRAIN_CHKPT} \
    --epochs 100 \
    --blr 5e-4 --layer_decay 0.65 \
    --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 \
    --dist_eval --data_path ${IMAGENET_DIR}
```



4.[ACM MM2021-还在用ViT的16x16 Patch分割方法吗？中科院自动化所提出Deformable Patch        ](https://www.bilibili.com/read/cv12643704/)







































## swa

[SWA Object Detection](https://arxiv.org/abs/2012.12645)

**cyclical learning rates**额外再训练模型12个epoch，然后平均每个epoch训练得到的weights作为最终的模型权重

初始学习率为0.02，结束学习率为0.0002，选择1个epoch作为循环长度。在每个循环中，学习率从一个较大的值lrmax开始，然后相对地降低到最小值lrmin。请注意，学习率在每次迭代时都会降低，而不是在每个epoch时，即在一次次迭代中从0.02降到0.0002

![image-20220716183639411](E:\MarkDown\picture\image-20220716183639411.png)

## CyclicLR周期性学习率

> **torch.optim.lr_scheduler.CyclicLR**

该策略以恒定的频率在两个边界之间循环学习速率

![image-20220716182828327](E:\MarkDown\picture\image-20220716182828327.png)

循环学习率策略改变每批的学习率后。在一个batch被用于train之前，应该先调用*s**t**e**p*

方法。

此类内置了三种策略，正如论文中提出的那样：

* **triangular**：没有振幅缩放的基本三角形周期。
* **triangular2**：一种基本的三角形周期，每个周期将初始振幅放大一半。
* **exp_range**：在每个周期迭代中，通过gamma^cycleiterations来周期性缩放初始振幅。

**参数：**

* **optimizer（[\*Optimizer\*](https://pytorch.org/docs/1.6.0/optim.html#torch.optim.Optimizer)）：** 优化器；
* **base_lr（float or list）**：初始学习率，即每个参数组在循环中的下界。
* **max_lr（float or list）**：每个参数组在循环中的学习速率上限。在功能上，它定义了周期振幅（max_lr-base_lr）。任何周期的lr都是base_lr和缩放振幅的总和；因此，根据缩放函数scaling，实际上可能无法达到max_lr。
* **step_size_up** (int)：在学习率增加的半个周期中的训练迭代次数。默认值：2000.
* **step_size_down** (int)：在学习率增加的半个周期中的训练迭代次数。默认值：None，为None时默认与**step_size_up**一致.
* **mode（str）**： {triangular, triangular2, exp_range} 可选，默认triangular。值与上面详述的策略相对应。如果scale_fn不是None，则忽略此参数.
* **gamma** (float)：默认1.0。“exp_range”缩放函数中的常量：*g**a**m**m**a**c**y**c**l**e* *i**t**e**r**a**t**i**o**n**s**
* **scale_fn**：缩放函数。
* **scale_mode（str）**：{‘cycle’, ‘iterations’}可选。定义是根据循环数还是循环迭代来计算**scale_fn**。默认‘cycle’.
* **cycle_momentum** (bool)：如果为真，则动量循环在“基本动量”和“最大动量”倒数之间。默认值：True.
* **base_momentum（float or list）**：基本动量.
* **max_momentum（float or list）**：最大动量.
* **last_epoch（int）**：最后一个epoch的索引，默认为-1.
* **verbose（bool）**：默认为False；如果为True，每次更新打印一条标准输出信息.

用法示例：

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)
data_loader = torch.utils.data.DataLoader(...)
for epoch in range(10):
    for batch in data_loader:
        train_batch(...)
        scheduler.step()
```



我们这里使用swa的方法，在每个epoch中，学习率从0.02降低到0.0002，因此step_size_up设置为1，即在一次迭代内上升到0.2，step_size_down设置为len(train_lines) / batch_size - 1，即在之后的迭代内慢慢下降到0.0002

```python
for epoch in range(Init_Epoch, UnFreeze_Epoch):
    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0002, max_lr=0.02,step_size_up= 1,step_size_down = 291)
    for iteration, batch in enumerate(gen):
        optimizer.step()
        scheduler.step()  # scheduler.step()的更新要放在每个迭代中
```

具体使用见train_swa.py和aa.py（模型融合



==但pytorch1.6已经集成swa了==

```python
from torch.optim.swa_utils import AveragedModel, SWALR
from torch.optim.lr_scheduler import CosineAnnealingLR

# scheduler = CosineAnnealingLR(optimizer, T_max=100) # 使用学习率策略（余弦退火）


# 采用SGD优化器
optimizer = torch.optim.SGD(model.parameters(),lr=1e-4, weight_decay=1e-3, momentum=0.9)
# 随机权重平均SWA,实现更好的泛化
swa_model = AveragedModel(model).to(device)
# SWA调整学习率
swa_scheduler = SWALR(optimizer, swa_lr=1e-6) # 当SWA开始的时候，使用的学习率策略
for epoch in range(1, epoch + 1):
    for batch_idx, (data, target) in enumerate(train_loader):   
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        # 在反向传播前要手动将梯度清零
        optimizer.zero_grad()
        output = model(data)
        #计算losss
        loss = train_criterion(output, targets)
        # 反向传播求解梯度
        loss.backward()
        optimizer.step()
        lr = optimizer.state_dict()['param_groups'][0]['lr']   
    swa_model.update_parameters(model)
    swa_scheduler.step()
# 最后更新BN层参数
torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)
# 保存结果
torch.save(swa_model.state_dict(), "last.pt")

```







optimizer.step()和scheduler.step()的区别

optimizer是指定**使用哪个优化器**，scheduler是**对优化器的学习率进行调整**，正常情况下训练的步骤越大，学习率应该变得越小。optimizer.step()通常用在每个mini-batch之中，而scheduler.step()通常用在epoch里面,但是不绝对。可以根据具体的需求来做。只有用了optimizer.step()，模型才会更新，而scheduler.step()是对lr进行调整。通常我们在scheduler的step_size表示scheduler.step()每调用step_size次，对应的学习率就会按照策略调整一次。所以如果scheduler.step()是放在mini-batch里面，那么step_size指的是经过这么多次迭代，学习率改变一次。







## **Multi-Sample Dropout**

使用连续的dropout，可以加快模型收敛，增加泛化能力，详细见论文，代码实现如下：

```python
dropouts = nn.ModuleList([
    nn.Dropout(0.5) for _ in range(5)
])

for j, dropout in enumerate(dropouts):
    if j == 0:
        logit = self.fc(dropout(h))
    else:
        logit += self.fc(dropout(h))
```































![image-20220313191307935](E:\MarkDown\picture\image-20220313191307935.png)



语义分割一些顶会改进




### 1.resize方法

常规：固定的图像调整器，如双线性插值、最近邻等

提出方法：可学习的图像调整器

经典的调整器通常会具备更好的小图像感知质量（即对人类识别图片更加友好），本文提出的可学习调整器不一定会具备更好的视觉质量，但能够提高CV任务的性能。

在不同的任务中，可学习的图像调整器与baseline视觉模型进行联合训练。这种**可学习的基于cnn的调整器创建了机器友好的视觉操作，因此在不同的视觉任务中表现出了更好的性能** 。作者使用ImageNet数据集来进行分类任务，实验中使用四种不同的baseline模型来学习不同的调整器，相比于baseline模型，使用本文提出的可学习调整器能够获得更高的性能提升。



https://arxiv.org/abs/2103.09950



### 2.把一个图片分成多个patch，进行处理 然后在合并每部分的结果。

https://www.zhihu.com/question/453750066







# 改进思路

- 添加attention 机制
  添加inception

  目前主要有四种normalization，是不是自己也可以增加一种normalization

  可以在自己设计的网络模型上再添加迁移学习

  很多论文在经典的网络上修改网络卷积核的大小，然后添加跳跃连接之类的，作为创新点。

  自己的网络结构创新点1：好几个Block放在一起作为自己的一个Scale（自己命名的）。

  VGG使用小卷积核（3*3）代替大的卷积核，减少参数量和进行更多的非线性映射，以增加网络的拟合/表达能力。
  VGG使用小池化核（2*2）

  可使用膨胀卷积+残差网络进行结合+感知损失，可将普通的卷积变成膨胀卷积

  自己的网络结构和各种其他经典的网络结构。经典的网络结构也可以看是否添加某项参数（正则项，激活函数之类的），进行比较，比较的多可行度更高。
  在添加跳跃链接的时候还可以加上另一个功能会使得效果更好。

  可以扩展网络的对目标的识别，横向对接多个卷积核，之后再进行相加。横向添加卷积核，之后再添加多个经典的残差模块1

  使用极限学习机代替BP神经网络，也是一个创新点。如果人家是最大池化，我们可以使用最大池化和均值池化相结合的方法，增大局部感受野

  可以修改残差快，比如在残差快的跳跃连接上接入BN、激活、权重之类的。

  可以在残差快里面再用一个自己的模块，或者套用一个模块

  偶数卷积和基数卷积
  
- 将残差网络改为 深度残差收缩网络

- 搜索该方向最新比较有代表性的综述性论文，对该方向的来龙去脉以及最新研究有一定清晰的认知；

  找到该领域比较经典和最新的研究，研读paper；「以医学图像语义分割为例，FCN和UNet是必读的两篇经典文献；其次，在这方面比较经典的应用有Deeplab系列，PSPNet，Segnet等；最新的研究进展：（1）结合注意力机制～SE～GC～Dual attention～Cross attention～...；（2）引入多尺度～ASPP～PSP～Inception～...；（3）应用GAN去解决样本量不足的问题；（4）往弱监督或者NAS方向搞；（5）最简单的一种：设计组装多种不同的模块，比如来个串行或并行分支，搞个不同的空洞率组合，加法或乘法操作混合搭配等等」；

  接下来就是从github上找源码「以第3点为例。先从简单的UNet入手，选定一个框架，发paper的话强烈建议Pytorch。」

  了解框架的使用「以Pytorch为例，比如数据部分是如何载入（定义一个dataset类，里面通常包含三个方法：getitem-用于迭代的获取数据；len-用于获取数据集的长度；init-定义数据路径及一些配置参数等）；模型是如何定义（一般是写成一个类，继承nn.Module），init里面定义操作，forward里面进行调用，训练过程如何定义（首先用DataLoader加载数据，然后定义损失函数，优化算法，学习策略，设备等，需要格外注意的是cpu和gpu之间的传送以及数据读入的格式是否正确）。如果用tf的话建议使用tf2.0.里面更新了许多有用的api，书写的方式也越发趋向于torch，重点是与keras进行了无缝对接，记住用tf.keras」

  框架熟悉之后便可以尝试定义模型，以UNet为例，便可以在此基础上添加许多模块，把前面看过的paper的idea结合到网络中来，不断的跑实验，边跑边总结我为什么要加这个模块，加这个模块有什么意义？「比如我需要引入**注意力机制**，因为它可以使网络更加注重轮廓的学习。比如你分割的目标比较散落，尺寸有大有小，这种情况就可以借鉴ASPP的机制，引入多尺度来捕获尺寸大小的目标。再比如她想要效果达到实时性，那可以优先尝试深度可分离卷积，把通道数调小一倍，结合更新的方法」

  在这个过程中掌握调参的技巧，熟练运用框架，比如能复现新的损失函数；尝试下转换连接方式，融入传统视觉的方法到网络中，实现完全的端到端的训练等等。如果gpu不支持的话可以在经济条件允许的情况下去平台租服务器运行，不过长久来说非常不划算。

  做实验的一点心得就是：我加这个模块的意义是什么？一定要思考清楚，不然后期真的跑到有效果也没用，毕竟学术圈讲究的是你的idea。「我能说unet在许多的分割类比赛都能拿第一吗？」







[和deeplab区别](https://www.bilibili.com/video/av711466772/)

deeplab(或者说为什么选deeplab而不是别的网络)

之前的语义分割网络网络中，分割结果往往比较粗糙，一是因为池化导致丢失信息，二是没有利用标签之间的概率关系。

deeplab首先使用空洞卷积，避免池化带来的信息损失，然后使用CRF条件随机场，进一步优化分割精度。在v3中将空洞卷积应用在了级联模块，并取消了CRF

深度学习方法 Mask R-CNN实例分割的代表性网络，基于faster-RCNN改进

mask rcnn是集目标检测，分类，分割于一体的网络，他的分割是将已经通过box框出的目标进行分割，物体单一，背景不复杂。现代的实例分割方法主要是**先检测对象边界框，然后进行裁剪和分割**，Mask R-CNN 推广了这种方法。他使用残差网络作为卷积结构；增加FCN用于语义分割(针对于目标框里的图像进行FCN的操作)deeplabV3是专门针对整张图像做分割得网络，其中有多尺度连接，网络结构复杂。所以对于deeplabV3没有必要加入到mask rcnn里面去，加进去了也提高不了多少性能，反而会使mask这个网络变得复杂。





Automatic Object Segmentation Based on GrabCut

Innovative classification of dolphins using deep neural networks and GrabCut这个是根据mask来grabcut

图像处理方法 GrabCut

基于目标轮廓增强的GrabCut图像分割方法

grabcut image segmentation method based on target edge enhancement一个自动框取的方法

[自动Grabcut](http://www.doc88.com/p-80087076332944.html)

<img src="E:\MarkDown\picture\image-20210411130559907.png" alt="image-20210411130559907" style="zoom:50%;" />

[提升deeplabv3性能](https://zhuanlan.zhihu.com/p/98507264)







[Matlab 手动图像配准](https://blog.csdn.net/qq_37653019/article/details/108776572?ops_request_misc=&request_id=&biz_id=102&utm_term=%E5%88%B6%E4%BD%9C%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E6%89%8B%E5%8A%A8%E5%9B%BE%E5%83%8F%E9%85%8D%E5%87%86%E5%B7%A5%E5%85%B7&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-108776572.pc_search_result_no_baidu_js&spm=1018.2226.3001.4187)







先用duiqi.py对齐，然后再输出mask和matte，然后将输出融合后抠图

问题1：部分图**对齐的不好**，改进对齐程序。想办法让灰度图对齐

对齐方法如下：

1 使用OpenCV对两幅图像进行角点检测

2 求取两幅图像的角点平均值

3 比较两幅图像的角点平均值获得偏移值

4 根据偏移值矫正图像



问题2：将哪个模型的融合？

[判断图像是否相似](https://www.jb51.net/article/83315.htm)

adobe和自训练的不错

输入图像自动输出自训练的合成和自训练和adobe的合成，然后**写个判定**，就是fg和bg（因为是分割的棕色背景，所以bg用蓝色和蓝色图比以此判断合成的图像是否有过多的背景干扰）组合后判断与原图差距？

4.trainset效果

黄色背景下黄色胶带效果不好，卫生纸效果一般，黄色干脆面不行，黄色木支架不行，黄色饮料一般

换成紫色后，胶带镂空不行，姜慧杯子镂空处理的不行，别的都不错了











![image-20220902100145327](E:\MarkDown\picture\image-20220902100145327.png)

![image-20220902100237457](E:\MarkDown\picture\image-20220902100237457.png)



![image-20220902100424601](E:\MarkDown\picture\image-20220902100424601.png)

![image-20220902100442460](E:\MarkDown\picture\image-20220902100442460.png)

![image-20220902100501079](E:\MarkDown\picture\image-20220902100501079.png)

![image-20220902100521697](E:\MarkDown\picture\image-20220902100521697.png)

![image-20220902100546857](E:\MarkDown\picture\image-20220902100546857.png)



![image-20220902100635738](E:\MarkDown\picture\image-20220902100635738.png)

































