**Ultrasound Nerve Segmentation数据集（师兄**

DRIVE数据集（眼底血管



##  1、Deep-Learning-master程序

C:\Users\yzew\Desktop\Deep-Learning-master

笔记本上程序为未改动的，训练医药数据集成功和公测数据集失败的代码

che服务器上为改动后重新训练公测数据集的代码



医药数据集上表现较好

参数：通道数和类别数都设为了1

![image-20211007103025127](E:\MarkDown\picture\image-20211007103025127.png)



公测数据集上表现差

参数：通道数和类别数都设为了1

![image-20211007103328563](E:\MarkDown\picture\image-20211007103328563.png)

尝试：将class改为2？不应该是1+1么，在che服务器上试跑。|
改为2后报错
ValueError: Target size (torch.Size([8, 1, 420, 580])) must be the same as input size (torch.Size([8, 2, 420, 580]))

**尝试2**：试跑三通道的voc数据集



## 2、unet-pytorch-main

C:\Users\yzew\Desktop\unet-pytorch-main

在服务器上训练，在笔记本上预测

预测时只需要改动predict.py中的img = "ADE_train_00015308.jpg"、unet.py中的"num_classes"    : 21,及 "model_path" 

训练时改动train.py中的num_class以及数据集位置、txt等

医药数据集

不能批量预测，单独测试图片后发现效果一般

![image-20211007104139669](E:\MarkDown\picture\image-20211007104139669.png)



voc数据集

效果一般

![image-20211007104235173](E:\MarkDown\picture\image-20211007104235173.png)

![image-20211007104214692](E:\MarkDown\picture\image-20211007104214692.png)



![image-20211007104252500](E:\MarkDown\picture\image-20211007104252500.png)



试跑公测数据集：

训练时改动train.py中的num_class以及数据集位置、txt等



华为数据集：

train.py 训练,num_class为26

![image-20211007152937068](E:\MarkDown\picture\image-20211007152937068.png)

train_medical.py训练，class为26

训练几个epoch后，loss就小到接近0了

![image-20211007152955629](E:\MarkDown\picture\image-20211007152955629.png)

尝试继续用train_medical.py训练，然后把医药数据集的特殊处理去掉

![image-20211007153529184](E:\MarkDown\picture\image-20211007153529184.png)



unet.py

```python
# 32行，blend控制是否让识别结果和原图混合，默认true，改一下。

# 63行，设置的标签颜色。需要和生成的标签对应把？
elif self.num_classes <= 26:
            self.colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128), (0, 128, 128), 
                    (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0), (64, 0, 128), (192, 0, 128), 
                    (64, 128, 128), (192, 128, 128), (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128), (128, 64, 12),(46,139,87),(255,255,240),(210,105,30),(0,255,255),(0,0,139)]


#---------------------129行--------------------------#
        #   创建一副新图，并根据每个像素点的种类赋予颜色
        #------------------------------------------------#
        seg_img = np.zeros((np.shape(pr)[0],np.shape(pr)[1],3))
        for c in range(self.num_classes):
            seg_img[:,:,0] += ((pr[:,: ] == c )*( self.colors[c][0] )).astype('uint8')
            seg_img[:,:,1] += ((pr[:,: ] == c )*( self.colors[c][1] )).astype('uint8')
            seg_img[:,:,2] += ((pr[:,: ] == c )*( self.colors[c][2] )).astype('uint8')
```



blend改为false。

![image-20211007161143130](E:\MarkDown\picture\image-20211007161143130.png)

![image-20211007161307602](E:\MarkDown\picture\image-20211007161307602.png)



重新下载程序并运行后：（在Download中预测，在che 中训练）

![image-20211007182603524](E:\MarkDown\picture\image-20211007182603524.png)



![image-20211007182659301](E:\MarkDown\picture\image-20211007182659301.png)

将unet.py中的color修改为原来未改动的后，进行训练，输出红色的图片。





选用的loss函数有问题，目前使用的loss函数是binary_corssentropy，这种loss函数将正样本和负样本均拿来计算loss值

解决方案方案是：选用DSC loss函数；这种loss（直观理解）约束的是正样本的准确率。



2、学习率调大调小



改为二值分割训练，并将输出结果图片中除了黑色的部分都显示为白色，仍为全黑

请教师兄:

模型不收敛，因为即使将多分类转换为二分类后，其标签的统一性也比较差，各个标签差别较大，且数据量变为30张图片更小了，因此训练难度仍然较大
尝试：

1、筛选出像素点够大的标签进行训练

2、分为正面、背面、侧面分别进行训练(数据集更少了)

![epoch_loss_2021_10_11_14_26_53](E:\MarkDown\picture\epoch_loss_2021_10_11_14_26_53.png)



1、labelme查看标签对应情况

2、数据集选取正面、裁切

3、预测程序



## 3、师姐程序

1、关于报错的weight

通过nn.CrossEntropyLoss()的weight来减轻样本不平衡问题

如分割实验，label标注的0-3四类，0类的比重过大，1类其次，2，3类都很少，则可以设置（0.1、  0.8、 1、 1 ）

应用：比如这里0类背景类比重过大，可以尝试设为(0.5,1)?



疑问，二分类的话输出层不应该就是只有一个通道了？多分类就需要有 class_num(类别数, 包括背景) 个通道,











全部图每次训练，loss均有差异，有时1.28左右，有时1.39左右，有时1.25左右，1.31



![image-20211014143753988](E:\MarkDown\picture\image-20211014143753988.png)



可以在预处理上添加相应的数据增强（翻转，剪切，旋转等），如数据量较本身较少，可减少网络通道数。另外，目标区域相对较小，可以尝试focal loss。



![img](E:\MarkDown\picture\R9H@TR9[G}@N3AQ9(])NE.png)

















































任务

1、medical没有损失函数之类的指标图，补上

**1、改损失函数**，验证效果，好像是有效的，在predict26中更为完整，参数（0.5，1.5）

**2、改预测为黑白**

3、改程序为多GPU，再测试效果（多GPU因为bn会在单个gpu运算等原因，可能会对结果造成一定影响）

**4、裁切图片再训练**













1、基于vgg16预训练模型下，从头开始训练

将标签图改为P模式后，将学习率调低，设为1e-6后，训练正面图片开始有效果了，此时loss=1.3

bs2，predect20，仅有几个权值文件的输出是有效果的

![image-20211020180450615](E:\MarkDown\picture\image-20211020180450615.png)

![image-20211020180505984](E:\MarkDown\picture\image-20211020180505984.png)

2、将bs逐渐增大，到6时，训练全部图片开始有效果了

Epoch15-Total_Loss1.2785

![image-20211020180634998](E:\MarkDown\picture\image-20211020180634998.png)

![image-20211020180646791](E:\MarkDown\picture\image-20211020180646791.png)



3、对图像进行裁剪后，输出全红。

之前针对全部图片和正面图片得到结果，是在学习率设为1e-6情况下的，这时损失函数都是在epoch为1时的loss的基础上略微有下降。

再次同参数下进行训练来复现时，发现每次开始的loss均有一些差异，且复现不出原来的结果

如对筛选出的正面图片进行训练时，bs设为4，loss范围为1.3472-1.32 第二次  1.1983-1.1632，
batch_size改为了2，loss1.2793-0.5，第二次loss1.2885-0.5，第三次1.24-0.5

猜测是学习率1e-6太低，有时没有学习到。



4、之前程序是仅加载vgg16的预训练模型。之后再载入在voc数据集上训练得到的权值文件基础上进行训练。

bs=2，学习率1e-6，loss:从1.2降到0.46

![image-20211021131544245](E:\MarkDown\picture\image-20211021131544245.png)



bs改为4 loss从5.3降到0.668

![image-20211021132136248](E:\MarkDown\picture\image-20211021132136248.png)

![image-20211021135422942](E:\MarkDown\picture\image-20211021135422942.png)

bs=6 loss 1.15-0.44

![image-20211021135200720](E:\MarkDown\picture\image-20211021135200720.png)

![image-20211021135144040](E:\MarkDown\picture\image-20211021135144040.png)





























































































3、对损失函数进行修改，

```c++
# 备份原来的，虽然能正常运行但是应该是不对的。可能用了广播机制？
class CrossEntropyLoss2d(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(CrossEntropyLoss2d, self).__init__()
        self.nll_loss = nn.NLLLoss2d(weight, size_average)

    def forward(self, inputs, targets):
        return self.nll_loss(F.log_softmax(inputs), targets)
weight=torch.from_numpy(np.array([0.5, 1.5])).float()
# weight = torch.FloatTensor([0.3,1])
criterion = CrossEntropyLoss2d(weight.cuda())
            

# 备份
def CE_Loss(inputs, target, num_classes=21):
    n, c, h, w = inputs.size()
    nt, ht, wt = target.size()
    if h != ht and w != wt:
        inputs = F.interpolate(inputs, size=(ht, wt), mode="bilinear", align_corners=True)

    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)
    temp_target = target.view(-1)

    CE_loss  = nn.NLLLoss(ignore_index=num_classes)(F.log_softmax(temp_inputs, dim = -1), temp_target)
    return CE_loss
            
            
# 改法一、直接把weight加进去就行，md
def CE_Loss(inputs, target, num_classes=21):
    n, c, h, w = inputs.size()  # (8,2,512,512)
    nt, ht, wt = target.size()  # (8,512,512)
    if h != ht and w != wt:
        inputs = F.interpolate(inputs, size=(ht, wt), mode="bilinear", align_corners=True)

    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)
    temp_target = target.view(-1)

    CE_loss  = nn.NLLLoss(weight=torch.from_numpy(np.array([0.5, 1.5])).float(),ignore_index=num_classes)(F.log_softmax(temp_inputs, dim = -1), temp_target)
    return CE_loss

            
# 改法二、按师姐程序写法改的，但其实没必要
class CrossEntropyLoss2d(nn.Module):
    # weight：可选的，应该是一个tensor，里面的值对应类别的权重，如果样本不均衡的话，这个参数非常有用，长度是类别数目
    # szie_average：默认是True，会将mini-batch的loss求平均值；否则就是把loss累加起来
    def __init__(self, weight=None, size_average=True):
        super(CrossEntropyLoss2d, self).__init__()
        self.nll_loss = nn.NLLLoss(weight, size_average,ignore_index=2)
 
    def forward(self, inputs, targets):
        n, c, h, w = inputs.size()  # (8,2,512,512)
        nt, ht, wt = targets.size()  # (8,512,512)
        if h != ht and w != wt:
            inputs = F.interpolate(inputs, size=(ht, wt), mode="bilinear", align_corners=True)

        temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)
        temp_target = targets.view(-1)

        return self.nll_loss(F.log_softmax(temp_inputs, dim = -1), temp_target)

weight=torch.from_numpy(np.array([0.5, 1.5])).float()
# weight = torch.FloatTensor([0.3,1])
criterion = CrossEntropyLoss2d(weight.cuda())
```





















新下载程序newnew 

1.只改了学习率为1e-6，bs默认为2

正面图：epoch为16、20时的模型有结果，之后的为纯红

全部图：无结果

2.将bs改为1

全部图：无效果

正面图：无效果

3.bs改为4

正面图：可

全部图：无效果

改Loss后，全部图仍无效果

4.bs改为6

全部图：开始有效果了

改Loss后：epoch为20时全黑，为40时全白，但epoch为26等时，存在输出



[batchsize影响](https://zhuanlan.zhihu.com/p/86529347)

batch size过小，花费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。





原程序unet跑苹果数据集，放voc文件夹下训练

![image-20211022134622585](E:\MarkDown\picture\image-20211022134622585.png)







注意：在config.py里改学习率是没用的，要在useful.py里更改

**1、补充预测程序**

在医药数据集上训练并预测，检验预测程序是否有问题

学习的并不好。因为数据集只有不到30张，且FCN网络没有加载预训练权重，也没有怎么调参，只是先用小数据集看下自写的预测程序是否能正常使用

if epoch <5: return 0.0001  if epoch >= 5: return 0.00001

<img src="E:\MarkDown\picture\image-20211021224819441.png" alt="image-20211021224819441" style="zoom:50%;" />

**2、添加val部分**

**3、增加显示loss图的程序**

**4、测试默认网络FCN效果**

标签图为P模式时全黑。改为L模式试试？

**1）全部图片训练，标签图为L模式**，效果较差

![image-20211022105531507](E:\MarkDown\picture\image-20211022105531507.png)

**2）裁剪的正面图片，L模式**

没预测出什么东西，相关性还是不强

左图为label，右图为预测结果

![image-20211022143431978](E:\MarkDown\picture\image-20211022143431978.png)

![image-20211022143516707](E:\MarkDown\picture\image-20211022143516707.png)

**3）裁剪出苹果的图片进行训练**，L模式

对于这种简单的图片，简单调参后的预测结果还可以

分别为原图、Label、预测图

![image-20211022122838255](E:\MarkDown\picture\image-20211022122838255.png)



**5、验证loss增加weight后的效果**

在FCN训练裁剪后正面图上测试。同参数仅loss增加了weight，[0.3, 1.0]

左上：原图                                 右上：标签图
左下：增加weight前的预测图  右下：增加weight后的预测图

可以看到，预测图和标签图相似度还是较差，增加weight后仅是更规整了些

![image-20211022224733253](E:\MarkDown\picture\image-20211022224733253.png)



**7、改为UNet**

损失函数使用BCEloss
73行和109行改了loss = criterion(logits,mask.long())

改为Unet后

![image-20211023143251582](E:\MarkDown\picture\image-20211023143251582.png)

net = Unet256(in_shape=(3,image_size,image_size),num_class=1)中num_class的设置对网络没有影响，网络输出通道数是固定的1，因此相应的也需要对预测函数进行更改，用医药数据集再次进行测试，结果仍较差

其他如FCN和另一个程序的Unet，都是输出通道数为2，改一下这里unet的通道数，用医药数据集测试如下，lr=0.0001

![image-20211023160154212](E:\MarkDown\picture\image-20211023160154212.png)

再对**正面裁剪图**进行训练看看效果

1e-4

![image-20211025151732771](E:\MarkDown\picture\image-20211025151732771.png)

1e-5

![image-20211025151759906](E:\MarkDown\picture\image-20211025151759906.png)

1e-6

![image-20211025151830564](E:\MarkDown\picture\image-20211025151830564.png)

对**全部图片**进行训练，左预测，右标签

![image-20211025155547325](E:\MarkDown\picture\image-20211025155547325.png)

损失函数修改，还是CELoss
没差别

![image-20211025165912817](E:\MarkDown\picture\image-20211025165912817.png)

增加weight

![image-20211025192302540](E:\MarkDown\picture\image-20211025192302540.png)

![image-20211025192247108](E:\MarkDown\picture\image-20211025192247108.png)



感觉是因为各种缺陷相关性太差导致学不到有用信息，因此对相似类型的缺陷进行训练看看效果

![image-20211025145448036](E:\MarkDown\picture\image-20211025145448036.png)

![image-20211025145414167](E:\MarkDown\picture\image-20211025145414167.png)



6、李师兄提供思路：既然训练苹果标的图(尺寸较小)没问题，但训练2000*2000的原图效果差，可能是因为载入图片缩放到512 *512，导致白色标签部分更小了，影响训练效果，因此可以将2000 *2000的图片裁剪成多份512的patch分别进行训练

将每个2000的图片裁切为四个1024的图片进行训练



1024，bs=1，lr=0.000001

全黑无结果

1024，bs=2，lr=0.00001

























8、没有数据增强？？起码要加上翻转吧，其他操作如旋转或随机裁剪（注意若需要填充像素时，加入的像素是否需要在算损失时ignore）、扭曲等酌情加入

9、更改损失函数为diceloss等试试效果（dice对解决样本不均衡问题可能效果较好，但会导致训练不稳定）



8、增加保存最好模型的部分

9、找有没有预训练权重，加上















样本不均衡解决方法：

![image-20211022221312977](E:\MarkDown\picture\image-20211022221312977.png)

![image-20211022221438210](E:\MarkDown\picture\image-20211022221438210.png)























che

logits 2通道





























