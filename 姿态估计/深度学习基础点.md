b c h w -> b c (h w)

mean( KLDivLoss( log(softmax(out)), labels ) )

![image-20230520214412048](E:\MarkDown\picture\image-20230520214412048.png)

## pytorch

print(x.shape)打印形状

[tuple转tensor](https://blog.csdn.net/qq_41204464/article/details/129331275)

## numpy基础语法

### 1.

X[:,0]就是取矩阵X的所有行的第0列的元素，X[:,1] 就是取所有行的第1列的元素。
X[:,  m:n]即取矩阵X的所有行中的的第m到n-1列数据，含左不含右。
X[0,:]就是取矩阵X的第0行的所有元素，X[1,:]取矩阵X的第一行的所有元素。



































训练技巧

***



[【CV算法内功】系列之PyTorch框架性能提升大作战|先行篇](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247483983&idx=1&sn=6dc395e4b61c218f8662cb773c5b50b1&chksm=cfb4dac0f8c353d6e98f3072867fe00bf276e844d692a4e1a669beda81693140cfdfe1b8dd55&scene=21#wechat_redirect)

[补充篇](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247484105&idx=1&sn=fd873feb7d79de4c335aad62a4ee9813&chksm=cfb4da46f8c35350d491a9763f594ac39de9ac218ac2533891c3b88cec0ea80deb2b9f754874&token=82376451&lang=zh_CN#rd)

[算法岗面试](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247483789&idx=1&sn=65aead083019cf0f000249c830f3282d&chksm=cfb4d902f8c3501472293df837981a483188304bc31bd7ea828500acd6095dccb489119b9fc1&scene=21#wechat_redirect)



## labelme

https://blog.csdn.net/yangshuai66666666/article/details/112203552

https://zhuanlan.zhihu.com/p/371756150



## 算法落地/部署

即推理Inference

[AI模型部署](https://cloud.tencent.com/developer/article/1841046)



## backbone

基础网络结构
backbone主干网络，指提取特征的网络，作用就是提取图片中的信息，供后面的网络使用，常用的有resnet,VGG等，这些网络已经证明了在分类等问题上的特征提取能力是很强的。这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接自己的网络，让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。
![在这里插入图片描述](E:\MarkDown\picture\20210328014541403.png)
第一行是按着如何使网络结构更深的发展思路来进行推进的。
第二行是在玩 module 的概念，当然我们看到第一行和第二行后面是被结合到一起了的。
第三行是按着如何使网络更快，结构更轻量化来进行发展的。这里仅仅只是列出了一些比较经典的网络，而且每种网络都有多个版本。下图是轻量化网络更详细的发展情况。
![在这里插入图片描述](E:\MarkDown\picture\2021032801461123.png)







## Embedding编码和one-hot编码

embedding实质上就是将一个个patch，通过和一个类似卷积核的东西相乘，将patch变换为一个个向量，是**向量映射**

如用RGB三个维度可以表示一种颜色，RGB这个三维向量就是Embedding，颜色就是实体，每个颜色有唯一的RGB值，就好比每个实体有自己的Embedding。



> Embedding 这样一种**将离散变量转变为连续向量**的方式为神经网络在各方面的应用带来了极大的扩展。该技术目前主要有两种应用，NLP 中常用的 word embedding 以及用于类别数据的 entity embedding。在神经网络中，embedding 是非常有用的，因为它不光可以减少离散变量的空间维数，同时还可以有意义的表示该变量。

实质：将离散变量转变为连续向量；减少离散变量的空间维数



embedding 有以下 3 个主要目的：

1. 在 embedding 空间中查找最近邻，这可以很好的用于根据用户的兴趣来进行推荐。
2. 作为监督性学习任务的输入。
3. 用于可视化不同离散变量之间的关系。

要了解 embedding 的优点，我们可以对应 One-hot 编码来观察。One-hot  编码是一种最普通常见的表示离散数据的表示，首先我们计算出需要表示的离散或类别变量的总个数 N，然后对于每个变量，我们就可以用 N-1 个 0  和单个 1 组成的 vector 来表示每个类别。这样做有两个很明显的缺点：

**1.对于具有非常多类型的类别变量，变换后的向量维数过于巨大，且过于稀疏。**（另一种说法：当类别的数量很多时，特征空间会变得非常大。映射后的向量容易产生维数灾难

疑问：为什么说维数过于巨大？？？

	如有100个类，则每类由99个0和1个1组成的vector表示，类别再多也都是二维啊？？？这里的数组是二维·	的，但one hot编码得到的是n个向量，向量是有方向的
	多一个类别，在进行独热编码的时候就会多一维向量。Wikipedia中共包含37000本书，对于每本书来说，向量的维度都为37000，这将无法对任何机器学习模型进行训练。可以这么理解，两个类别，[[0,1],[1,0]]，在空间中就是x、y轴，而[[0,0,1],[0,1,0],[1,0,0]]在空间中就是三个向量xyz构成的三维。

什么叫过于稀疏？？？

One Hot的每一维只表示某个商品的有无，信息量太少



**2.映射之间完全独立，并不能表示出不同类别之间的关系。**

```python
# One Hot Encoding Categoricals
books = ["War and Peace", "Anna Karenina", 
          "The Hitchhiker's Guide to the Galaxy"]
books_encoded = [[1, 0, 0],
                 [0, 1, 0],
                 [0, 0, 1]]
Similarity (dot product) between First and Second = 0
Similarity (dot product) between Second and Third = 0
Similarity (dot product) between First and Third = 0

# 嵌入的理想化表示
# Idealized Representation of Embedding
books = ["War and Peace", "Anna Karenina", 
          "The Hitchhiker's Guide to the Galaxy"]
books_encoded_ideal = [[0.53,  0.85],
                       [0.60,  0.80],
                       [-0.78, -0.62]]
Similarity (dot product) between First and Second = 0.99
Similarity (dot product) between Second and Third = -0.94
Similarity (dot product) between First and Third = -0.97
```

而为了更好的表示类别实体，我们还可以是用一个 embedding neural network 和 supervised 任务来进行学习训练，以找到最适合的表示以及挖掘其内在联系。

One-hot 编码的最大问题在于其转换不依赖于任何的内在关系，而通过一个监督性学习任务的网络，我们可以通过优化网络的参数和权重来减少 loss 以改善我们的 embedding 表示，loss 越小，则表示最终的向量表示中，越相关的类别，它们的表示越相近。

embedding：
	举个例子，在电影评论中收集到50000个单词，每一个单词我们都可以使用100维的向量对其进行表示（若one-hot，则就是五万维了），进而使用嵌入神经网络对其进行训练以获取评论的情感倾向。如“brilliant”或“excellent”均与“positive”评价有很强的关联，在嵌入空间的位置便会更为邻近。



**如何生成embedding**

[矩阵分解、无监督建模、有监督建模](https://zhuanlan.zhihu.com/p/138310401)

最常用：无监督建模

无监督建模是生成Embedding的常用方法，按组织方式可以将数据分为序列和图两类，针对序列数据生成Embedding常采用word2vec或类似算法（item2vec, doc2vec等），针对图数据生成Embedding的算法称为Graph  Embedding，这类算法包括deepwalk、node2vec、struc2vec等，它们大多采用随机游走方式生成序列，底层同样也是word2vec算法。关于word2vec算法的实现细节，此处不再赘述。

![image-20211123211832123](E:\MarkDown\picture\image-20211123211832123.png)

word2vec:

输入输出层维度相同；Hidden Layer 没有激活函数，也就是线性的单元

当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如 **隐层的权重矩阵**。

![preview](E:\MarkDown\picture\v2-1b8925f9baac754e7248460285ded795_r.jpg)



<img src="E:\MarkDown\picture\image-20211123211958186.png" alt="image-20211123211958186" style="zoom: 100%;" />

![image-20211123212012306](E:\MarkDown\picture\image-20211123212012306.png)

### embedding在pytorch中的使用

torch.nn.Embedding(num_embeddings, embedding_dim,  padding_idx=None, max_norm=None, norm_type=2.0,  scale_grad_by_freq=False, sparse=False, _weight=None)
相当于随机生成了一个tensor，可以把它看作一个查询表，其size为[embeddings_num，embedding_dim] 。其中embeddings_num是查询表的大小，embedding_dim是每个查询向量的维度。生成一个查询表后，每个输入的向量embedding后都会和查询表对应生成一个输出向量

```python
import torch
import torch.nn as nn

embedding = nn.Embedding(5, 3)  # 定义一个具有5个单词，维度为3的查询矩阵,或5行三列
print(embedding.weight)  # 展示该矩阵的具体内容
test = torch.LongTensor([[0, 2, 0, 1],
                         [1, 3, 4, 4]])  # 该test矩阵用于被embed，其size为[2, 4]
# 其中的第一行为[0, 2, 0, 1]，表示获取查询矩阵中ID为0, 2, 0, 1的查询向量，即获取查询矩阵的第0，2，0，1行
test = embedding(test)
print(test.size())  # 输出embed后test的size，为[2, 4, 3]，增加
# 的3，是因为查询向量的维度为3
print(test)  # 输出embed后的test的内容
```

![image-20210908195150245](E:\MarkDown\picture\image-20210908195150245.png)

注意！如果需要Embedding的矩阵中的查询向量不为1，2这种整数，而是1.1这种浮点数，就不能与查询向量成功匹配，会报错，且如果矩阵中的值大于了查询矩阵的范围，比如这里是4，也会报错。

所以若是每个像素点为256的图片，则查询矩阵维度应大于256，即[*,256]



###  patch Embedding

用于将原始的2维图像转换成一系列的1维patch embeddings。

**不懂**

```python
import torch
import torch.nn as nn


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = (img_size, img_size)
        patch_size = (patch_size, patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])#  //整除
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.project = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        # 将输出变为通道768，大小为原来的/16

    def forward(self, x):
        B, C, H, W = x.shape# batch 通道 高 宽
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.project(x)# 1，768，14，14
        x = x.flatten(2)# 1,768,196
        x = x.transpose(1, 2)# 1,196,768
        return x


if __name__ == "__main__":
    x = torch.rand([1, 3, 224, 224])
    model = PatchEmbed()
    y = model(x)
    print(y.shape)# torch.Size([1, 196, 768])0
```



## Softmax

softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。

假设有一个数组V，V_i 表示V中的第i个元素，那么这个元素的softmax值为:
![image-20220328141521639](E:\MarkDown\picture\image-20220328141521639.png)
该元素的softmax值，就是该元素的指数与所有元素指数和的比值

![image-20220328141641397](E:\MarkDown\picture\image-20220328141641397.png)

这里yi只有真实类别处为1，因此为-log(真实类别预测的概率)

## 三维张量的transpose



<img src="E:\MarkDown\picture\image-20211009143008071.png" alt="image-20211009143008071" style="zoom:67%;" />



```python
vc = arr.transpose(1,0,2)
print(vc)
>>>结果
[[[ 0  1  2  3]
  [12 13 14 15]]

 [[ 4  5  6  7]
  [16 17 18 19]]

 [[ 8  9 10 11]
  [20 21 22 23]]]
```



## nn.Unfold()函数

pytorch中的nn.Unfold()函数，在图像处理领域，经常需要用到卷积操作，但是有时我们只需要在图片上进行滑动的窗口操作，将图片切割成patch，而不需要进行卷积核和图片值的卷积乘法操作。这是就需要用到nn.Unfold()函数，该函数是从一个batch图片中，提取出滑动的局部区域块，也就是卷积操作中的提取kernel filter对应的滑动窗口。
该函数的输入是（bs,c,h,w),其中bs为batch-size,C是channel的个数。
而该函数的输出是（bs,Cxkernel_size[0]xkernel_size[1],L)其中L是特征图或者图片的尺寸根据kernel_size的长宽滑动裁剪后得到的多个patch的数量。

```python
import torch.nn as nn
import torch
batches_img=torch.rand(1,2,4,4)#模拟图片数据（bs,2,4,4），通道数C为2
print("batches_img:\n",batches_img)

nn_Unfold=nn.Unfold(kernel_size=(2,2),dilation=1,padding=0,stride=2)
patche_img=nn_Unfold(batches_img)
print("patche_img.shape:",patche_img.shape)
print("patch_img:\n",patche_img)
```

![在这里插入图片描述](E:\MarkDown\picture\2021041313320280.png)










## 消融研究

通常用于神经网络，尤其是相对复杂的神经网络，如R-CNN。我们的想法是通过删除部分网络并研究网络的性能来了解网络。

你论文提了三个贡献点：A、B、C。你去掉A，其它保持不变，发现效果降低了，那说明A确实有用。你去掉B，其它保持不变，发现效果降的比A还多，说明B更重要。

## SOTA

**SOTA**全称是state of the art，是指在特定任务中目前表现最好的方法或模型。
Benchmark(基准)和baseline都是指最基础的比较对象。你论文的motivation来自于想超越现有的baseline/benchmark，你的实验数据都需要以baseline/benckmark为基准来判断是否有提高。唯一的区别就是baseline讲究一套方法，而benchmark更偏向于一个目前最高的指标，比如precision，recall等等可量化的指标。举个例子，NLP任务中BERT是目前的SOTA，你有idea可以超过BERT。那在论文中的实验部分你的方法需要比较的baseline就是BERT，而需要比较的benchmark就是BERT具体的各项指标。

作者：许力文 MorrisXu
链接：https://www.zhihu.com/question/433986039/answer/1618236738
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 虚拟环境相关

https://zhuanlan.zhihu.com/p/94744929

创建虚拟环境后

activate ***
cd到requirement.txt的位置
pip install -r requirements.txt





## 数学基础

### 矩阵求导

矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）：![img](E:\MarkDown\picture\8LDO48C$8@[GWU0353$FOVS-1636096943810.png)https://zhuanlan.zhihu.com/p/263777564
矩阵求导公式的数学推导（矩阵求导——基础篇）：![img](file:///C:\Users\yzew\AppData\Roaming\Tencent\QQTempSys\8LDO48C$8@[GWU0353$FOVS.png)https://zhuanlan.zhihu.com/p/273729929
矩阵求导公式的数学推导（矩阵求导——进阶篇）：![img](file:///C:\Users\yzew\AppData\Roaming\Tencent\QQTempSys\8LDO48C$8@[GWU0353$FOVS.png)https://zhuanlan.zhihu.com/p/288541909

[pytorch求梯度](https://blog.csdn.net/h1m2q3/article/details/116144689)

## 卷积

### 2D卷积

​	如图所示，每个3*3卷积核的一次卷积结果对应一个输出结果块，得到的各层特征图直接相加得到输出

![img](E:\MarkDown\picture\20190726091642767.gif)

![img](E:\MarkDown\picture\20190726091721978.gif)

​	其中，滤波器的卷积核数量与前一层Feature Maps数量相等，前一层有n个特征图(或说通道数)，就有n个卷积核与之对应，输出n个特征图直接相加得到输出Feature Map。
​	一个滤波器只在当前层产生一个Feature Map，可通过滤波器的数量来控制产生的Feature Map的数量。如生成C个Feature Map，则需要C个w * h * n的滤波器

![alt](E:\MarkDown\picture\aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS92Mi04NmUyYmQ5NzBkMDdmOWQ2ZTFkOTIxYjI0OGU0NWEzYV9iLndlYnA)

### 3D卷积

论文：3D Convolutional Neural Networks for Human Action Recognition，3D CNN

​	这里的2维 3维指滑窗操作的维度，而滑窗操作不在channel维度上进行，不管有几个channel，它们都共享同一个滑窗位置（虽然2D多channel卷积的时候每个channel上的卷积核权重是独立的，但滑窗位置是共享的）。所以在讨论卷积核维度的时候，是不把channel维加进去的。

​	相比2D增加了一个新的维度，即时间维度(在医学图像里为切片维度)。其特点是滤波器的卷积核数量小于前一层通道数。

​	图中a为单通道；b为L通道（多通道图像可以指同一张图片的3个颜色通道，也可以指多张堆叠在一起的图片，不过这样做的2维卷积和3d卷积不同，没有结合第三维度的信息，多通道的信息被完全压缩了），对应一个滤波器的L个卷积核，输出L个特征图后直接相加得到Feature Map；c多了L这个维度，对应滤波器维度也变成了d，滤波器需要在whL三个维度上平移来达成b的效果，生成w2 h2 L2的output。

![img](E:\MarkDown\picture\20200312162346511.png)

​	即如果是2D卷积就是一个卷积核数量为4(即图b中的L)的滤波器去卷积产生一张Feature Map。但是3D卷积共享一个卷积核产生了多张Feature Map（这里的“多张”就对应c中的L2），这些Feature Map之间包含着时间维度的信息，即连续帧之间的关联信息。但是如果想要不同类型的特征呢？与2D卷积一样，用不同的滤波器即可。


![image-20211104202031299](E:\MarkDown\picture\image-20211104202031299.png)

**hardwired kernels**

​	在实际应用中我们不可能直接对连续的帧直接做3D卷积，因为这样提取的信息过于简单，所以需要在3D卷积之前做一次hardwired kernels 卷积。
​	作者利用 hardwired kernels 生成了灰度、x方向梯度、y方向梯度、x方向光流、y方向光流五种特征信息，前面三个通道的信息可以直接对每帧分别操作获取，后面的光流（x，y）则需要利用两帧的信息才能提取，因此H1层的特征maps数量：（7+7+7+6+6=33），特征maps的大小依然是60 * 40。

![在这里插入图片描述](E:\MarkDown\picture\20190726100654674.png)

### 空洞卷积

rate为扩张率

空洞卷积实际卷积核计算：K=k+(k-1)(r-1)
感受野计算：当前感受野=上层感受野(这里为1)  + (k-1)Si (Si为步长，这里都为1)（Si其实是累加步长，这样计算就没问题了，算出来的当前感受野是对应input的）

因此对于3*3卷积，d=3，则卷积核为7，感受野为1+ 6=7

![在这里插入图片描述](E:\MarkDown\picture\20190709164232982.png)

![image-20210916102616584](E:\MarkDown\picture\image-20210916102616584.png)

对于3*3卷积，d=3，则卷积核为7，感受野为1+ 6=7

图中的c，padding为3

（（（（为什么图中和公式的计算感受野存在差异？？？

（b）卷积的感受野已经增大到了7x7（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，1-dilated作为2-dilated的上层感受野，就能达到7x7的conv））））

### 转置卷积

deconv

反卷积(转置卷积)，可以用来增大输入的高和宽

![img](E:\MarkDown\picture\v2-286ac2cfb69abf4d8aa06b8eeb39ced3_b.webp)

![img](E:\MarkDown\picture\v2-2e99f89da83f29131f1a4c7593a6d5fa_b.webp)			

input中每个元素分别和卷积核相乘  后相加

![image-20210921215533553](E:\MarkDown\picture\image-20210921215533553.png)

stride如果为2，则0123的块再向右移一列

![image-20210922202217264](E:\MarkDown\picture\image-20210922202217264.png)

### 深度可分离卷积

参数数量和运算成本比较低

![image-20220101211518068](E:\MarkDown\picture\image-20220101211518068.png)

mobilenet

xception

inception

### 卷积计算

![image-20210820194743810](E:\MarkDown\picture\image-20210820194743810.png)

dilation使用空洞卷积时

![image-20210820194700953](E:\MarkDown\picture\image-20210820194700953.png)

三维卷积

![image-20210817145736438](E:\MarkDown\picture\image-20210817145736438.png)

![image-20210816133409918](E:\MarkDown\picture\image-20210816133409918.png)

如果你有一个n∗n∗nc（通道数）的输入图像，在这个例子中就是 6×6×3，这里的nc就是通道数目，然后卷积上一个f∗f∗nc，这个例子中是 3×3×3，然后你就得到了(n - f + 1) \times (n - f + 1) \times {n_{{c^’}}}:这里{n_{{c^’}}}其实就是下一层的通道数，它就是你用的过滤器的个数，在我们的例子中，那就是 4×4×2。这个假设时用的步幅为 1，并且没有 padding。如果你用了不同的步幅或者 padding，那么这个n − f + 1数值会变化。

![image-20210816134406437](E:\MarkDown\picture\image-20210816134406437.png)

### 图卷积GCN

[图卷积介绍](https://zhuanlan.zhihu.com/p/89503068)

### 各种卷积的总结

[万字长文带你看尽深度学习中的各种卷积网络](https://mp.weixin.qq.com/s/1gBC-bp4Q4dPr0XMYPStXA)

补充：

2：

**权重共享**（weights sharing）：权值共享就是说，给一张输入图片，用一个 卷积核 去扫这张图，卷积核里面的数就叫权重，这张图每个位置是被同样的卷积核扫的，所以 权重 是一样的，也就是共享。
**平移不变性**（translation invariant）：平移不变性意味着系统产生完全相同的响应（输出），不管它的输入是如何平移的 。在神经网络中，卷积被定义为不同位置的特征检测器，也就意味着，无论目标出现在图像中的哪个位置，它都会检测到同样的这些特征，输出同样的响应。比如人脸被移动到了图像左下角，卷积核直到移动到左下角的位置才会检测到它的特征。
而平移同变性（translation equivariance）意味着系统在不同位置的工作原理相同，但它的响应随着目标位置的变化而变化 。比如，实例分割任务，就需要平移同变性，目标如果被平移了，那么输出的实例掩码也应该相应地变化。

4：

为什么卷积操作后是线性分类器，加入1 * 1卷积后为非线性分类器？因为激活函数？
NIN([Network in Network](https://arxiv.org/pdf/1312.4400.pdf)，Inception的前身，提出了通过在中间加入1 * 1卷积来降维)结构中无论是第一个3x3卷积还是新增的1x1卷积，后面都紧跟着激活函数（比如relu，是线性激活函数）。将两个卷积串联，就能组合出更多的非线性特征。

为什么1*1卷积可以对特征进行池化？

使用2×2的最大池化，与使用卷积（stride为2）来做down sample性能并没有明显差别，而且使用卷积（stride为2）相比卷积（步进为1）+池化，还可以减少卷积运算量和一个池化层。

补充一个，交叉通道池化（cross channel pooling）操作不同通道的同一个位置：假设你有一个线性模块，该模型有50个输出（或者说是有50个channels）,但是你想要的是5个输出。你就可以使用cross-channel pooling来减少对channel进行下采样（使用最大池化）。
 如果是一个卷积模块，输出了50个特征图，那么通过cross-channel  pooling就可以输出5个特征图。具体思路：输出特征图（这里的特征图指的是经过cross-channel  pooling输出的5个特征图）中的每个点是10个特征图（这里的特征图指的是原始的50个特征图）中对应点的最大值。
  一个cross-channel  pooling层不是一个maxout层（这个应该是论文中的概念），两者之间是不同的。Maxout中的每个单元都在50个特征图上操作，而cross-channel pooling的每个单元是在10个特征图上进行操作。Maxout层中的Maxout单元的数量是没有限制的，而cross-channel  pooling层中，其数量不能超过输入的数量。

10.1

ResNeXt中的c用到了分组卷积，但其目的不是为了多GPU，只是将其分组，为了让block看起来更为简洁



## 池化

### 标准池化

通常包括最大池化(MaxPooling)和平均池化(Mean Pooling)。以最大池化为例，池化核尺寸为2 * 2，池化步长为2，可以看到特征图中的每一个像素点只会参与一次特征提取工作。这个过程可以用下图表示：

![img](E:\MarkDown\picture\v2-433b986730dad1542ba2f16a9c07147f_1440w.jpg)

### 重叠池化

操作和标准池化相同，但唯一不同地方在于滑动步长stride小于池化核的尺寸k，可以想象到这样的话特征图中的某些区域会参与到多次特征提取工作，最后得到的特征表达能力更强。这个过程可以表示为下图：

![img](E:\MarkDown\picture\v2-5fc44e630eec4d465f05bfc8dff792b9_1440w.jpg)

### SPP-Net**空间金字塔池化**

何凯明2014年的paper：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition，使得我们构建的网络，可以输入任意大小的图片，不需要经过裁剪缩放等操作，并且可以提高精度。

![image-20211106141843758](E:\MarkDown\picture\image-20211106141843758.png)

![img](E:\MarkDown\picture\v2-6b2b2836502d9e4ca46f5f7409f30cee_1440w.jpg)

对于不同尺寸的CNN_Pre输出能够输出固定大小的向量当然是其最大的好处，除此之外SPP的优点还有：

1. 可以提取不同尺寸的空间特征信息，可以提升模型对于空间布局和物体变性的鲁棒性。
2. 可以避免将图片resize、crop成固定大小输入模型的弊端。

## 一些结构

### 残差网络

背景知识：

    为什么要构建深层网络？
    答：认为神经网络的每一层分别对应于提取不同层次的特征信息，有低层，中层和高层，而网络越深的时候，提取到的不同层次的信息会越多，而不同层次间的层次信息的组合也会越多。
    ResNets为什么能构建如此深的网络？
    答：深度学习对于网络深度遇到的主要问题是梯度消失和梯度爆炸，传统对应的解决方案则是数据的初始化(normlized initializatiton)和（batch normlization）正则化，但是这样虽然解决了梯度的问题，深度加深了，却带来了另外的问题，就是网络性能的退化问题，深度加深了，错误率却上升了，而残差用来设计解决退化问题，其同时也解决了梯度问题，更使得网络的性能也提升了。

普通网络（Plain network），类似VGG，没有残差，凭经验会发现随着网络深度的加深，训练错误会先减少，然后增多（并证明的错误的增加并不是由于过拟合产生，而是由于网络变深导致难以训练）。从理论上分析，网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的增加，训练误差会越来越多，这被描述为网络退化。
ResNets的提出，可以解决上述问题，即使网络再深吗，训练的表现仍表现很好。它有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的信息。
![image-20210818135639951](E:\MarkDown\picture\image-20210818135639951.png)

实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1, 如下图。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。
![image-20210818135738824](E:\MarkDown\picture\image-20210818135738824.png)

ResNet就是用这种跳跃结构来作为网络的基本结构。
 假如本来我们要优化的目标是H（x）=F（x）+ x，但通过这个结构就把优化的目标有H（x）转化成H（x）- x。

***

为什么经过优化目标转化后就可以解决退化问题呢？我们之前说到，深层网络在浅层网络的基础上只要上面几层做一个等价映射就可以达到浅层网络同样的效果，但是为什么不行呢？就是因为我们的算法很难将其训练到那个程度，也就是没办法将上面几层训练到一个等价映射，以至于深网络最后达到一个更差的效果。
通过改变结构，把训练目标进行转变，由H（x）转变为H（x）- x，因为这时候就不是吧上面几层训练到一个等价映射，而是将其逼近于0，这样训练的难度比训练一个等价映射应该下降多了。

```python
#---------------------------------------------------------------------#
#   残差结构
#   利用一个1x1卷积下降通道数，然后利用一个3x3卷积提取特征并且上升通道数
#   最后接上一个残差边
#---------------------------------------------------------------------#
class BasicBlock(nn.Module):
    def __init__(self, inplanes, planes):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes[0], kernel_size=1,
                               stride=1, padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(planes[0])
        self.relu1 = nn.LeakyReLU(0.1)
        
        self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes[1])
        self.relu2 = nn.LeakyReLU(0.1)

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu2(out)

        out += residual
        return out
```

### Bottleneck layer

瓶颈层
<img src="E:\MarkDown\picture\image-20220224104217845.png" alt="image-20220224104217845" style="zoom:50%;" />

```python
class Bottleneck(nn.Module):
    expansion = 4
    """1x1卷积将通道数变为planes来降低特征维度，3x3卷积，再1x1卷积升维到4xplanes"""

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,
                               bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion,
                                  momentum=BN_MOMENTUM)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out

```



### SE模块

SE 模块主要包含Squeeze和Excitation两部分

![image-20210818174046259](E:\MarkDown\picture\image-20210818174046259.png)

1.压缩
<img src="E:\MarkDown\picture\image-20210818174133009.png" alt="image-20210818174133009" style="zoom: 67%;" />

通过全局平均池化（global average pooling）将特征图压缩为1×1×C向量。

2.激励
<img src="E:\MarkDown\picture\image-20210818174333452.png" alt="image-20210818174333452" style="zoom:67%;" />

由两个全连接层组成，其中SERatio是一个缩放参数，这个参数的目的是为了减少通道个数从而降低计算量。
 第一个全连接层有C*SERatio个神经元，输入为1×1×C，输出1×1×C×SERadio。
 第二个全连接层有C个神经元，输入为1×1×C×SERadio，输出为1×1×C。

3.scale操作

通道权重相乘，原有特征向量为W×H×C，将SE模块计算出来的各通道权重值分别和原特征图对应通道的二维矩阵相乘，得出的结果输出。

4.应用

ResNet中添加SE模块形成SE-ResNet网络，SE模块是在bottleneck结构之后加入的，如下图左边所示。

![image-20210818191045630](E:\MarkDown\picture\image-20210818191045630.png)

MobileNetV3版本中SE模块加在了bottleneck结构的内部，在深度卷积后增加SE块，scale操作后再做逐点卷积，如上图右边所示。MobileNetV3版本的SERadio系数为0.25。使用SE模块后的MobileNetV3的参数量相比MobileNetV2多了约2M，达到5.4M，但是MobileNetV3的精度得到了很大的提升，在图像分类和目标检测中准确率都有明显提升。

激活函数为Hard-Sigmoid



## 训练相关





### batchsize

batchsize是否会影响模型结果？batchsize过小是否可能导致最终累积的梯度计算不准确？

batchsize越小其实是对收敛越好，因为随机梯度下降理论上是带来了噪音，样本越少噪音越多，但深度神经网络太过复杂，一定的噪音会使得不会走偏，泛化性更好

### softlabel

标签由1 0 0变为0.9 0.05 0.05，更容易用softmax去拟合(很难用指数去逼近1)，在图片分类里面是默认都会使用的小trick
https://zhuanlan.zhihu.com/p/410491474

### feature scaling

<img src="E:\MarkDown\picture\image-20211220204746660.png" alt="image-20211220204746660" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211220204728285.png" alt="image-20211220204728285" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211220204944160.png" alt="image-20211220204944160" style="zoom:50%;" />



<img src="E:\MarkDown\picture\image-20211221191540575.png" alt="image-20211221191540575" style="zoom:70%;" />





### 神经网络调参

deep learning（rnn、cnn）调参的经验？


三、训练技巧
要做梯度归一化,即算出来的梯度除以minibatch size
clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w12+w22….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15

adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。
除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。
rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好。
word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果。

### 梯度消失 vanishing gradient problem

<img src="E:\MarkDown\picture\image-20211221191953989.png" alt="image-20211221191953989" style="zoom:50%;" />

后半段有较大的梯度，已经converge收敛了，而前半段学习的较慢
问题产生原因：
sigmoid激活函数，使得梯度在靠近input的方向越来越小
解决方法：
除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数

### 梯度爆炸
30.什么是梯度爆炸？
误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。
在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。
网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。
31.梯度爆炸会引发什么问题？
在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。
梯度爆炸导致学习过程不稳定。—《深度学习》，2016。
在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。
32.如何确定是否出现梯度爆炸？
训练过程中出现梯度爆炸会伴随一些细微的信号，如：
模型无法从训练数据中获得更新（如低损失）。
模型不稳定，导致更新过程中的损失出现显著变化。
训练过程中，模型损失变成 NaN。
如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。
以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。
训练过程中模型梯度快速变大。
训练过程中模型权重变成 NaN 值。
训练过程中，每个节点和层的误差梯度值持续超过 1.0。

###  如何修复梯度爆炸问题？

有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。
（1） 重新设计网络模型
在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。
使用更小的批尺寸对网络训练也有好处。
在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。
（2）使用 ReLU 激活函数
在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。
使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。
（3）使用长短期记忆网络
在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。
使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。
采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。
（4）使用梯度截断（Gradient Clipping）
在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。
处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。
——《Neural Network Methods in Natural Language Processing》，2017.
具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。
梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。
——《深度学习》，2016.
在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或clipvalue 参数，来使用梯度截断。
默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。
（5）使用权重正则化（Weight Regularization）如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。
对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。
——On the difficulty of training recurrent neural networks，2013.
在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用L1 或 L2 正则化项进行权重正则化bn



6.参数初始化
下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。

下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5

**Xavier**初始化论文：（这个李沐提到过）
http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
**He**初始化论文：
https://arxiv.org/abs/1502.01852
**uniform**均匀分布初始化：
w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])

Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)
He初始化，适用于ReLU：scale = np.sqrt(6/n)
normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0

Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)
He初始化，适用于ReLU：stdev = np.sqrt(2/n)
svd初始化：对RNN有比较好的效果。

### 激活函数

#### Sigmoid

Sigmoid(S型)会导致梯度消失

网络的Loss函数相对于某层神经元的偏导表达式包含多个Sigmoid梯度的乘积，实际的神经网络层数少则数十多则数百，这么多范围在(0,0.25)的数的乘积，将会是一个非常小的数字。而梯度下降法更新参数完全依赖于梯度值，极小的梯度无法让参数得到有效更新，即使有微小的更新，浅层和深层网络参数的更新速率也相差巨大。该现象就称为“**梯度消失(Vanishing Gradients)**”

#### ReLU

计算速度更快，且可以避免梯度消失<img src="E:\MarkDown\picture\image-20211221200246049.png" alt="image-20211221200246049" style="zoom:50%;" />

问题：
线性激活函数，变为线性模型：多个叠加，仍未非线性
为负时微分为0，为0时不能微分

ReLU(Rectified Linear Unit)([ˈlɪniə(r)])线性整流函数的提出 就是为了解决梯度消失问题，因为ReLU的梯度只可以取两个值：0或1。将输入小于0的值幅值为0，输入大于0的值不变

ReLU会导致神经元死亡

假设bias变得太小，以至于输入激活函数的值总是负的，那么反向传播过程经过该处的梯度恒为0,对应的权重和偏置参数此次无法得到更新。如果对于所有的样本输入，该激活函数的输入都是负的，那么该神经元再也无法学习，称为神经元”死亡“问题。



#### LeakyReLU

LeakyReLU的提出就是为了解决神经元”死亡“问题

LeakyReLU输入小于0的部分，值为负，且有微小的梯度

<img src="E:\MarkDown\picture\image-20211221201245131.png" alt="image-20211221201245131" style="zoom:50%;" />

#### Maxout

是一个可学习的激活函数，可以拟合任意的的凸函数
![image-20211221202302796](E:\MarkDown\picture\image-20211221202302796.png)
如每次将两个输出作为一组，取最大的向后输出

<img src="E:\MarkDown\picture\image-20211221201906208.png" alt="image-20211221201906208" style="zoom:67%;" />

ReLU可以看为是Maxout的一个特例

![image-20211221202007078](E:\MarkDown\picture\image-20211221202007078.png)

Maxout可以拟合出各种不同的激活函数
<img src="E:\MarkDown\picture\image-20211221202145451.png" alt="image-20211221202145451" style="zoom:67%;" />



### 优化算法

<img src="E:\MarkDown\picture\image-20211220215559486.png" alt="image-20211220215559486" style="zoom:67%;" />

<img src="E:\MarkDown\picture\image-20211220215649253.png" alt="image-20211220215649253" style="zoom:80%;" />

SGDM：更慢；更好的收敛；更稳定；有小的generalization gap，即在训练集和测试集上的表现不会差太多

https://zhuanlan.zhihu.com/p/61955391

#### 随机梯度下降法SGD

stochastic gradient descent<img src="E:\MarkDown\picture\image-20211220200439284.png" alt="image-20211220200439284" style="zoom:50%;"><img src="E:\MarkDown\picture\image-20211220200544032.png" alt="image-20211220200544032" style="zoom:50%;" />

```python
class SGD:
    def __int__(self, lr=0.01):
        self.lr = lr
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]

#使用
optimizer=SGD()

optimizer.update(params, grads)
```



#### SGD with Momentum（SGDM

动量：想象是个球滚下山谷，借助势能滚出局部最小点

<img src="E:\MarkDown\picture\image-20211220214011021.png" alt="image-20211220214011021" style="zoom:75%;" />

<img src="E:\MarkDown\picture\image-20211221204035966.png" alt="image-20211221204035966" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211220213653958.png" alt="image-20211220213653958" style="zoom: 80%;" />

```python
class Momentum:
    """Momentum SGD"""
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():                                
                self.v[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            params[key] += self.v[key]

```

#### AdaGrad

<img src="E:\MarkDown\picture\image-20211221202930832.png" alt="image-20211221202930832" style="zoom:50%;" /><img src="E:\MarkDown\picture\image-20211220194518328.png" alt="image-20211220194518328" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211220194608233.png" alt="image-20211220194608233" style="zoom:50%;" />

这里有个疑问，gt越大，迈的步伐越大，而最后一个式子中gt的系数，分母使得gt越大步伐越小？？

解释：
反差
<img src="E:\MarkDown\picture\image-20211220195415100.png" alt="image-20211220195415100" style="zoom:50%;" />

```python
class AdaGrad:
    """AdaGrad"""
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

#### RMSProp

Adagrad的问题：如果一开始的gt比较大，那学习率部分的分母一累加就很大了，会导致走没几步就卡住了
有点momentum的思想，加入了过去的影响，但不会像Adagrad累加的那种，永无止境的变大。这里α为可学习的参数，α越小，则更倾向于相信当前时刻gt的影响，α越大，则前一时刻gt-1的权重越大
![image-20211221203355663](E:\MarkDown\picture\image-20211221203355663.png)

<img src="E:\MarkDown\picture\image-20211220214134924.png" alt="image-20211220214134924" style="zoom:80%;" />

#### Adam

结合AdaGrad和RMSProp两种优化算法的优点

<img src="E:\MarkDown\picture\image-20211220214706962.png" alt="image-20211220214706962" style="zoom:80%;" />

### 防止过拟合：



#### early stopping早停

<img src="E:\MarkDown\picture\image-20211221212213909.png" alt="image-20211221212213909" style="zoom:67%;" />

#### regularization正则化/权重衰退

![img](E:\MarkDown\picture\{YO3CI2TLN$I{0IY_[1P8B.jpg)

![image-20220329221449020](E:\MarkDown\picture\image-20220329221449020.png)

![image-20220329222121275](E:\MarkDown\picture\image-20220329222121275.png)

每一次更新的时候，因为蓝各大的引入，将w进行了一次缩小。使用的话直接在SGD中设置weight_decay即可

[深入理解L1、L2正则化](https://www.cnblogs.com/zingp/p/10375691.html)

正则化：在loss function中加入额外项，让w更小，让曲线更平滑，减少噪声造成的影响

<img src="E:\MarkDown\picture\image-20211220152115964.png" alt="image-20211220152115964" style="zoom:67%;">

<img src="E:\MarkDown\picture\image-20211221212548929.png" alt="image-20211221212548929" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211221212943767.png" alt="image-20211221212943767" style="zoom:50%;" />

随着正则化参数λ的增大，曲线越平滑，train loss会增大（后半部分的影响），但test loss可能会更小

#### Dropout

使用有噪音的数据等价于Tikhonov正则，如L2正则使得权重不会太大，这里这个噪音是随机噪音。
而丢弃法就是在层之间加入噪音。不过此方法常作用在多层感知器的隐藏层输出上，CNN等模型上很少用到。
<img src="E:\MarkDown\picture\image-20220330185135732.png" alt="image-20220330185135732" style="zoom:40%;" /><img src="E:\MarkDown\picture\image-20220330185304286.png" alt="image-20220330185304286" style="zoom:40%;" />

通过除1-p，使得期望不变。这里丢弃法只应用在训练中

对小数据(因为当训练集的数量不多而网络参数相对多时，训练样本误差带来的影响很大，拟合的过程中强行逼近到了误差值)防止过拟合有很好的效果,值一般设为0.5

> 为什么dropout能够有效解决overfitting的问题?
>
> 取平均的作用： 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
>
> 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）



dropout使得每次训练时的网络都不一样；且加入dropout后训练时的performance会变差，因为总有些神经元会消失

<img src="E:\MarkDown\picture\image-20211221224032966.png" alt="image-20211221224032966" style="zoom:70%;" />

小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329

<img src="E:\MarkDown\picture\image-20211221224518776.png" alt="image-20211221224518776" style="zoom:50%;" />

注意，训练好后，在测试时需要将权重全除2

![image-20211221224648953](E:\MarkDown\picture\image-20211221224648953.png)

#### 交叉验证cross validation.

[机器学习四种交叉验证方式](https://blog.csdn.net/weixin_39830233/article/details/110550650)

#### k-折交叉验证

在数据集不够的情况下去做，而一般深度学习的数据集都比较大，所以一般不做h

[实现](https://blog.csdn.net/u014264373/article/details/116241388)

k-fold cross validation，一般用五折或十折

![image-20211104102657265](E:\MarkDown\picture\image-20211104102657265.png)

如图，对模型一，在验证集不同的情况下得到五个error，计算平均error。
之后依次计算五/十个模型的平均error，选个平均error最低的模型，再在完整的训练集上train。



**交叉验证的目的：**在实际训练中，模型通常对训练数据好，但是对训练数据之外的数据拟合程度差。用于**评价模型的泛化能力**，从而进行**模型选择**。

基本思想：将数据分为训练集和验证集，用训练集对模型进行训练，用验证集测试模型的泛化误差。但因数据是有限的，比如对一个100张图片的数据集，拿10张做验证集后，数据就更少了。因此为了对数据形成重用，提出了k折交叉验证。

原理：将数据分为K组，将每个子集数据分别做一次验证集，其余的K-1个子集数据作为训练集，这样一共会得到K个模型。这K个模型分别在各自的验证集中评估结果，最后的误差加权平均就得到交叉验证误差

#### 多GPU的计算和Sync BatchNorm(多GPU进行归一化)

https://zhuanlan.zhihu.com/p/69940683



![image-20210912150826436](E:\MarkDown\picture\image-20210912150826436.png)



![image-20210912150940489](E:\MarkDown\picture\image-20210912150940489.png)



如果每块GPU可以设置较大的bs，则同步BN也可不用。该方法会降低并行速度，但可以提高精度

#### DataParallel和DistributedDataParallel

其中一个为老方法，第二个为较新方法且可以用于多机多卡，且速度更快

![image-20210912152036170](E:\MarkDown\picture\image-20210912152036170.png)

<img src="E:\MarkDown\picture\image-20210912152132398.png" alt="image-20210912152132398" style="zoom:25%;" />





167行引入同步BN开关



model = nn.DataParallel(model)



### net.train()、net.eval()

神经网络模块存在两种模式，train模式（net.train())和eval模式（net.eval())。一般的神经网络中，这两种模式是一样的，只有当模型中存在dropout和batchnorm的时候才有区别。比如针对上图中的训练网络，四层有三层进行了dropout，这是为了训练出更加准确的网络参数，一旦我们用测试集进行结果测试的时候，一定要使用net.eval()把dropout关掉，因为这里我们的目的是测试训练好的网络，而不是在训练网络，没有必要再dropout。

### 损失函数

从loss处理图像分割中类别极度不均衡的状况

[从loss处理图像分割中类别极度不均衡的状况](https://blog.csdn.net/m0_37477175/article/details/83004746)

#### L2 Loss

![image-20220328142826953](E:\MarkDown\picture\image-20220328142826953.png)

蓝色曲线表示y=0时，变化y'得到的函数，绿色为似然函数，e^-l；橙色为损失函数的梯度，一次函数。y和y'越接近，看橙色的曲线，梯度绝对值会越来越小。但有时预测值y'离原点比较远的时候，也不想要这么大的梯度，因此引入L2 Loss

#### L1 Loss

![image-20220328143041067](E:\MarkDown\picture\image-20220328143041067.png)
梯度永远是常数；但零点处不可导，且有不平滑性，在训练末期可能会不稳定

#### Huber's Robust Loss

![image-20220328143200339](E:\MarkDown\picture\image-20220328143200339.png)

#### CE_loss交叉熵损失

Cross Entropy Loss

[讲解](https://blog.csdn.net/weixin_40522801/article/details/106616295)



<img src="E:\MarkDown\picture\image-20211009140954234.png" alt="image-20211009140954234" style="zoom:67%;" />



对其求ln后，为logsoftmax = logits - log(reduce_sum(exp(logits), axis))，reduce_sum为求和，axis为对哪一维进行求和

```c++
def CE_Loss(inputs, target, num_classes=21):
    n, c, h, w = inputs.size()
    nt, ht, wt = target.size()
    if h != ht and w != wt:
        inputs = F.interpolate(inputs, size=(ht, wt), mode="bilinear", align_corners=True)
    # 四维 n, c, h, w
    # 转换为 n h w c
    # 再view进行reshape，view中一个参数定为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变。最后变为二维的即 nhw,c
    temp_inputs = inputs.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)
    # 从n h w转换为 nhw
    temp_target = target.view(-1)


    # softmax(x)+log(x)+nn.NLLLoss====>nn.CrossEntropyLoss
    # 对temp_inputs的-1维即最后一维进行log_softmax
    CE_loss  = nn.NLLLoss(ignore_index=num_classes)(F.log_softmax(temp_inputs, dim = -1), temp_target)
    return CE_loss
```



#### Dice Loss

小目标图像分割任务（医疗方向）中，往往一幅图像中只有一个或者两个目标，而且目标的像素比例比较小，使网络训练较为困难，因此需要选择合适的loss function，对网络进行合理的优化，关注较小的目标。

dice loss 比较适用于样本极度不均的情况，一般的情况下，使用 dice loss 会对反向传播造成不利的影响，容易**使训练变得不稳定**



![image-20211009144704448](E:\MarkDown\picture\image-20211009144704448.png)

![image-20211009144845827](E:\MarkDown\picture\image-20211009144845827.png)

![image-20211009144900145](E:\MarkDown\picture\image-20211009144900145.png)

#### Focal Loss

https://www.cnblogs.com/endlesscoding/p/12155588.html

#### 自定义损失函数及调用



```python
import torch
import torch.nn as nn

class BinaryDiceLoss(nn.Module):
	def __init__(self):
		super(BinaryDiceLoss, self).__init__()
	
	def forward(self, input, targets):
		# 获取每个批次的大小 N
		N = targets.size()[0]
		# 平滑变量
		smooth = 1
		# 将宽高 reshape 到同一纬度
		input_flat = input.view(N, -1)
		targets_flat = targets.view(N, -1)
	
		# 计算交集
		intersection = input_flat * targets_flat 
		N_dice_eff = (2 * intersection.sum(1) + smooth) / (input_flat.sum(1) + targets_flat.sum(1) + smooth)
		# 计算一个批次中平均每张图的损失
		loss = 1 - N_dice_eff.sum() / N
		return loss

class MultiClassDiceLoss(nn.Module):
	def __init__(self, weight=None, ignore_index=None, **kwargs):
		super(MultiClassDiceLoss, self).__init__()
		self.weight = weight
		self.ignore_index = ignore_index
		self.kwargs = kwargs
	
	def forward(self, input, target):
		"""
			input tesor of shape = (N, C, H, W)
			target tensor of shape = (N, H, W)
		"""
		# 先将 target 进行 one-hot 处理，转换为 (N, C, H, W)
		nclass = input.shape[1]
		target = F.one_hot(target.long(), nclass)

		assert input.shape == target.shape, "predict & target shape do not match"
		
		binaryDiceLoss = BinaryDiceLoss()
		total_loss = 0
		
		# 归一化输出
		logits = F.softmax(input, dim=1)
		C = target.shape[1]
		
		# 遍历 channel，得到每个类别的二分类 DiceLoss
		for i in range(C):
			dice_loss = binaryDiceLoss(logits[:, i], target[:, i])
			total_loss += dice_loss
		
		# 每个类别的平均 dice_loss
		return total_loss / C
    

criterion = MultiClassDiceLoss()
loss = criterion(logits,mask.long())
```

### 预训练(pre-training/trained)与微调(fine-tuning)

什么是预训练和微调？

预训练(pre-training/trained)：你需要搭建一个网络来完成一个特定的图像分类的任务。首先，你需要随机初始化参数，然后开始训练网络，不断调整直到网络的损失越来越小。在训练的过程中，一开始初始化的参数会不断变化。当你觉得结果很满意的时候，就可以将训练模型的参数保存下来，以便训练好的模型可以在下次执行类似任务时获得较好的结果。这个过程就是pre-training。

之后，你又接收到一个类似的图像分类的任务。这个时候，你可以直接使用之前保存下来的模型的参数来作为这一任务的初始化参数，然后在训练的过程中，依据结果不断进行一些修改。这时候，你使用的就是一个pre-trained模型，而过程就是fine-tuning。

所以，预训练就是指预先训练的一个模型或者指预先训练模型的过程；微调 就是指将预训练过的模型作用于自己的数据集，并参数适应自己数据集的过程。

微调的作用

在CNN领域中。很少人自己从头训练一个CNN网络。主要原因上自己很小的概率会拥有足够大的数据集，从头训练，很容易造成过拟合。

所以，一般的操作都是在一个大型的数据集上训练一个模型，然后使用该模型作为类似任务的初始化或者特征提取器。比如VGG，Inception等模型都提供了自己的训练参数，以便人们可以拿来微调。这样既节省了时间和计算资源，又能很快的达到较好的效果。





### 报错

CUDA error:out of memory

#### 检查一下是不是有loss的累加，如果有，那么一整个epoch的梯度都会常驻显存

https://pytorch.org/docs/stable/notes/faq.html 

#### 查一下gpu上运行的进程，如果没有的话，就是gpu内存泄漏了，具体的百度查一下释放gpu内存的方法。

#### 首先设置显存自适应增长：

import os
import tensorflow as tf
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  
gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

然后命令 nvidia-smi 看一下什么程序占用最多显存
杀掉之后把脚本所在文件夹里头的python编译文件全部删掉 

最后 重启jupyter就可以了 









## NLP相关

[RNN - LSTM - GRU](https://zhuanlan.zhihu.com/p/60915302)

### 1.RNN（循环神经网络）

[RNN综述](https://blog.csdn.net/heyongluoyao8/article/details/48636251)

[RNN反向传播算法BPTT](https://blog.csdn.net/qq_32172681/article/details/100060263)

如图1.1所示，全连接网络的特点是：**层与层之间是全连接的，每层之间的节点是无连接的**，因此全连接网络只能处理一个一个的输入。X是一句话，可以将x1，x2，x3分别视为一个字(或一个拼音，一个音素，语音的一帧数据等)。将x1和x2换一下位置，对于网络本身的学习来说是没有任何变化的。即对于全连接网络来说，前一个输入和后一个输入是完全没有关系的。但是我们在现实生活中会遇到很多和序列有关的问题，例如：“We love working on deep learning”, 打乱顺序为“working love learning we on deep”，意思是完全不同，甚至是完全无法理解的。这种**序列的信息决定事件本身**的任务，就需要**先前的知识**与当前信息共同决定输出结果。因此对处理序列的网络提出需求，也是RNN网络存在的意义。



传统神经网络只能单独处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。

> 比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。

循环神经网络如图
<img src="E:\MarkDown\picture\image-20210908102324490.png" alt="image-20210908102324490" style="zoom:50%;" />

x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；U是输入层到隐藏层的权重矩阵，o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。
循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于**上一次隐藏层**的值s。**权重矩阵W**就是隐藏层上一次的值作为这一次的输入的权重。
![image-20210908102550905](E:\MarkDown\picture\image-20210908102550905.png)

![image-20210908102701128](E:\MarkDown\picture\image-20210908102701128.png)

问题：

​	RNN基于这样的机制，信息的结果依赖于前面的状态或前N个时间步。普通的RNN可能在学习长距离依赖性方面存在困难。例如，如果我们有这样一句话，“The man who ate my pizza has purple hair”。在这种情况下，purple hair描述的是The man，而不是pizza。所以这是一个长距离的依赖关系。

​	如果我们在这种情况下反向传播，就需要应用链式法则。在三个时间步后对第一个求梯度的公式如下：∂E/∂W = ∂E/∂y_3  ∂y_3/∂h_3  ∂h_3/∂y_2  ∂![y_2](https://math.jianshu.com/math?formula=y_2)/∂h_1 .. 这就是一个长距离的依赖关系。如果任何一个梯度接近0，所有的梯度都会成指数倍的迅速变成零。这样将不再有助于网络学习任何东西。这就是所谓的消失梯度问题。

​	消失梯度问题与爆炸梯度问题相比，对网络更有威胁性，梯度爆炸就是由于单个或多个梯度值变得非常高，梯度变得非常大。之所以我们更关心梯度消失问题，是因为通过一个预定义的阈值可以很容易地解决梯度爆炸问题。幸运的是，也有一些方法来处理消失梯度问题。如LSTM结构（长短期记忆网络）和GRU（门控性单位）可以用来处理消失的梯度问题。

### 2.LSTM(RNN变种1)

RNN 理论上可以建立长时间间隔状态之间的依赖关系，但由于[梯度爆炸或消失问题](https://zhuanlan.zhihu.com/p/60915302)，实际上只能学到短期依赖关系。为了学到长期依赖关系，长短期记忆（Long Short Term Memory，LSTM）中引入了门控机制来控制信息的累计速度，包括有选择地加入新的信息，并有选择地遗忘之前累计的信息
![image-20210908110241769](E:\MarkDown\picture\image-20210908110241769.png)





### 3.GRU(RNN变种2)

门控循环单元，通过引入两个门来对前面信息进行控制
![image-20210908104322778](E:\MarkDown\picture\image-20210908104322778.png)

![image-20210908104853479](E:\MarkDown\picture\image-20210908104853479.png)

![image-20210908105045349](E:\MarkDown\picture\image-20210908105045349.png)

<img src="E:\MarkDown\picture\image-20210908105125046.png" alt="image-20210908105125046" style="zoom: 50%;" />

按元素做乘法，若遗忘门Rt接近0，则此式也接近0，则把上一时刻输入“忘掉”

![image-20210908105609268](E:\MarkDown\picture\image-20210908105609268.png)

即若Zt为1，则不改变Ht-1，相当于忽略了Xt输入的影响







## 数学基础

### 矩阵求导

分子布局和分母布局：分子布局就是分子是列向量形式，分母布局就是分母是列向量形式

分子布局：

![image-20211108231247011](E:\MarkDown\picture\image-20211108231247011.png)

分母布局：标量不变，向量拉伸；Y横向拉，X纵向拉
梯度向量形式是分母布局

![image-20211108231301187](E:\MarkDown\picture\image-20211108231301187.png)

例一

A为n * 1的列矩阵，X为n * 1的列向量，f(x)=A^T * X为标量

(A^T * X)^T=(X^T * A)

因为f(x)为标量，f(x)=A^T * X=X^T * A

此时，d f(x) / dx=A



例二





矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）：![img](E:\MarkDown\picture\8LDO48C$8@[GWU0353$FOVS.png)https://zhuanlan.zhihu.com/p/263777564
矩阵求导公式的数学推导（矩阵求导——基础篇）：![img](E:\MarkDown\picture\8LDO48C$8@[GWU0353$FOVS-1635991812088.png)https://zhuanlan.zhihu.com/p/273729929
矩阵求导公式的数学推导（矩阵求导——进阶篇）：![img](E:\MarkDown\picture\8LDO48C$8@[GWU0353$FOVS-16421607852781.png)https://zhuanlan.zhihu.com/p/288541909







***

## 机器学习

Supervised Learning：提供输入和正确的输出，即提供给机器有label的数据集，机器通过计算loss来找出loss最低的函式

Reinforcement Learning 强化学习：自监督学习需要告诉他每个输入理想的输出，而强化学习是比如让机器自己和自己或自己和别人下棋，机器根据最后是输还是赢，自己想办法，根据reward引导学习的方向。Alpha Go 是通过先自监督学习到一定程度后强化学习

Unsupervised Learning 无标注

### 监督学习

监督学习方法可以分为生成方法（generative approach）和判别方法（discriminative approach）。所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）

在监督学习中，对于数据集中的每个样本，想要进行算法预测并得到正确答案

数据集样本有对应的正确标签

#### Generative Model概率生成模型

生成方法由数据学习联合概率分布![img](E:\MarkDown\picture\974b7810d8cbe98e01ef2b6045074f20.svg)，然后求出条件概率分布![img](E:\MarkDown\picture\d1ac2b47baad1187bebce1e42a115640.svg)作为预测的模型，即生成模型：<img src="E:\MarkDown\picture\image-20211221105738799.png" alt="image-20211221105738799" style="zoom:60%;" />

典型模型有：高斯混合模型、隐马尔可夫模型、朴素贝叶斯分类器等

#### discriminative model判别模型

在机器学习领域判别模型是一种对未知数据 ![img](E:\MarkDown\picture\30b454d9858a5c23a60321d10aabcea8.svg)与已知数据 ![img](E:\MarkDown\picture\9121438210d452bb58225dd05a93d3ec.svg)之间关系进行建模的方法。判别模型是一种基于概率理论的方法。已知输入变量 ![img](E:\MarkDown\picture\9121438210d452bb58225dd05a93d3ec.svg)，判别模型通过构建条件概率分布![img](E:\MarkDown\picture\8546991c44a00c1fcc2425883de88b86.svg)预测![img](E:\MarkDown\picture\30b454d9858a5c23a60321d10aabcea8.svg)。生成模型的定义与判别模型相对应：生成模型是所有变量的全概率模型，而判别模型是在给定观测变量值前提下目标变量条件概率模型。因此生成模型能够用于模拟（即生成)模型中任意变量的分布情况，而判别模型只能根据观测变量得到目标变量的采样。

<img src="E:\MarkDown\picture\image-20211221145630020.png" alt="image-20211221145630020" style="zoom:50%;" />

由下图，通常来说判别模型得到的结果往往会更好

<img src="E:\MarkDown\picture\image-20211221145747267.png" alt="image-20211221145747267" style="zoom:50%;" />

因为生成模型往往会做了一些假设，如下图，给出的test data明显应该属于class 1
但我们用naive bayes朴素贝叶斯分布来计算，可以得到test data属于类别1的概率还不到0.5。

<img src="E:\MarkDown\picture\image-20211221150518162.png" alt="image-20211221150518162" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211221153536276.png" alt="image-20211221153536276" style="zoom:80%;" />



但生成模型在某些场景下效果也会比判别模型要更好

生成模型在概率分布的假设下，需要更少的训练数据；而判别模型根据数据说话，往往需要更大的数据量；受噪声影响更小；

### 无监督学习

样本都没有标签或都具有相同的标签

例子：无监督学习算法将数据分为多个不同的簇，即聚类算法

### 弱监督学习

弱监督学习可以分为三种典型的类型，不完全监督（Incomplete supervision），不确切监督（Inexact supervision），不精确监督（Inaccurate supervision）。

​    不完全监督是指，训练数据中只有一部分数据被给了标签，有一些数据是没有标签的。

​    不确切监督是指，训练数据只给出了粗粒度标签。我们可以把输入想象成一个包，这个包里面有一些示例，我们只知道这个包的标签，Y或N，但是我们不知道每个示例的标签。

​    不精确监督是指，给出的标签不总是正确的，比如本来应该是Y的标签被错误标记成了N。

### 强化学习

强化学习不要求预先给定任何数据，而是通过接收环境对动作的奖励（反馈）获得学习信息并更新模型参数

### online algorithm在线算法、offline离线算法

![image-20211220213103477](E:\MarkDown\picture\image-20211220213103477.png)

![image-20211220213146538](E:\MarkDown\picture\image-20211220213146538.png)

![image-20211220213218358](E:\MarkDown\picture\image-20211220213218358.png)

常用离线算法

![image-20211220213233644](E:\MarkDown\picture\image-20211220213233644.png)



### variance和bias

bias:平均的f离真实的f的距离
variance:当次的f离平均f的距离

<img src="E:\MarkDown\picture\image-20211220161248755.png" alt="image-20211220161248755" style="zoom:50%;" />

<img src="E:\MarkDown\picture\image-20211220161756983.png" alt="image-20211220161756983" style="zoom:50%;" />

越复杂的function，variance越大，bias越小

error来自于bias：underfitting，train loss大，应该让模型更复杂
error来自于variance：overfitting，train loss小但test loss大，应该增加数据量或正则化

### classification和regression

回归问题：目标是预测一个连续值的输出
分类问题：预测离散值输出

如将二分类看作回归问题。像左图就是预测的比较好的。但如果像右图，最右下角的错误的点，会让回归模型为了减少error而发生偏移，这样分类就会发生错误

![image-20211221100715089](E:\MarkDown\picture\image-20211221100715089.png)

### 逻辑回归和线性回归

线性回归：线性回归的目的就是找到和样本拟合程度最佳的线性模型，解决回归问题
逻辑回归：将线性回归应用到分类问题中，即在线性回归基础上加上了个sigmoid函数，使得输出为0-1

![image-20211221144723658](E:\MarkDown\picture\image-20211221144723658.png)![image-20211221145136646](E:\MarkDown\picture\image-20211221145136646.png)

单纯的逻辑回归，无法处理下图左边的问题，存在限制。可以通过feature transformation的方法解决

<img src="E:\MarkDown\picture\image-20211221160008313.png" alt="image-20211221160008313" style="zoom:60%;" />

但这个transformation是人工找到的，我们需要一种方法让他自己学习出来
<img src="E:\MarkDown\picture\image-20211221160251454.png" alt="image-20211221160251454" style="zoom:67%;" />

这就是神经网络
<img src="E:\MarkDown\picture\image-20211221160514427.png" alt="image-20211221160514427" style="zoom:50%;" />

### backpropagation反向传播

<img src="E:\MarkDown\picture\image-20211221185511562.png" alt="image-20211221185511562" style="zoom:80%;" />

<img src="E:\MarkDown\picture\image-20211221190822018.png" alt="image-20211221190822018" style="zoom:50%;" />

### deep learning

模组化的思想。deep的影响带来的不是参数量的增加，同参数量下多层的效果往往比单层要好。其思想是在前一模组的基础上进行分类等工作，更高效的使用参数。如下图，仅有长发男的数据比较少，若只是一层网络来分类的话，对于此类的效果就会比较差。但如果是多层网络，就是先通过前一层来进行男女或长短发的基本分类，在进行细致分类，准确性就会更高

<img src="E:\MarkDown\picture\image-20211223162354446.png" alt="image-20211223162354446" style="zoom:50%;" />









### BN层

[NFNet，不需要bn层](https://blog.csdn.net/zhouchen1998/article/details/113824617)

### FC：全连接层**（**fully connected layers，FC）

在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽
目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能

### 全连接神经网络Full Connect Neural Network

每一层的每一个节点都与上下层节点全部连接，这种神经网络称作全连接网络

同MLP

### MLP多层感知机

多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构。多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。 

MLP相当于多层全连接层的组合，通常采用三层网络，即输入层、隐含层、输出层

### DNN

MLP的发展，MLP是最简单的DNN。本质没有区别，就是特指网络层数比较深

### CNN卷积神经网络

深度卷积神经网络(deep convolutional neural networks, CNNs)

[CNN网络](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_40519315/article/details/104388980)在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。是分类网络。

![img](E:\MarkDown\picture\v2-c85eab0d36395872ac8f5538d0a57994_1440w.jpg)

### Graph Neural Networks

aggregate合计：如图中要计算h30的下一层h31，则需要依照黄色圈中的四个相邻数据
![image-20211223203521194](E:\MarkDown\picture\image-20211223203521194.png)

![image-20211223204027727](E:\MarkDown\picture\image-20211223204027727.png)

![image-20211223204109657](E:\MarkDown\picture\image-20211223204109657.png)



黄色点是距离h30为2的点，左边那个比较好理解，右边那个为h30先向别处移动一下，再移动回去，就为原点

![image-20211223204438775](E:\MarkDown\picture\image-20211223204438775.png)
![image-20211223204712021](E:\MarkDown\picture\image-20211223204712021.png)



### FCN

Fully Convolutional Networks for Semantic Segmentation

FCN中用卷积层替换了CNN中的全连接层，这样网络输出不再是类别而是 heatmap。不含全连接层(fc)的全卷积(fully conv)网络，可适应任意尺寸输入。是分割网络。

- 用全卷积网络代替全连接层，可适应任意尺寸输入

- 增大数据尺寸的反卷积层(上采样)

- 结合不同深度层结果的跳级结构(skip Layer)

  网络结构详图：输入可为任意尺寸图像彩色图像；输出与输入尺寸相同，深度为：20类目标+背景=21类。

  ![https://inews.gtimg.com/newsapp_bt/0/13266268722/1000](E:\MarkDown\picture\1000)

![img](E:\MarkDown\picture\v2-789a131785f8f37a0fd199f9d4a502da_1440w.jpg)

**缺点**：

1. 分割的结果不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。
2. 因为模型是基于CNN改进而来，即便是用卷积替换全连接, 但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系.忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。

### Segnet

是继FCN后的又一次改进

![img](E:\MarkDown\picture\v2-3d9b8ff2d9d5d1d7e56414bc3b497bd6_1440w.jpg)

特点：
做最大池化时，Segnet会记录最大像素的index，以利于后面的上采样。上采样过程中，如果放置最大像素值的位置错误，就会导致最后的loss增加，性能就会降低

<img src="E:\MarkDown\picture\v2-a8242915c74c6ccb8a8e70137f0269d0_1440w.jpg" alt="img" style="zoom:200%;" />

​	解码器输入的是pooling  indices，上采样时将值回归到它们原始的位置，其余的用0补全，此时得到的上采样图是稀疏的，然后该稀疏的特征图要可训练的卷积核进行卷积，才能得到密集化的feature map。上采样使用池化层索引的优势有：1）提升边缘刻划度；2）减少训练的参数；3）这种模式可以包含在任意编码-解码的网络中。

### high-level feature和low-level feature

字面意思理解，就是高级特征和低级特征

在编码过程中，feature的等级逐渐升高，比如进行很浅的几次卷积生成的特征图，可以称为low-level，例如一张图片经过Sobel 边缘检测算子得到的具有大量边缘信息的特征图。是一些表面，表观的信息，比如纹理、边缘、颜色

![image-20210921204038022](E:\MarkDown\picture\image-20210921204038022.png)

high-level feature就是多次滤波后得到的特征图，是深层的核心特征，是高级的语义信息

引入low-level的特征就会弥补很多细节信息。



### Kmeans

[详细](https://zhuanlan.zhihu.com/p/184686598)

[一般](https://zhuanlan.zhihu.com/p/78798251#:~:text=K-means,%E6%98%AF%E6%88%91%E4%BB%AC%E6%9C%80%E5%B8%B8%E7%94%A8%E7%9A%84%E5%9F%BA%E4%BA%8E%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%EF%BC%8C%E5%85%B6%E8%AE%A4%E4%B8%BA%E4%B8%A4%E4%B8%AA%E7%9B%AE%E6%A0%87%E7%9A%84%E8%B7%9D%E7%A6%BB%E8%B6%8A%E8%BF%91%EF%BC%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%B6%8A%E5%A4%A7%E3%80%82)

### Ground True

    机器学习包括有监督学习(supervised learning)、无监督学习(unsupervised learning)、半监督学习（semi-supervised learning）
    有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注。正确的t标注是ground truth， 错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。模型函数的数据则是由(x, y)的形式出现的。其中x为之前的输入数据，y为模型预测的值。标注会和模型预测的结果作比较。在损耗函数(loss function / error function)中会将y 和 t 作比较，从而计算损耗(loss / error)。
    如果标注数据不是ground truth，那么loss的计算将会产生误差，从而影响到模型质量。另外，标记数据还被用来更新权重，错误标记的数据会导致权重更新错误。因此使用高质量的数据是很有必要的。

Ground Truth： 就是指正确打标签的训练数据 或 简单来说就是有效的正确的数据
在图像中，比如在一些抠图的项目中，很多人就把Alpha图叫做Ground Truth，Alpha就可以理解成是输入的原始图片对应的Alpha图，也就是原始图对应的标签，或者说是给原始图片用Alpha打了一个标签，而正确的对应于原图的Alpha图就是 Ground Truth 。
前景蒙版（alpha matte）：也称前景透明度或透明度蒙版，是前背景分离的结果，是一个灰度图，每一个像素点的灰度值表示原始图像每个像素属于前景物体的程度，即后文提到的alpha ，白色代表某一个像素确定属于前景，黑色代表某一个像素确定属于背景。

### 正负样本

图像分割中，正样本表示要分割的东西，负样本为背景

### 稀疏激活性

神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。

### 稀疏表示原理

把高维数据用低维度表示。比如，描述一个人现在的位置，东经多少，北纬多少，海拔多少，三个维度。但若换一个说法，科技楼实验室里，就是一个维度。参照物变了，也就是所谓的基变了，维度也就变了。这里的字典就是这个基。字典就是一个矩阵（n维），这个矩阵比之前的的高维数据（k维）的维度要低得多，即n<<k。数据对象y可以表示成y=a1x1+…+anxn。其中，xi是字典的列向量，ai是一个线性组合，称之为稀疏表示系数，整个ai构成的矩阵记为A。所谓的稀疏表示，其实就是求这个系数的矩阵。为了实现稀疏，系数矩阵的很多值，都是0。

 

#### 过完备

首先，Dictionary是一种sparse representation（稀疏表示）的模型。如果字典D中的元素恰能够张成n维的欧式空间，则字典D是完备的。如果m>>n，字典D是冗余的，若同时保证还能张成n维的欧式空间，则字典D是过完备的。比如你的数据是y，字典是D，y=Dx，其中x是稀疏的。如果你的D是方阵或者长方形矩阵（正交矩阵除外），字典很可能是不确定的。相反的，如果你的D是个冗余矩阵，你拥有多于必要的列来表达这个数据（冗余）。这样的字典称为Overcomplete dictionary，这种字典的优势是更有利于表达highly diversified（高度多元化）的数据（图像）。



### 小样本机器学习算法

​	针对数据稀缺的挑战，瞄准解决现有学习方法过于依赖大量标注样本、对环境适应能力差且跨任务迁移能力弱以及实时性的问题，**面向自主智能感知任务，重点突破基于知识和小样本的快速学习与认知理论，突破核心数据与信息提取方法，为小目标精准识别优化提供理论基础。**







小样本学习和元学习

[B站小样本学习和元学习](https://www.bilibili.com/video/BV1Et4y1i7pu?from=search&seid=16123421030133588156) Few-Shot Learning (小样本学习) 和 Meta-Learning (元学习)的基本概念

分辨事物的异同

Support Set

训练集规模很大，而Support Set很小，每个类下只有一张或几张图片，在做预测的时候提供一些额外的信息
k-way：Support Set有k个类别
n-shot：每个类别有n个样本

**Meta-Learning元学习**

learn to learn，让机器自己学会学习，学会区分不同的事物，分辨异同，而不是样本是什么。通过Query和Support Set对比异同

**Few-Shot Learning小样本学习** 

是元学习的一种
训练集里可能没有某个类别，但可以通过对比Query和Support Set来找到Query的类别



通过函数判断相似度，通过相似度函数来分类

![image-20210702212600166](E:\MarkDown\picture\image-20210702212600166.png)



**Siamese Network**

在大数据集上训练

训练方法一：

![image-20210702213833102](E:\MarkDown\picture\image-20210702213833102.png)



训练方法二：Triplet Loss

![](E:\MarkDown\picture\image-20210702214047174.png)
![image-20210702214315104](E:\MarkDown\picture\image-20210702214315104.png)





不要降采样之类的

可以空洞卷积



调研可行性和指标之类

一、度量学习？
两个分支的孪生网络，基于匹配的思想

二、元学习
啊

三、微调？



小目标检测

样本少量，看能否用在yolo，efishent上



## 指标

### 语义分割评价指标

[PA、CPA、MPA、IoU、MIoU详细总结和代码实现](https://blog.csdn.net/sinat_29047129/article/details/103642140)

### 模型复杂度FLOPs、Params

1、模型复杂度的衡量

![image-20210819160827409](E:\MarkDown\picture\image-20210819160827409.png)

2、硬件性能的衡量

![image-20210819160842138](E:\MarkDown\picture\image-20210819160842138.png)

3、模型复杂度的计算公式

![image-20210819160857431](E:\MarkDown\picture\image-20210819160857431.png)

### mAP

越大越好

* mean Average Precision, 即各类别AP的平均值
* AP: PR曲线(Precision-Recall曲线)下面积
* Precision查准率: TP / (TP + FP)=TP/所有预测框数量，在你认为的正样本中， 有多大比例真的是正样本
* Recall查全率: TP / (TP + FN)=TP/所有GT数量，在真正的正样本中， 有多少被你找到了
* TP: True Positive，IoU>0.5的检测框数量（同一Ground Truth只计算一次）
* FP: False Positive，IoU<=0.5的检测框，或者是检测到同一个GT的多余检测框的数量
* FN: False Negative，没有检测到的GT的数量

![image-20210819210107080](E:\MarkDown\picture\image-20210819210107080.png)

交并比Intersection Over Union (IOU)
度量两个检测框（对于目标检测来说）的交叠程度
![image-20210819210307190](E:\MarkDown\picture\image-20210819210307190.png)

B_gt 代表的是目标实际的边框（Ground Truth，GT），B_p 代表的是预测的边框，通过计算这两者的 IOU，可以判断预测的检测框是否符合条件，IOU 用图片展示如下：
![image-20210819210340008](E:\MarkDown\picture\image-20210819210340008.png)



### miou

n_ii 是类别i被预测为i的像素个数，反应在图中就是交集

t_i为i的像素个数，而n_ji为类别j被预测为i的像素个数，其和为交集加重合的部分，因此减去n_ii就为并集

![image-20210912161834940](E:\MarkDown\picture\image-20210912161834940.png)



![](E:\MarkDown\picture\image-20210912162157944.png)

![image-20210912162346886](E:\MarkDown\picture\image-20210912162346886.png)

mean Accuracy：五个每个类别的acc相加除5

<img src="E:\MarkDown\picture\image-20210912162257547.png" alt="image-20210912162257547" style="zoom:50%;" />

mIoU：各类别IoU相加除类别数

<img src="E:\MarkDown\picture\image-20210912162515526.png" alt="image-20210912162515526" style="zoom: 50%;" />



## 数据集

### 人体姿态估计

**1. LSP数据集**

Leeds Sports Pose 是由利兹大学计算机学院于 2010  年发布的一个体育姿势数据集。被分为竞技、羽毛球、棒球、体操、跑酷、足球、排球和网球几类，共包含约 2000 个姿势注释，图像均来自于  Flickr 。通过对图像进行缩放，聚焦的人物长度约 150  像素，并且每个图像都带有14个关节位置的注释，其左侧和右侧关节始终“以本人的视角”进行标记。

![preview](E:\MarkDown\picture\v2-5eb79c9b5d8ca1be7f632a2ebe666f5a_r.jpg)

数据集标注情况，截取自Graviti数据可视化功能

**数据大小：**33.78MB

https://gas.graviti.cn/dataset/data-decorators/LeedsSportsPose



**2. FLIC数据集**

FLIC（Frames Labeled In  Cinema）数据集由宾夕法尼亚大学-工程与应用科学学院 GRASP 实验室于 2013  年发布。该数据集是从好莱坞流行电影中自动收集的图像数据集，包含5003张图像。这些图像截取了30部电影的整十倍的帧数，通过最先进的人物检测器获得。每个图像由五个人标注10个上半身关节。此外，图像中拥有 5 个中值标记以保证异常值注释具有鲁棒性。最后，如果该人被遮挡或严重不正面，将被手动删除。

**数据集大小：**1.38GB

https://gas.graviti.cn/dataset/data-decorators/FLIC

**3. MPII Human Pose数据集**

MPII Human Pose人体姿势数据集是人体姿势预估的一个  benchmark。数据集中包含约25000张标注图像，标注人数超过 4万人，涵盖了410种人类活动。这些图像是从 YouTube video  中抽取出来的。此外，在测试集中还收录了身体部位遮挡、3D 躯干、头部方向的标注。

**数据集大小：**11.26GB

https://gas.graviti.cn/dataset/shannont/MPIIHumanPose







### 语义分割常见数据集格式

标注工具：labelme

EISeg：半自动，可以通过模型自动标注一些常见的类别

![image-20210912161022425](E:\MarkDown\picture\image-20210912161022425.png)

调色板模式，图像仍为单通道

![image-20210912161349515](E:\MarkDown\picture\image-20210912161349515.png)

没有边缘信息，需要将多边形坐标解码成标签形式

### 常用数据集

#### Pasca VOC

- 20个类别
- 6929张标注图片
- 可以做分类和语义、实例分割



#### CityScapes

- 道路驾驶场景
- 30个类别
- 5000图片有精细的标注
- 20000图片有粗糙的标注

- Both class-level and instance-level



#### MS COCO

- 80类
- 33万张图片，超过20万张有标注
- 150万个物体的个体
- instance level





## 目标检测

（1）**two-stage方法**，如R-CNN系算法，其主要思路是先通过启发式方法（selective search）或者CNN网络（RPN)产生一系列稀疏的候选框，然后对这些候选框进行分类与回归，two-stage方法的优势是准确度高；
（2）**one-stage方法**，如Yolo和SSD，其主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步，所以其优势是速度快，但是均匀的密集采样的一个重要缺点是训练比较困难，这主要是因为正样本与负样本（背景）极其不均衡（参见[Focal Loss](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.02002)），导致模型准确度稍低。

### 传统目标检测

![image-20210820185231712](E:\MarkDown\picture\image-20210820185231712.png)

主要分为两个步骤：训练+预测，其中训练主要是用来得到分类器，比如SVM,预测就是使用训练好的分类器对图像中的滑动窗口进行特征提取然后分类，最后得到检测的结果。下面以人脸检测为例：

![image-20210820185356804](E:\MarkDown\picture\image-20210820185356804.png)

![image-20210820185419590](E:\MarkDown\picture\image-20210820185419590.png)

预测阶段有两种滑动窗口策略：

1. 策略1：使用不同大小的滑动窗口，对每个滑动窗口提取特征并分类判断是否是人脸，最后经过NMS得到最后的检测结果，本文的SSD本质上就是这种策略，不同检测层的anchor就类似于不同大小的滑动窗口
2. 策略2：构造图像金字塔，只使用一种大小的滑动窗口在所有金字塔图像上滑动，对每个滑动窗口提取特征并分类判断是否是人脸，最后经过NMS得到最后的检测结果，MTCNN就是采用了这种策略





Single Shot MultiBox Detector,Single shot指明了SSD算法属于one-stage方法，MultiBox指明了SSD是多框预测。



### 先验框Anchor

[1](https://blog.csdn.net/weixin_42392454/article/details/110950290?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163102116216780269867279%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163102116216780269867279&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-110950290.pc_search_ecpm_flag&utm_term=%E5%85%88%E9%AA%8C%E6%A1%86&spm=1018.2226.3001.4187)

### 非极大值抑制(Non-Maximum Suppression，NMS)

抑制不是极大值的元素，可以理解为局部最大搜索

目标检测中提取分数最高的窗口。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。

### one-stage和two-stage,MS和SS

one-stage目标检测算法（也称one-shot object detectors），其特点是一步到位，速度相对较快。另外一类目标检测算法是two-stage的，如[Faster R-CNN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.01497)算法先生成候选框（region  proposals，可能包含物体的区域），然后再对每个候选框进行分类（也会修正位置）。这类算法相对就慢，因为它需要多次运行检测和分类流程。而one-stage检测方法，仅仅需要送入网络一次就可以预测出所有的边界框，因而速度较快，非常适合移动端。最典型的one-stage检测算法包括[YOLO](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.08242)，[SSD](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.02325)，[SqueezeDet](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1612.01051)以及[DetectNet](https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/detectnet-deep-neural-network-object-detection-digits/)。

### 多尺度训练和测试

Multi-scale Training和Mult-scale Test    目标检测里的多尺度技术

#### 1. One Stage网络的Multi-scale Training。

每sample一个batch的数据喂入网络中训练时，都会先对采样出的数据resize+padding到一个随机大小。而这个大小的波动范围是预设好的。我们来看一下yolov2是怎么描述多尺度训练的：

每十个batch，网络随机选择一个新的图片尺寸。因为我们的模型下采样翻了32倍，我们选择32的几个倍数{320，352，....，608}，即最小320* 320，最大608 *608作为后面的训练尺度

即训练的时候用不同的尺度去帮助模型适应各种大小的目标，获得对尺寸鲁棒性。

多尺度训练一个明显的好处是：**不增加推理时间。**所以不管是业务还是竞赛，大胆上多尺度训练就对了。还有一些值得一说的地方：

1). 最后输出是全连接的网络不能用多尺度。原因很简单，尺度变了，最后一层的权重数量就对不上输入数量了。比如yolov1和带有flatten接fc的分类网络(如VGG)。

2). 多尺度训练，在合理范围内扩大多尺度的范围，是能获得更高收益的。比如上文中FCOS的短边范围[640, 800]。这个域扩充到[480, 960]，还可以获得非常可观的收益。当然什么是合理的范围，需要各位自己去把控品味了。

#### 2. One Stage网络的Multi-scale Testing。

![image-20210910195242855](E:\MarkDown\picture\image-20210910195242855.png)

多尺度测试分为3步：
Step1. 各自尺度进行单尺度测试
Step2.  把所有尺度归一化到同一个尺度，如图，蓝色预测和绿色预测的框都是在不同大小的图片下出的，这样所有结果放到一起NMS是对不上的。所以，图里为了表示明白，把蓝色和绿色的预测框都按照图片相对橙色的大小比例进行缩放。Step3. 缩放对齐后的结果放到一起做NMS。

#### 3.Two Stage网络的Multi-scale Training。

Two Stage的代表作是Faster R-CNN。不过多尺度训练不分One Stage和Two Stage，做法是一样的。

#### 4.Two Stage网络的Multi-scale Testing

![image-20210910195834938](E:\MarkDown\picture\image-20210910195834938.png)

首先，不同尺度的图，通过Backbone+RPN和各自的NMS之后，会得到各自的proposals。这里对proposals的处理和one  stage对结果的处理一样，要先把尺度统一到同一张图的大小上去，比如这里都统一Resize到深蓝色图的大小。然后合并到一起做阈值为0.7的NMS，得到Proposals。这时候的proposals的尺寸是相对于深蓝色图的大小，R-CNN阶段我们依然希望用多尺度，所以需要把proposals分别resize到橙色和绿色的图的尺寸上去，然后各自过R-CNN。后面的步骤与RPN和one stage是一样的，先各自做NMS，然后Resize到统一尺寸后再合并做阈值为0.5的NMS。

## 图像分类

默认技巧：
softlabel



## 图像分割

[图像分割炼丹的「奇技淫巧」](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650786169&idx=3&sn=78daf0132ed86258bca062d69688788a&chksm=871a0d07b06d8411fe2f92abf753f6737acab1cc2119883fb1fad5b370178e2ec728968c7823&scene=21#wechat_redirect)



## 语义分割网络的基础实现

[语义分割预处理与后处理方法](https://blog.csdn.net/weixin_43162240/article/details/105137693#comments_16943083)

[用pytorch创建自己的数据集](https://blog.csdn.net/qq_36107350/article/details/89029044)

[FCN模型实现-Pytorch+预训练VGG16](https://blog.csdn.net/qq_41685265/article/details/104919668)



### 语义分割经典网络

#### SegNet 和 DeconvNet

DeconvNet的上采样方法和SegNet一样，只是网络结构多了两个全连接层



### 实例分割经典网络

#### Mask RCNN

mask rcnn是集目标检测，分类，分割于一体的网络，在进行目标检测的同时进行实例分割，他的分割是将已经通过box框出的目标进行分割，物体单一，背景不复杂。现代的实例分割方法主要是**先检测对象边界框，然后进行裁剪和分割**，Mask R-CNN 推广了这种方法。他使用残差网络作为卷积结构；增加FCN用于语义分割(针对于目标框里的图像进行FCN的操作)

实例分割的代表性网络，基于faster-RCNN改进，在其基础上增加了一个分支进行语义分割

![961D424BCFBCC4256835D38D8D81F63E7652FD47_size50_w764_h467](E:\MarkDown\picture\961D424BCFBCC4256835D38D8D81F63E7652FD47_size50_w764_h467.jpeg)

图片首先为防止失真，在图像边缘加上灰条，变成正方形的大小，再经CNN这一主干网络进行特征提取，传入region proposal网络即RPN得到一个建议框，在有效特征层上进行截取，把这些可能含有物体的区域选取出来就可以了，这些被选取出来的Region又叫做ROI （Region of Interests），即感兴趣的区域。然后在ROI Align层进行resize到相同的固定大小，再进入FC Layers全连接层进行后续操作。Mask RCNN则多了一个分支进入语义分割网络



### 传统抠图方法

基于深度学习的抠图方法通过编解码和神经网络，可以提取到图像高级语义特征和纹理特征

#### 绿幕抠图：

#### [色差抠图](https://blog.csdn.net/sD7O95O/article/details/111398631?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%BB%BF%E5%B1%8F%E6%8A%A0%E5%9B%BE%E7%BC%BA%E7%82%B9&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-8-111398631.pc_search_result_no_baidu_js&spm=1018.2226.3001.4187)

C#语言在.NET平台下的OpenCVSharp库

原理：色差抠图，绿色部分渲染为黑色，其他部分渲染为白色生成mask图

因为环境光照的影响，背景绿幕中的各个点颜色并不完全相同，所以这里使用像素点的green == max  (blue,green,red)&& green >  30是否为true来判断一个点是否是绿色，30是一个阈值，可以根据情况来调节识别效果。然后找到图中的若干个轮廓信息。为了去掉一些绿幕中的褶皱或者光线问题造成的小面积干扰，对于找到的轮廓信息，需要删除掉面积较小的轮廓，只保留面积较大的轮廓

缺点：精细度不够，且前景中不能有绿色——绿幕抠图效果不好的，换个蓝屏抠啊

#### [α抠图，不用色差抠图](https://blog.csdn.net/cheapter/article/details/85622721?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161820424416780262592540%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161820424416780262592540&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-1-85622721.pc_search_result_no_baidu_js&utm_term=%E7%BB%BF%E5%B1%8F%E6%8A%A0%E5%9B%BE%E7%BC%BA%E7%82%B9&spm=1018.2226.3001.4187)

采用了基于图的共享采样方式，有效改善了合成视频边缘的“绿边”和锯齿状现象

上一个方法的色差法，根据颜色滤除背景生成mask，边缘分割明显，前景细节丢失，绿色溢出较为严重。

这个方法把前景图像从RGB空间转换到HSV空间中，然后对图像二值化进行轮廓寻找，接着进行形态学操作后进行均值滤波，生成trimap图。在边缘处计算α对图像进行加权叠加。

问题：

1.与绿屏中的前景边缘过的细节处理的不是太好

2.因为是通过图像处理生成的trimap，生成图像不合适的话会对结果造成较大的影响。所以在结果不满意的时候需要调整trimap的获取过程，一般通过形态学操作以及改变滤波器的核大小进行调整。









## 深度学习方向

3维重建              

3D点云

对抗攻击

OCR

无监督生成学习：最近的denoising  diffusion probabilistic model(DDPM)绝对是热坑，效果好，但是速度慢没有meaningful latent  space限制了很多应用，有待发掘。我去年实习写了一篇DiffWave是这个方法在语音上的应用，效果很好，最近应该能看到这个模型的application井喷，比如3D point cloud生成。

姿态估计 Pose Estimation
姿态估计方向主要包含：人体姿态估计(2D/3D)、人体姿态预测(视频)、手势姿态估计、头部姿态估计、动物姿态估计等等。其中人体姿态估计为比较热门的研究方向，主要可以用于人体行为判断、AR 试衣、自动驾驶(预测路人行动)等领域。其主要实现的方式是通过检测人体关键点来完成人体的动作、行为识别。

视觉SLAM
https://zhuanlan.zhihu.com/p/170735395

图像融合
https://zhuanlan.zhihu.com/p/372630355

图像检索 Image Retrieval
图像检索是一种用于从大型数字图像数据库中浏览，搜索和检索图像的一种技术。常规的图像检索有基于文本的检索、基于内容的检索以及基于语义的检索。其中基于语义的检索由于其需要对海量的图片进行语义级别的标注，不仅主观性强而且费时费力，同时其语义也很难全面的表达图像中所包含的所有信息，因此实际中很少实现。而基于内容的检索，也称为 CBIR 技术，常见的应用场景有 “以图搜图”，在实际中被广泛使用。

图像去噪 Image Denoising
去噪方向主要包含：去噪、图片去噪。由于数字设备常受到相机抖动、运动的物体、暗光和噪声等影响而导致捕获的照片“不干净”。因此噪技术具有很大的应用价值。传统噪方法有：利用非局部相似性、字典学习、MRF、WNNM 等；现代方法主要是基于深度学习技术：栈式稀疏去噪自编码器、多层感知机、卷积编解码网络、深层神经网络等。相对于前者，后者是一种端对端的训练方式，无需手动调整参数，拥有更强的学习能力。



图像去雾 Image Dehazing
图像去雾的目的是消除雾霾环境对图像质量的影响，增加图像的可视度。传统的图像去雾方法主要有暗通道先验（DCP） 方法，最大对比度（MC）方法，颜色衰减先验（CAP）方法，色度不一致方法，其中以何凯明的暗通道先验方法最为著名；现代深度学习图像去雾方法主要分为两种：一种是基于大气退化模型，另一种则是训练一种端到端的图像去雾模型！其中端到端方式已成为深度学习中的主流去雾模型。



图像去雨Image Deraining
图像去雨是从包含雨水的图像生成去除雨水的图像。早期的去雨方法主要包含稀疏编码和 GMM 方法。现代基于深度学习的去雨方法绝大部分使用：全监督方法，其采用多阶段的方式或 encoder-decoder 的架构，用全卷积学习雨图到无雨图的映射或残差来训练模型。



行人重识别 Person Re-ID

行人重识别研究研究不同于目标识别，它能够实现跨越时间和空间对目标人体（人群）进行跟踪、匹配与身份鉴定，这是近年来计算机视觉的研究热点之一。因此，行人重识别技术需要分析目标的空间依赖关系，还需要分析目标变化的历史信息。行为识别涉及到技术主要包含兴趣点提取，密集轨迹，光流和表观并举，3D 卷积网络，LSTM 和 GCN 等。



 **缺陷检测** **Defect detection**

缺陷检测，在工业上应用非常广泛，如电路板表面缺陷检测、金属零部件表面缺陷检测、布匹检测、固件缺陷检测、混领土裂缝检测、公路裂缝检测等。传统的基于机器视觉的算法很难对缺陷特征进行完整的识别，而且通常会耗费大量精力，得不偿失。由于卷积神经网络在特征提取业的强大能力以及目标检测算法日趋成熟的背景下，使得业界普遍将度学习技技术应用到陷检测领域当中。

**视频理解 Video Understanding**视频理解，主要是基于视频中的时序信息来进行视频分析。相对于图像而言，视频多了一维时序信息，其应用场景相对也比较广泛，比如在智能安防领域中我们可以使用视频理解技术来取代人工进行相应的视频监控。







​        











