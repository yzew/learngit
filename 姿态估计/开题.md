AR游戏。最早的时候需要体感手柄之类的传感器，现在不知道能不能只基于 RGB 摄像头了。另外“姿势监测/矫正”也是一大类应用场景，诸如坐姿监测、健身姿势矫等等。

[深度学习的多个loss如何平衡](https://www.zhihu.com/question/375794498)

## 回归和heatmap

> 可不可以将两者进行结合？回归方法有各部位位置的先验，可以用来指导heatmap方法，让其  

​	回归方法由于全连接层很容易过拟合，因此能更多地记住关键点之间的相对位置，并且由于是一次计算回归出全部的点，因此这个相对位置也是全局的，这种特性对于人头这种刚体是有利的，因为点与点之间具有更多的相对运动关系(比如转一下头，几乎所有的人脸关键点都会因此发生运动)。
​	而反观人体，点与点之间的相对关系是更解耦的，只有关节相连的部分会有一定的相对运动关系(动左手的时候并不会影响右手)，这个时候全连接层的这种特性就是有害的。
​	热图方法因为更多是受空间和局部特征影响，因此更适合于人体这种解耦的情况，相当于模型的各个卷积核各司其职，只关心自己要预测的那个点，能更少地受影响。
​	不过以上都只是相对而言的，毕竟人脸也不是所有点都是耦合的，我张嘴并不会导致眉毛眼睛运动，所以回归方法用在人脸上很容易出现的一个问题是表情细节的捕捉能力较差，如果模型或者数据量不够，往往会由于过拟合导致关键点无法匹配到咧嘴、眨眼、挑眉之类的细微动作，要改善也往往依赖于定向采集数据，在这方面热图方法则需要的数据量会小，而且有几率通过泛化学到(即训练数据没有挑眉，但关键点依然能追得很准)。
​	所以总体来说，哪些关键点存在耦合，哪些是解耦的，这方面如果数据量不够大，通过先验知识来
加入是更高效的，也因此人脸大部分点存在耦合，所以用回归方法能有效降低大部分点位的误差，使得最终的metric点数变好，毕竟metric只是一个平均值，但像局部的这种拟合不足问题就无法从metric平均值上反映出来，需要我们做更细的点位划分，这其实也是在人工引入先验知识了。

## 编解码的误差分析

编码过程中也存在量化误差，由于量化误差，以上述方式生成的热图是不准确和有偏差的（图4）。这可能会引入次优监督信号DARK：无偏sub-pixel亚像素中心坐标编码

![image-20221202225806053](E:\MarkDown\picture\image-20221202225806053.png)

解码是对低分辨率下的概率图中得到坐标，映射到高分辨率的原图中。此时存在一个问题，即高分辨率上的gt坐标点，可能不是刚好落在低分辨率的概率图的像素点上，而是可能在亚像素位置。这就导致你用低分辨率的概率图取最大值，再上采样得到的坐标，这个**分辨率恢复的过程会引入量化误差**，可能存在偏移，这会导致解码不准确，预测概率图中的最大激活点不对应于关节在原始坐标空间中的准确位置。因此往往通过次最大点补偿等方法补偿这个亚像素级偏移，但这种补偿也缺乏可解释性。

DARK的方法与补偿方法相比要好得多，利用了分布信息来预测亚像素的准确位置





## Dark

1. lib/core/function.py Line:166 get_final_preds()   is decode function.
2. BLUR_KERNEL is 11 for the input size of 256x192
3. 

概率图转换为坐标的过程中，最后的预测被认为是激活最大的位置（通过argmax求得）。称这个过程为坐标解码，从热图到坐标。

认为解码的坐标

通过m来近似μ

因此通过基于泰勒展开的分布近似来综合考虑概率图的分布信息，



传统argmax方法：

```python
 for idx_joint in range(num_joints):
	img = imgHat[idx_joint]
	# get predicted keypoint coordinate (x, y)
	M = img.argmax()
	xHat, yHat = M/img.size(1), M%img.size(1)
```

比如关节点热力图imgHat: 17x64x48 (HRNet),17个关节点,每个关节对应一张64x48热力图.

对于每张热力图,通过argmax()函数获取最大值在抻平的坐标M, 通过size(1)获取每行多少个元素. 关节点所在的x,y分别是二者的**除数**和**余数**.



传统的softmax方法：

softmax=<img src="E:\MarkDown\picture\image-20221202215708862.png" alt="image-20221202215708862" style="zoom:47%;" />

torch

```python
torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)

data=torch.FloatTensor([[1.0,2.0,3.0],[4.0,6.0,8.0]])
prob = F.softmax(data,dim=0) # dim = 0,在列上进行Softmax;dim=1,在行上进行Softmax

output:
tensor([[1., 2., 3.],
        [4., 6., 8.]])
tensor([[0.0474, 0.0180, 0.0067],
        [0.9526, 0.9820, 0.9933]])
```

argmax方法

```python
torch.argmax(input, dim=None, keepdim=False) # 返回指定维度最大值的序号

a = torch.tensor(
              [
                  [1, 5, 5, 2],
                  [9, -6, 2, 8],
                  [-3, 7, -9, 1]
              ])
b = torch.argmax(a, dim=0)

tensor([1, 2, 0, 1])
torch.Size([3, 4])
```





2、深度卷积：深度卷积的引入，使得参数量增加了0.1M（兆），AP却能有4.04的提升

通过same padding使得经过卷积后的图像分辨率不变

**M****和****MB****的换算关系：**

比如说我有一个模型参数量是1M，在一般的深度学习框架中(比如说PyTorch)，一般是32位存储。32位存储的意思就是1个参数用32个bit来存储。那么这个拥有1M参数量的模型所需要的存储空间的大小即为：1M * 32 bit = 32Mb = 4MB。因为1 Byte = 8 bit。现在的quantization技术就是减少参数量所占的位数：比如我用8位存储，那么：所需要的存储空间的大小即为：1M * 8 bit = 8Mb = 1MB。



为什么训练慢？

1、常用的卷积做过充分的优化，这个transformer大量的元素乘矩阵乘的。而且这个transformer block加的太多了。

2、显存分析

1B（字节）=8b（位）
1 KB = 1024 B
1 MB = 1024 KB

我们这个Q×K显存为0.25M×num_heads

![image-20221202204402248](E:\MarkDown\picture\image-20221202204402248.png)

现在一些网络如[大白话Pyramid Vision Transformer](https://zhuanlan.zhihu.com/p/353222035)、CeiT改善了这个



答辩PPT的介绍顺序有些乱，没有突出自己的创新点

​	之后会改进

你第一章这个在HRNet上对误差的改进，对网络的参数量之类的影响大么

​	因为更改的主要是图像转换的方法以及解码的方法，都是在公式上的改进，因此对网络的参数量几乎没有影响

为什么会想到增加这个精修模块？

​	现在的方法主要通过隐式的学习人体结构信息等约束信息，来改善遮挡问题、相似的身体部位造成的混淆问题等。因此想到通过将分组学习、图卷积和自注意力结合起来，通过精修模块显式的将人体结构信息引入到网络中

精修模块中的这个分组策略是你自己想到的么，还是现有的方法？

​	将关键点分组进行训练的策略有被提出过，一个的目的是在COCO数据集的关键点模型上进行末端关键点的修正，一个是在MPII数据集的关键点模型上根据关键点间相似度进行分组，这里采用的分组策略是在coco数据集的人体关键点模型上进行分组，在已有分组策略的基础上进行了改进。







受一篇自底向上方法的关节点解耦的文章启发，那篇文章就是在backbone的输出后，将每个关节点单独作为一个分支，加了两层卷积来针对性的进行特征提取。就有了个思路，在骨干网络后加一个模块提取每个关节点的特征。

之后看到了一篇分组学习的文章，就是说大部分方法都是去学习一个共享表示。当然相关任务间的共享特征肯定是好的，如某些部位(如膝盖)的提示为其他相关部位(如脚踝)的定位提供了重要信息和约束，如果表示不共享，则很难学习这些部位。但不是所有部位都彼此相关的，例如，左手腕或右手腕的提示几乎不能提供关于右脚踝位置的信息，认为那些不相关或弱相关的任务共享特征共享机制会造成性能恶化。然后提出了分组学习

就想到了我要在骨干网络后加一个模块，骨干网络学习共享表示，每组分支学习他们之间的相关性。这两个方法中分组后都是通过两三层卷积简单的进行学习。这时候我就想，那既然是学习每组关节点之间的相关性，那用transformer不是更好。又想到backbone的输出对应K个关节点的坐标，那就可以用GCN了，因为GCN顶点里村的就是每个关节点的坐标，因此就把他们组合在了一起。



想通过这些组合的结构来帮助引入人体结构信息



### 2.5 Test Time Augmentations

所有用于姿态估计的SOTA方法都依赖于测试时间增强(TTA)来提高性能。翻转测试和多尺度测试是两种常用的测试技术。翻转测试增加了2X的复杂度，而多尺度测试在三个尺度{0.5X, 1X,  2X}上运行推理，增加复杂度(0.25X+1X+4X)=5.25X。随着翻转测试和多尺度测试的进行，复杂性将增加5.25*2x=10.5X。

除了增加计算复杂度外，准备扩充数据本身也很昂贵。例如，在翻转测试中，需要翻转图像，这会增加系统的延迟。类似地，多尺度测试需要对每个尺度进行调整大小操作。这些操作可能非常昂贵，因为它们可能不会加速，不像CNN的操作。融合各种前向传播的输出需要额外的成本。对于嵌入式系统来说，在没有任何TTA的情况下，能够获得具有竞争力的结果才是最重要的。

因此，`YOLO-Pose`的所有结果都没有任何TTA。



| **解耦关节点** |           |         |      |      |       |
| -------------- | --------- | ------- | ---- | ---- | ----- |
| DEKR           | hrnet_w32 | 512x512 | 2    | 0.68 | 29.6M |



编解码过程

Heatmap-based方法通常输出的特征图会是4倍下采样

HRNet，对256的输入下采样四倍到64，然后维持64这个高分辨率的特征图。最后输出64×64×K。最后坐标映射回原图

Simplebaseline通过三个步长为2的反卷积层解码。最后坐标映射回原图

posefix也是一个ResNet50+上采样

VITPose输入的是H/16,W/16,C到transformer中

LOFR也是下采样的输入。

![image-20221128145605900](E:\MarkDown\picture\image-20221128145605900.png)





精修网络训练策略

单独进行训练，输入image和加入误差的坐标(gt基础上加一些偏移量)，然后用gt做监督。预测时输入Image和其他网络的输出json，得到修正后的结果



# 人体分割

https://github.com/liruilong940607/Pose2Seg

https://paperswithcode.com/paper/self-correction-for-human-parsing



数据集

COCOPerson、OCHuman

![image-20221119213445796](E:\MarkDown\picture\image-20221119213445796.png)

**LIP Dataset**

![image-20221119213221140](E:\MarkDown\picture\image-20221119213221140.png)

**数据集内容：**Look in Person（LIP）是一个新的大规模数据集，专注于对人的语义理解。

从现实世界场景中收集，图像包含具有挑战性的姿势和视角，严重遮挡，各种外观和低分辨率的人的外观。

**数据集数量：**数据集包含50000张图像，这些图像带有19个语义人体部位标签的精细逐像素注释，和具有16个关键点的2D人体姿势。





# HRFormer改进

MHSA(multi-head self-attention)：特征映射X∈N×D，划分为P个小窗口，每个窗口K×K。MHSA后面跟的是有深度卷积的FFN

N×D划分为P个K*K×D，输入多头后还是K  * K×D

![image-20221117101014433](E:\MarkDown\picture\image-20221117101014433.png)

 ![image-20221117101019434](E:\MarkDown\picture\image-20221117101019434.png)

其中blocks指HRFormer block

多头前后都是N×D，用卷积的时候展开成二维就行。





# 具体的输入输出



![image-20221117112807658](E:\MarkDown\picture\image-20221117112807658.png)

有两种策略，一种是多阶段网络，端到端训练；一种是posefix的形式，即通过误差分布合成图片，单独训练这部分网络，可以作为一个部件改善任意方法的性能



GCN为两层的图卷积网络

adaptive convolution 自适应卷积来自[Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression]

前面的HRFormer输出特征图（C=K）

1、GCN的输入？C×D维的向量，可以直接输出heatmap图按通道展开为C×(H×W)，输入到GCN中就是C个节点，每个节点为1乘HW。输出C×(H×W)，经过GCN，每个节点内特征进行了交互。

2、特征图的尺寸？缩放后的H×W大小，可能160

3、注意力的输入，转为vector？直接展开为一维。这里分组进行注意力的时候，只取GCN的输出中对应通道的节点来做交叉注意力



transformer

![image-20221116222242700](E:\MarkDown\picture\image-20221116222242700.png)





# 分割网络

为什么不用传统的方法像是梯度算子边缘算子来输入？

传统方法得到的边缘信息是针对整张图而不能针对人这个类别。深度学习方法得到的结果更有针对性







































分类：

按网络输出：回归坐标；回归关节点概率图。或叫坐标回归；热图检测。基于回归（从输入图像直接映射到身体关节位置）和基于检测（生成关节位置的中间图像块或热图）

而小区域表示提供了密集的像素信息，具有较强的鲁棒性。与原始图像尺寸相比，小区域表示的检测结果限制了最终关节坐标的精度。





单阶段（端到端培训）和多阶段（分阶段培训）

从算法的网络结构方面考虑：

single-stage指经过一次encoder-decoder操作得到最终的关节点位置；multi-stage指经过多次encoder-decoder操作得到最终的关节点位置，比如Hourglass network

自底向上大部分都是单阶段

自顶向下全部为多阶段



单阶段方法的训练比多阶段方法更容易，但中间约束更少。多阶段方法通常在多个阶段中预测人体姿势，并伴有中间监督。例如，一些多人姿势估计方法首先检测人的位置，然后为每个检测到的人估计人的姿势。其他3D人姿势估计方法则首先在2D平面中预测关节位置，然后将其扩展到3D空间。







自顶向下：**目标检测器**、**多分支**（量化误差的offset分支、解耦关键点、与分割结合等，不知道怎么说）、**多阶段**（除了常规的multi-stage多次编解码外，额外增加预测验证网络对较难关键点进行精修也算两阶段）、**坐标回归与热图检测**、注意力模块**

自底向上：**关节点聚类**（可以结合实例分割算法帮助聚类）









[还不错的综述](https://zhuanlan.zhihu.com/p/72561165)





## 如何让网络更多的通过人体结构的约束来识别关键点

>  **按照人的直观视觉理解的话，主要会涉及到以下问题：**

* 关键点及周围的局部特征是什么样的？不同人体、人与外界环境之间的交互关系是什么？
* 关键点之间、人体肢体的空间约束关系是什么样的，以及层级的人体部件关系是什么样的？

> **基于Deep CNN的方法试图通过神经网络的拟合能力，建立一种隐式的预测模型来避开上述的显式问题**，而从大量的图像数据和标签监督信息中用神经网络去学习图像数据与构建的标签信息之间的映射。

基于去显式分析人体姿态问题的方法：目前也有少数方法用part-based()的层级树结构建立人体姿态模型并利用CNN，来进行学习与预测。 

**Body structure constraint:** 

Tang, W., Wu, Y ., 2019. Does learning specific features for related parts help
human pose estimation?, in: Proc. IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1107–1116.

Tang和Wu(2019)提出了一种基于部件的分支网络(PBN)来学习每个部件组的具体表示，而不是从一个分支预测所有的关节热图。然后通过计算关节互信息对数据驱动的零件群进行拆分。

Tang, W., Y u, P ., Wu, Y ., 2018a. Deeply learned compositional models for
human pose estimation, in: Proc. European Conference on Computer Vision,
pp. 190–206.

Ke, L., Chang, M.C., Qi, H., Lyu, S., 2018. Multi-scale structure-aware network
for human pose estimation. arXiv preprint arXiv:1803.09894 

Ning, G., Zhang, Z., He, Z., 2018. Knowledge-guided deep fractal neural net-
works for human pose estimation. IEEE Transactions on Multimedia 20,
1246–1259

(Ning et al, 2018)设计了一种分形网络，将身体先验知识强加于网络中进行引导。利用学习到的投影矩阵将外部知识视觉特征编码到基本网络中。

**Temporal constraint:**

Luo, Y ., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang, J., Lin, L., 2018.
Lstm pose machines, in: Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5207–5215.

**network compression**

Feng, Z., Xiatian, Z., Mao, Y ., 2019. Fast human pose estimation, in: Proc.
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8.

**bounding box refinement**

Fang, H., Xie, S., Tai, Y .W., Lu, C., 2017. Rmpe: Regional multi-person pose
estimation, in: Proc. IEEE International Conference on Computer Vision,
pp. 2334–2343.





# 一、研究重点

> ## 1.人体检测算法
>
> 自上而下的人体关键点检测算法由人体检测和人体关节点检测两部分组成，关节点的检测精度受限于检测算法的影响，且主要依赖于检测算法的Proposals。若人体检测框出现对人体覆盖不全、漏检等情况，就会对后续的关节点检测算法造成较大的影响。因此需要设计合理的 Proposals 筛选和修正机制，以保证在进行姿态估计时不会出现漏检、误检以及重复检测等情况
>
> ## 2.设计合理的关节点聚类算法（down-top）
>
> 对于自底向上的方法，关节点检测阶段输出的Heatmap中的局部最大值被视作人体关节点的候选位置，但Heatmap中不包含有关实例的信息，即这些关节点属于哪个人体。当图像中存在多个人物时，需要一种关节点聚类机制，使得属于每个人物的关节点被分配到对应的人体。本课题拟采用简单又准确的关联嵌入实现关节点聚类。



为什么要把遮挡分为自遮挡和环境遮挡两部分：自遮挡是高自由度引起的，提高局部感受野等操作是无法改善的，需要通过结构信息；环境遮挡是可以通过自适应卷积等改善的，因此归类到关键点可见度上

# 难点

## 1.高自由度——> 姿态多变、自遮挡和相互遮挡——> 人体结构、分组

> 由于人体具有相当的柔性，会出现各种姿态和形状，**人体任何一个部位的微小变化都会产生一种新的姿态**，有着较高的自由度；人类灵活的身体意味着关键点之间有着更复杂的内在关联和更高自由度的肢体动作，这对模型训练提出了更高的挑战；
>
> 复杂姿势

——》人体结构可以解决高自由度问题和部分自遮挡问题

### 1.1.用Transformer中的self-attention来捕捉结构信息

Transformer更能学习到关键点之间的相关性

![image-20221105154332099](E:\MarkDown\picture\image-20221105154332099.png)

### 1.2 通过GCN挖掘关键点之间固有的空间关系先验

CVPR 2022：Location-Free Human Pose Estimation(LOFR)；腾讯优图；5.25；9.27被收录

Xu X, Gao Y, Yan K, et al.  Location-Free Human Pose Estimation[C]//Proceedings of the IEEE/CVF  Conference on Computer Vision and Pattern Recognition. 2022:  13137-13146.

Location-Free Human Pose Estimation（LOFR）

关节之间的类间差异很微妙，相邻或对称关节具有相似的语义上下文或外观。如图所示，这往往会导致位置混淆和错误响应。模型很难捕获细粒度的关节特定特征，以消除混淆，尤其是在没有明确的位置监控的情况下。关节之间的固有结构关系在帮助区分或推断不确定位置方面起着关键作用。因此，如何挖掘模型的内在结构关系是至关重要的

![image-20221103111356561](E:\MarkDown\picture\image-20221103111356561.png)

![image-20221103111403577](E:\MarkDown\picture\image-20221103111403577.png)

一般将图卷积得到的结果作为cross attention的Q

> 
>
> ![image-20221028120449534](E:\MarkDown\picture\image-20221028120449534.png)

**怎么监督的？？？应该是按关键点的序号**

 为了使得每个关键点的预测更加精准，应该更有针对性地对各个关键点进行特征提取和回归，实现了关键点之间的解耦。可以通过分组卷积实现；或多分支结构显式的将一个关键点的表示学习与其他关键点解耦，从而提高回归质量。单分支结构必须隐式地解耦特征学习，这增加了优化难度
Backbone输出->按通道划分为K份->自适应卷积->1×1卷积(将通道数映射为2)得到offset

![image-20221027195528237](E:\MarkDown\picture\image-20221027195528237.png)

### 1.4 AID

对于Heatmap-based方法而言，如果训练时没有加入AID之类的随机擦除数据增强，对于遮挡的鲁棒性其实是不如Regression-based方法的，毕竟回归方法连超出bbox以外的关键点都能直接回归出来。对于自顶向下的方法，通过应用cutout对图片的一部分进行整体的裁剪，可以提升AP，更多的关注约束信息，而不是对外观信息过拟合。

### 1.5.body center heatmap改善相互遮挡

自底向上：

**ICCV 2021：ROMP**输出一个body center heatmap可以帮助模型有效的定位被遮挡的人。感觉也会对聚类有帮助。其中center map的gt是用gt的躯干关键点取平均。对于重叠严重的人，用CAR碰撞感知表示将他们分开一段距离来减轻模糊性

***



## 2.关键点的可见性——> 环境遮挡、相似的身体部位(删掉，不提了)、尺度、视角——>增大局部的感受野、解耦关键点、尺度分支、改善数据增强方法

总解决方法：解耦、自适应卷积（找找其他增大感受野的方法）、尺度分支



对于遮挡和自相似：关键点局部信息的区分性很弱，即背景中很容易会出现同样的局部区域造成混淆，所以需要考虑较大的感受野区域；

## 2.1.**环境造成的遮挡**

> 环境造成的遮挡：复杂环境下也可能会由于景物的遮挡导致前景信息难以提取
>

### 2.1.1.DERK中的自适应卷积

​	传统卷积只能看到中心像素及其周围的邻域点，而DEKR认为回归关键点坐标的特征必须集中注意到关键点周围的区域。于是，DEKR设计了一种**自适应卷积，它能够使得像素点的激活范围不再局限于其邻域，而是集中在关键点周围**。加上该偏移量的学习之后，可变形卷积核的大小和位置可以根据当前需要识别的图像内容进行动态调整，其直观效果就是不同位置的卷积核采样点位置会根据图像内容发生自适应的变化，从而适应不同物体的形状、大小等几何形变



## 2.2.相似的身体部位

>  身体自相似的部位

### 2.2.1解耦

![image-20221027194459671](E:\MarkDown\picture\image-20221027194459671.png)







## 2.3.**尺度变化（自底向上）**、**视角变化**

> 尺度变化会导致较小尺度的人体，其关键点容易不可见

### **2.3.1.高质量的高分辨率热图**

对于自底向上，以精确定位小型人群的关键点，应对尺度变化；

### 2.3.2.尺度分支（自底向上

通过CVPR2021：Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation的尺度预测分支，提出了尺度自适应heatmap回归方法（SAHR），可以自适应调整每个关键点的标准偏差，更能容忍各种人类尺度和标记模糊性。

自下而上的方法必须解决人体尺度可能存在的巨大差异。对于人体尺度的差异问题，gt heatmap通过2D高斯核覆盖所有骨骼关键点来构建，这些核的**标准偏差**是**固定**的，σ=2。这就造成了标签的模糊性

![image-20221026161652212](E:\MarkDown\picture\image-20221026161652212.png)

> 视角变化

相机的拍摄位置和角度，都会增加单目估计的难度。 

### 2.3.3 可以从数据增强方面考虑（不行

对于自顶向下，是裁剪出人之后再做数据增强么？可以更多的去做一些错切等操作

对于自底向上肯定是对原图片进行数据增强







对于视角变化，往往是侧身、俯身这种比较极端的尺度变化影响较大。可以尝试通过一些仿射变换如错切等操作改变视角

![image-20221026201043494](E:\MarkDown\picture\image-20221026201043494.png)



## 3.较难关键点的检测——> 高自由度的关键点、遮挡的关键点都是较难关键点——>打算划分到高自由度里

> 人体不同关键点的检测的难易程度是不一样的，对于腰部、腿部这类关键点的检测要明显难于头部附近关键点的检测，所以不同的关键点可能需要区别对待

### 3.1.OHKM

CPN的RefineNet主要解决更难或者不可见关键点的检测，这里对关键点进行难易程度进行界定主要体现在关键点的训练损失上，使用了常见的Hard  Negative Mining策略，在训练时取损失较大的top-K个关键点计算损失，然后进行梯度更新，不考虑损失较小的关键点

### 3.2.额外阶段精修

Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation又叫SCIO

   姿态估计的主要挑战之一是提高硬关键点(例如末端关键点)的准确性。**但是在现有的网络预测方法中，推理过程是纯正向的，从训练集中学到的知识直接应用到测试集中。**由于没有测试集的gt，没有有效的机制来验证预测结果是否准确，也没有反馈过程来根据实际测试样本调整预测结果，这种正向推理过程往往存在泛化问题。

![image-20221026165855193](E:\MarkDown\picture\image-20221026165855193.png)



> **6.人体检测**
>
> 6.1.**CVPR 2018：CPN** 在行人检测框架中使用soft NMS取代hard NMS，提升0.3map
>
> 6.2.Poseur: Direct Human Pose Regression with Transformers?
>
> •改进score计算方式，用似然概率函数在均值附近区域的概率积分作为score，显著提升了AP分数
>
> ![image-20221026162813258](E:\MarkDown\picture\image-20221026162813258.png)
>
> 6.3.**CVPR 2017**：**G-RMI**
>
> **Pose Rescoring**；**OKS-Based Non Maximum Suppression** 











为什么top-down方法更好的两个猜想：

* top-down方法，输入样本是中心为人体的剪裁后区域，不同尺寸的人体大小会标准化为相近的尺度,  神经网络训练时数据的一致性较高，并且推理的尺度基本是一致的。而Bottom-up面对的关键点特征来自于不同尺度空间下(一副图中人物尺度不一致)，个人认为这样的情况,  神经网络对关键点特征的位置响应的难度更大

* 自上而下的检测方法加入了整个人体的一个空间先验。而自下而上的关键点定位算法没有显示的去建模整个人体的空间关系，而是一次性检测出所有的关键点，只是建模了局部的空间关系——加一个约束，显式/隐式的学习整个人体的信息





​	



研究意义：

*  3D人体姿态估计的铺垫、3维人体重建的必备技术 （从2维到3维，从姿态到形态）
*  人体关键点的视频追踪问题的基础（从静态到动态）
*  动作识别的信息来源（从关键点的时序空间特征映射到动作语义问题）





展望：

​	2D 关键点预测是姿态估计任务的基础性研究问题，但它并不是代表着姿态估计就仅仅是预测关键点，其本身难以克服的问题如拥挤、自遮挡或相互遮挡，就是因为深度、3D信息的缺乏而导致，也许只有上升到3D的层面时才能被解决。

​	3D的应用没那么广泛，与序列信息结合的视频预测才是未来的趋势



发展方向：

* 更多的关注姿态的连续性信息	人体的运动和姿态的变化具有连性，通过分析姿态的变化过程，可以还原姿态失真的地方，例如解决遮挡、视觉缩短、环境干扰等棘手问题。因此人体姿态估计将来的重点研究方向不会再独立地考虑单张图片的人体态，而是会分析一组图片或者是多帧的视频序列，网络结构会更多地参考 RNN 等结构来处理时序问题

* 提高人体姿态特征检测的精度	为提高人体姿态特征检测的精度，可以基于精确化人体目标检测框和缩小人体姿态特征空间这两种思路。对于基于检测框proposals的算法，例如自上而下的姿态估计方法，目标检测领域的成果可以被直接拿来提高姿态估计的效果 同时，后续研究还要设计一些算法，例如改进 NMS 策略来过滤一些多余的检测结果，争取同时提高人体目标检测的召回率和准确率。虽然精确化人体提议框可以过排除大部分背景特征的干扰，但是目标检测算法的时间代价较高。由于提高人体姿态特征检测的精度的本质是要提取关键的人体姿态征，因此可以通过缩小人体姿态特征空间达到目的。





# 与分割结合

## 可解释性

自底向上受尺度影响较大

轻量化分割网络

## 相关文章

1.CVPR 2019：["Pose2Seg: Detection Free Human Instance Segmentation."](https://cloud.tencent.com/developer/article/1449040)
腾讯AI Lab联合清华大学提出基于骨骼姿态估计的人体实例分割。利用骨架检测来定位人体实例，从而构建更高效的实例分割方法。

2.PersonLab

## 数据集

1.**OCHuman**数据集：包含包围框、人体姿态关节点、以及实例分割掩模标注
![img](E:\MarkDown\picture\1620.jpeg)



2.[COCOPersons](https://blog.csdn.net/u011291667/article/details/84329990)：person_keypoints.json

[coco api的一些方法](https://blog.csdn.net/zym19941119/article/details/80241663)

stuff——语义分割

[stuff_val2017_pixelmaps.zip下载](https://github.com/open-mmlab/mmsegmentation/issues/1544)



Look into Person（LIP）数据集

**数据集数量：**数据集包含50000张图像，这些图像带有19个语义人体部位标签的精细逐像素注释，和具有16个关键点的2D人体姿势。

**数据集功能：**人体分割、姿态识别

https://paperswithcode.com/dataset/lip



![image-20221204140715288](E:\MarkDown\picture\image-20221204140715288.png)



![image-20221204140905357](E:\MarkDown\picture\image-20221204140905357.png)

![image-20221204141018509](E:\MarkDown\picture\image-20221204141018509.png)

![image-20221204141052917](E:\MarkDown\picture\image-20221204141052917.png)

![image-20221204141303133](E:\MarkDown\picture\image-20221204141303133.png)





# 研究现状可以参考李这个



![image-20221204145309260](E:\MarkDown\picture\image-20221204145309260.png)

![image-20221204150358800](E:\MarkDown\picture\image-20221204150358800.png)

![image-20221204145343812](E:\MarkDown\picture\image-20221204145343812.png)

![image-20221204145509291](E:\MarkDown\picture\image-20221204145509291.png)

![image-20221204145632949](E:\MarkDown\picture\image-20221204145632949.png)



![image-20221204145735253](E:\MarkDown\picture\image-20221204145735253.png)



![image-20221204145914548](E:\MarkDown\picture\image-20221204145914548.png)

![image-20221204150121210](E:\MarkDown\picture\image-20221204150121210.png)

![image-20221204150140850](E:\MarkDown\picture\image-20221204150140850.png)



![image-20221204150222553](E:\MarkDown\picture\image-20221204150222553.png)

![image-20221204150308084](E:\MarkDown\picture\image-20221204150308084.png)

发展趋势，分析以下现在的问题









![image-20221204151208048](E:\MarkDown\picture\image-20221204151208048.png)

![image-20221204151223065](E:\MarkDown\picture\image-20221204151223065.png)

![image-20221204151257842](E:\MarkDown\picture\image-20221204151257842.png)



![image-20221204151434932](E:\MarkDown\picture\image-20221204151434932.png)

![image-20221204151502921](E:\MarkDown\picture\image-20221204151502921.png)

![image-20221204151606825](E:\MarkDown\picture\image-20221204151606825.png)

![image-20221204151618731](E:\MarkDown\picture\image-20221204151618731.png)

![image-20221204151745335](E:\MarkDown\picture\image-20221204151745335.png)

![image-20221204151855859](E:\MarkDown\picture\image-20221204151855859.png)

![image-20221204152337842](E:\MarkDown\picture\image-20221204152337842.png)





拟解决的关键科学问题

写的是关键技术



![image-20221204153710706](E:\MarkDown\picture\image-20221204153710706.png)

![image-20221204153922321](E:\MarkDown\picture\image-20221204153922321.png)

![image-20221204154039893](E:\MarkDown\picture\image-20221204154039893.png)

![image-20221204154051111](E:\MarkDown\picture\image-20221204154051111.png)

![image-20221204155127894](E:\MarkDown\picture\image-20221204155127894.png)

![image-20221204155145374](E:\MarkDown\picture\image-20221204155145374.png)

![image-20221204155314820](E:\MarkDown\picture\image-20221204155314820.png)

![image-20221204155355116](E:\MarkDown\picture\image-20221204155355116.png)

![image-20221204155608159](E:\MarkDown\picture\image-20221204155608159.png)

![image-20221204155733589](E:\MarkDown\picture\image-20221204155733589.png)

![image-20221204155856188](E:\MarkDown\picture\image-20221204155856188.png)





写国内外研究现状，比如回归坐标和概率图的，自顶向下和自底向上的





![image-20221204163550957](E:\MarkDown\picture\image-20221204163550957.png)



![image-20221204163722488](E:\MarkDown\picture\image-20221204163722488.png)

![image-20221204164202720](E:\MarkDown\picture\image-20221204164202720.png)

![image-20221204164232553](E:\MarkDown\picture\image-20221204164232553.png)

![image-20221204164434631](E:\MarkDown\picture\image-20221204164434631.png)

![image-20221204164526569](E:\MarkDown\picture\image-20221204164526569.png)

 







![image-20221204172518438](E:\MarkDown\picture\image-20221204172518438.png)

![image-20221204172528734](E:\MarkDown\picture\image-20221204172528734.png)

![image-20221204172603517](E:\MarkDown\picture\image-20221204172603517.png)

![image-20221204172657375](E:\MarkDown\picture\image-20221204172657375.png)

![image-20221204172732936](E:\MarkDown\picture\image-20221204172732936.png)



![image-20221204172801232](E:\MarkDown\picture\image-20221204172801232.png)







对于程序中的均方误差，不同关节有个权重

![image-20221204195628008](E:\MarkDown\picture\image-20221204195628008.png)





Bone Loss

https://zhuanlan.zhihu.com/p/341946291

```python
import torch
import torch.nn as nn

class JointBoneLoss(nn.Module):
    def __init__(self, joint_num):
        super(JointBoneLoss, self).__init__()
        id_i, id_j = [], []
        for i in range(joint_num):
            for j in range(i+1, joint_num):
                id_i.append(i)
                id_j.append(j)
        self.id_i = id_i
        self.id_j = id_j

    def forward(self, joint_out, joint_gt):
        if len(joint_out.shape) == 4: # (b, n, h, w) heatmap-based featuremap 
            calc_dim = [2, 3]
        elif len(joint_out.shape) == 3:# (b, n, 2) or (b, n, 3) regression-based result
            calc_dim = -1
        
        J = torch.norm(joint_out[:,self.id_i,:] - joint_out[:,self.id_j,:], p=2, dim=calc_dim, keepdim=False)
        Y = torch.norm(joint_gt[:,self.id_i,:] - joint_gt[:,self.id_j,:], p=2, dim=calc_dim, keepdim=False)
        loss = torch.abs(J-Y)
        return loss.mean()
```





```python
{"segmentation": [[215.43,327.97,215.43,323.09,223.97,316.38,223.97,310.9,223.97,303.58,219.09,279.18,216.04,237.11,219.7,225.52,220.31,218.2,214.82,208.44,216.04,199.3,222.75,195.64,228.85,201.74,229.46,207.83,239.21,218.2,241.65,229.18,244.09,260.89,236.78,266.38,236.78,321.87,234.95,326.14,219.09,327.97]],
 "num_keypoints": 13,"area": 2444.22435,"iscrowd": 0,
 "keypoints": [216,206,2,218,203,2,0,0,0,226,204,2,0,0,0,230,218,2,223,219,2,231,241,2,0,0,0,222,258,2,0,0,0,226,261,2,224,260,2,228,290,2,225,286,2,230,319,2,227,315,2],
 "image_id": 511999,"bbox": [214.82,195.64,29.27,132.33],"category_id": 1,"id": 223042}
```



![image-20221206101607119](E:\MarkDown\picture\image-20221206101607119.png)                 

峰值偏移——对gt概率图取argmax，得到的就是最大值的位置，然后对其增加一个惩罚，固定其为1？



![image-20221206123700751](E:\MarkDown\picture\image-20221206123700751.png)



CVPR 2017 Semantic Instance Segmentation with a Discriminative Loss Function

+ CVPR 2021 Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation

https://bingqiangzhou.github.io/2019/09/16/PaperReading-SemanticInstanceSegmentationWithADiscriminativeLossFunction.html

https://blog.csdn.net/Xiong_Sun/article/details/81710934



3. Automatic Weighted Loss

   自动调整多任务的损失函数权重

地址：Mikoto10032/AutomaticWeightedLoss: Multi-task learning using uncertainty to weigh losses for scene geometry and semantics, Auxiliary Tasks in Multi-task Learning (github.com)

 https://zhuanlan.zhihu.com/p/367881339

由于模型有多个头部，进行关键点预测、手存在性预测、手势识别等任务（包括使用Bone Loss)，因此可以使用Multi-task learning的方法来动态调节每个任务的权重，将不同的loss拉到统一尺度下，这样就容易统一，具体的办法就是利用同方差的不确定性，将不确定性作为噪声进行训练。





如**标记像素后聚类法**

该方法受益于语义分割，可以预测高分辨率的对象掩模。与分割检测跟踪技术相比，标签像素跟踪聚类方法在经常使用的基准上精度较低。由于像素标记需要密集的计算，通常需要更多的计算能力。

![image-20221206150520434](E:\MarkDown\picture\image-20221206150520434.png)





body center heatmap改善相互遮挡

自底向上：

ICCV 2021：ROMP输出一个body center heatmap可以帮助模型有效的定位被遮挡的人。感觉也会对聚类有帮助。其中center map的gt是用gt的躯干关键点取平均。对于重叠严重的人，用CAR碰撞感知表示将他们分开一段距离来减轻模糊性













