# 分布式项目

> 看懂这个博客：https://blog.csdn.net/T_Solotov/article/details/124107667
>
> https://www.nowcoder.com/discuss/508770447215890432

> 要重写的都是纯虚函数

​	分布式网络通信框架是在Linux下通过c++开发的。服务发布方需要定义proto文件描述方法及请求响应类型、继承protobuf生成的基类并重写基类方法来调用本地方法，即可将本地方法发布为远程RPC方法。其中主机间的通信使用Muduo网络库；数据通信协议使用Protobuf进行数据的序列化和反序列化；使用Zookeeper作为服务注册中心，完成rpc服务的服务注册、服务发现。

* 通过Notify接口将服务对象和方法存储在unordered_map中，通过run接口进行rpc服务的发布及网络服务的启动；
* 通过onMessage事件回调进行网络消息的接收、反序列化及rpc方法的调用，其中通过自定义消息结构避免粘包问题；
* 对于rpc服务调用方，通过重写protobuf中的CallMethod方法，实现请求的序列化、网络消息的发送及接受、反序列化响应得到rpc方法的调用结果；
* 通过CMake构建项目集成编译环境，将框架编译为静态库进行使用。



## 难点

​	整个架构的设计。这个项目的大体思路是比较清晰的，但具体的功能实现和接口设计的实现就相对麻烦一些。

​	比如我们的服务对象和方法怎么保存呢？这就需要定义一个网络对象类，然后服务对象作为成员变量，unordered_map来保存方法名和方法。同时还需要通过Notify接口将服务对象和方法存储在unordered_map中，通过run接口进行rpc服务的发布及网络服务的启动；

**Watcher机制**

> ​	Zookeeper提供了分布式数据的发布/订阅功能，可以让客户端订阅某个节点，当节点发生变化(比如创建、修改、删除、数据获取、子节点获取)时，可以通知所有的订阅者。另外还可以为客户端连接对象注册监听器，可以监听到连接时的状态。这个实现机制在Zookeeper里面就是Watcher机制。
> ​	watcher实现机制类似观察者模式。
>
> **watcher监听是一次性的，当watcher被触发之后，需要重新注册才能监听。**
>
> 流程： 客户端注册watcher到服务器，同时将watch对象保存到WatchManager中。当服务器监听到znode数据变化时，通知客户端。同时客户端的WatchManager触发回调事件（process()）处理相应的逻辑，完成一次完整的watcher流程。

​	watcher机制就是ZooKeeper客户端对某个znode建立一个watcher事件，当该znode发生变化时，这些ZK客户端会收到ZK服务端的通知，然后ZK客户端根据znode的变化来做出业务上的改变。

​	在这个项目中设计了一个zk类，负责与zk建立连接等操作。其中ZkClient::Start()函数中调用了zookeeper_init(...)函数，将全局函数global_watcher(...)传入，完成注册watcher到服务器。zookeeper_init(...)函数的功能是【异步】建立rpcserver和zookeeper连接，并返回一个句柄赋给m_zhandle（客户端通过该句柄和服务端交互）。如何理解异步建立，就是说当程序在ZkClient::Start()函数中获得了zookeeper_init(..)函数返回的句柄后，连接还不一定已经建立好。因为发起连接建立的函数和负责建立连接的任务不在同一个线程里完成。（之前说过ZooKeeper有三个线程）
​	所以调用完zookeeper_init函数之后，下面还定义了一个同步信号量sem，并且调用sem_wait(&sem)阻塞当前主线程，等ZooKeeper服务端收到来自客户端callee的连接请求后，服务端为节点创建会话（此时这个节点状态发生改变），服务端会返回给客户端callee一个事件通知，然后触发watcher回调（执行global_watcher函数）

​	ZkClient::Start()函数中有一句调用：zoo_set_context(m_zhandle, &sem); ，我们将刚才定义的同步信号量sem通过这个zoo_set_context函数可以传递给m_zhandle进行保存。在global_watcher中可以将这个sem从m_zhandle取出来使用。
​	global_watcher函数的参数type和state分别是ZooKeeper服务端返回的事件类型和连接状态。在gloabl_watcher函数中发现状态已经是ZOO_CONNECTED_STATE说明服务端为这个节点已经建立好了和客户端callee的会话。此时调用sem_post(sem)解除主线程阻塞（解除ZkClient::Start()中的阻塞）。
​	这个同步机制保证了，当ZkClient::Start()执行完后，callee端确定和zookeeper服务端建立好了连接！！

## 微服务和分布式

**微服务**与分布式的区别是微服务的各服务可独立应用，也可组合使用。然后粒度会更细。

**分布式**是将一个项目拆分成多个模块，并将这些模块分开部署。拆分可以是水平拆分

* **水平拆分：**根据“分层”的思想进行拆分。例如，可以将一个项目根据“三层架构”拆分成 表示层、业务逻辑层和数据访问层，然后再分开部署：把表示层部署在服务器A上，把service和dao层部署在服务器B上

<img src="E:\MarkDown\picture\image-20230703223217437.png" alt="image-20230703223217437" style="zoom:33%;" />

* **垂直拆分：**根据业务进行拆分。例如，可以根据业务逻辑，将“电商项目”拆分成“订单项目”、“用户项目”和“秒杀项目”。显然这三个拆分后的项目，仍然可以作为独立的项目使用。像这种拆分的方法，就成为垂直拆分。

<img src="E:\MarkDown\picture\image-20230703223305232.png" alt="image-20230703223305232" style="zoom:23%;" />

**微服务**可以理解为一种非常细粒度的垂直拆分。例如，以上“订单项目”本来就是垂直拆分后的子项目，但实际上“订单项目”还能进一步拆分为“购物项目”、“结算项目”和“售后项目”，如图。现在看图中的“订单项目”，它完全可以作为一个分布式项目的组成元素，但就不适合作为微服务的组成元素了（因为它还能再拆，而微服务应该是不能再拆的“微小”服务，类似于“原子性”）。

<img src="E:\MarkDown\picture\image-20230703223436245.png" alt="image-20230703223436245" style="zoom:30%;" />

## 对比开源框架

> gRPChttps://github.com/grpc/grpc/tree/master/src/cpp

​	gRPC 就是利用了 protobuf，来实现了一个完整的 RPC 远程调用框架，其中的通讯部分，使用的是http协议，是在HTTP/2上实现的，gRPC双向流映射到HTTP/2流。呼叫报头和初始元数据的内容作为HTTP/2报头发送，并受到HPACK压缩。有效负载消息被序列化为长度前缀的gRPC帧的字节流，然后在发送者处将其分段为HTTP/2帧，并在接收者处重新组装。状态和尾随元数据作为HTTP/2尾随报头（又名尾随）发送。客户端通过在最后一个数据帧上设置END_STREAM标志来指示其消息流的结束。有关详细说明，请参见doc/PROTOCOL-HTTP2.md。

​	使用的话和我们这个框架是类似的。实现的话gRPC使用的是http协议，而我们项目中是tcp协议，用的Muduo网络库，实现起来比较方便。

<img src="E:\MarkDown\picture\image-20230703214140149.png" alt="image-20230703214140149" style="zoom:50%;" />

1.gRPC通信的第一步是定义IDL，即我们的接口文档（后缀为.proto）

2.第二步是编译proto文件，得到存根（stub）文件，即上图深绿色部分。

3.第三步是服务端（gRPC Server）实现第一步定义的接口并启动，这些接口的定义在存根文件里面

4.最后一步是客户端借助存根文件调用服务端的函数，虽然客户端调用的函数是由服务端实现的，但是调用起来就像是**本地函数**一样。

## 目的

现在其实是有一些开源框架的，比如grpc等。当时自己实现这个框架的目的主要是去了解分布式、protobuf以及zk这些工具的使用。因为实习入职前问了下说可能会被安排去做一些分布式相关的工作，因此就通过这个项目去了解熟悉一下这些东西。

## 集群和分布式

**集群**：每一台服务器独立运行一个工程的所有模块。

**分布式**：一个工程拆分了很多模块，每一个模块独立部署运行在一个服务器主机上，所有服务器协同工作共同提供服务。每一台服务器称作分布式的一个**节点**，根据节点的并发要求，对一个节点可以再做节点模块**集群**部署(比如提高一个需要高并发的结点的并发量)。 



> 以集群聊天服务器项目为例
>
> 对于单机的聊天服务器存在的问题，以及集群和分布式如何解决的：

* 并发量受限于硬件资源
  * 集群有多个服务器主机，每个独立运行一个服务器，缓解压力，提升了并发量。但在一个服务器中，有的模块并不需要高并发，有些浪费资源
  * 分布式就可以将需要高并发量的模块部署在多个主机上，不需要高并发的部署在单个或少量主机上
* 某一小块内容的改动，就需要重新编译、部署整个项目。
  我们期望的是哪个模块出问题了，只需要重新编译对应模块即可。
  * 集群没有解决这个问题，还需要在多个主机上多次部署
  * 分布式将每个模块编译成了可在不同主机上独立运行部署的小的服务，因此只需要编译对应的主机上的模块
* 有些模块是CPU密集型，有些是IO密集型，各模块对硬件资源的需求是不一样的。都部署在一台机器上并不合适。
  * 集群没啥用
  * 分布式可以将对应的模块部署到对应特点的主机上



![image-20230607204435751](E:\MarkDown\picture\image-20230607204435751.png)

缺点：容灾能力需要注意一下，因为一个服务器挂掉，就整个都不能用了

## 分布式框架需要关注的问题

![image-20230607205413791](E:\MarkDown\picture\image-20230607205413791.png)

* 对于一些大型项目，软件模块的划分会比较困难，比如应该将哪些模块放到一个主机上
* 各模块之间如何相互访问、调用？可以使用muduo库进行网络传输，将传输的部分封装到分布式网络框架当中。其中涉及了muduo库、protobuf和zookeeper

## RPC通信原理

> [rpc协议](https://xiaolincoding.com/network/2_http/http_rpc.html#http-%E5%92%8C-rpc)
>
> * **RPC** 是远程过程调用（Remote Procedure Call）。两个不同的服务器上的服务提供的方法不在一个内存空间，需要网络编程才能传递方法调用所需要的参数，方法调用的结果也需要通过网络编程来接收。如果手动网络编程来实现这个调用过程的话工作量大，因为需要考虑底层传输方式(TCP 还是UDP)、序列化方式等等方面。RPC可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。
> * **RPC 本质上不算是协议，而是一种调用方式**，而像 gRPC 和 百度的brpc 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。
> * RPC 有很多种实现方式，**不一定非得基于 TCP 协议**。
> * **RPC 调用分以下两种：**
>   * **同步调用：** 客户方等待调用执行完成并返回结果。（我们项目中的）
>   * **异步调用：** 客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果。

`caller`为**调用者**，使用这个rpc服务

`callee`是rpc服务的**提供者**，将本地服务发布在rpc

<img src="E:\MarkDown\picture\image-20230607212352110.png" alt="image-20230607212352110" style="zoom:70%;" />

![img](E:\MarkDown\picture\v2-6b87c85f47ab2cc8d24228df59269e7e_1440w.webp)

> 蓝色框就是要实现的**分布式网络通信框架**

<img src="E:\MarkDown\picture\image-20230607212739784.png" alt="image-20230607212739784" style="zoom:50%;" />

​	比如要调用一个远端的bool aa(int a, int b)方法，则先传入 User stub 函数名和参数，然后序列化、muduo发送到远端，远端反序列化、调用本地函数并返回结果。返回的结果能附带一些响应，比如正常运算或错误信息等。然后再序列化、muduo发送到调用端、反序列化得到结果。

## ==protobuf==

### 对比 json

​	protobuf（protocol buffer）是 google 的一种数据交换的格式，它独立于平台语言。可以把它用于分布式应用之间的数据通信。
​	google 提供了 protobuf 多种语言的实现：java、c#、c++、go 和 python，每一种实现都包含了相应语言的编译器以及库文件。

**优点：**

* xml 读 X M L，JSON 读 J S O N 或 [ˈdʒeɪs(ə)n]
* xml 和 json 都是文本存储的；而 protobuf 是二进制存储的，效率和兼容性都更好
* protobuf 不需要存储额外的信息
  * json 存的是 key-value，如 name:"zhang", pwd:"123"
  * 而 protobuf 存的是数据，如 "zhang""123"，更省空间

**使用**：

​	我们需要写一个 xxx.proto 文件。核心就是 message 和 server。在我们这个分布式项目里，message 就是我们要 rpc 的函数的传入参数以及返回值，server 是要 rpc 的函数。

​	我们需要做的就是在proto中定义方法的请求和响应，方便我们进行序列化和反序列化；在service中定义rpc方法。写好proto文件后，通过 `protoc test.proto --cpp_out=./` 命令生成c++文件：xxx.pb.cc和xxx.pb.h。

​	进行数据传输的时候，我们就可以创建message类型的对象，并通过生成的.cc文件提供的set_xx函数进行参数的赋值，通过SerializeToString函数进行序列化、ParseFromString函数进行反序列化、.xx()函数进行值的获取

​	对于 proto 文件中定义的 service 方法名，会生成xxx和xxx_Stub两个类。xxx类中包含我们定义的方法的纯虚函数，需要我们的服务提供者来继承并重写这个要发布为rpc的方法，将函数写成protobuf提供的这种格式后，就能通过传入request和response来调用本地方法，并得到response；而xxx_stub类是服务消费者的类，在caller调用方通过new一个channel来初始化stub对象，进而调用rpc方法，而调用rpc方法实际上是调用了callmethod方法，因此需要我们去重载这个函数，统一做request的序列化、网络发送和接收、response的反序列化

<img src="https://pic3.zhimg.com/80/v2-06afe948f0a16103182363c5a2184a96_1440w.webp" alt="img" style="zoom:50%;" />

### 使用

> 安装vscode-proto3插件；protobuf环境搭建见pdf文档
>

> 将本地方法**发布为远程rpc方法**的步骤：
>
> ​	通过proto文件定义传入传出参数和函数 =》 生成xx.pb.cc和.h文件 =》 得到xxx这个rpc服务提供者和xxx_Stub这个rpc服务消费者两个类 =》在rpc发布者和使用者的类中分别继承这两个类并重写相应的方法

#### proto文件

​	我们需要写一个xxx.proto文件。核心就是message和server。在我们这个分布式项目里，message就是我们要rpc的函数的传入参数以及返回值，server是要rpc的函数。

​	我们需要做的就是在proto中定义方法的请求和响应，方便我们进行序列化和反序列化；在service中定义rpc方法。写好proto文件后，通过 `protoc test.proto --cpp_out=./` 命令生成c++文件：xxx.pb.cc和xxx.pb.h。

```protobuf
syntax = "proto3";  // 声明了protobuf的版本

package fixbug;  // 声明了代码所在的包（对于C++相当于namespace）

// 定义下面的选项，表示生成service服务类和rpc方法描述，默认不生成
option cc_generic_services = true;

// 下面是定义的一些数据类型
// 一般也就数据、列表、映射表

// 将错误码和错误类型打包一下
message ResultCode
{
    int32 errcode = 1; // 1表示第一个字段
    bytes errmsg = 2; // 2表示第二个字段。字符串类型一般定义为bytes，string也行
}

// 定义登录请求消息类型（Login函数的输入参数）  name   pwd
// 对应到生成的cc文件中就是类和类的成员变量
message LoginRequest
{
    bytes name = 1;
    bytes pwd = 2;
}

// 定义登录响应消息类型（Login函数的输出参数）
message LoginResponse
{
    ResultCode result = 1;
    bool success = 2;
}

// 在protobuf里面怎么定义描述rpc方法的类型 - service
// service的名字为UserServiceRpc
service UserServiceRpc
{
    // LoginRequest是实参，LoginResponse为返回值
    rpc Login(LoginRequest) returns(LoginResponse);  
}
```

#### xx.pb.cc使用

> test/protobuf文件夹中
>
> main.cc为测试文件，编译的时候记得**链接protobuf库**：g++ main.cc test.pb.cc -lprotobuf

使用流程：

​	使用的时候，我们就可以创建message类型的对象，并通过生成的.cc文件提供的set_xx函数进行参数的赋值，通过SerializeToString函数进行序列化、ParseFromString函数进行反序列化、.xx()函数进行值的获取

```cpp
#include "test.pb.h" // 1.包含proto文件生成的.h文件
#include <iostream>
#include <string>
// 2.作用域
using namespace fixbug; // proto中定义了package fixbug;  // 声明了代码所在的包（对于C++相当于namespace）

int main()
{
    // 3.定义request，比如proto文件中定义了LoginRequest类型
    LoginRequest req;
    req.set_name("zhang san");  // set_xxx表示给xxx赋值
    req.set_pwd("123456");

    // 4.将request序列化 =》 char*
    std::string send_str;
    if (req.SerializeToString(&send_str))  // 序列化
    {
        // c_str()将send_str转成c语言的char*类型
        std::cout << send_str.c_str() << std::endl;
    }

    // 这个示例没有进行request发送再得到response，就展示了下request的序列化和反序列化
    // 5.反序列化
    LoginRequest reqB;
    if (reqB.ParseFromString(send_str))
    {
        std::cout << reqB.name() << std::endl;  // 读取就是 .xxx();
        std::cout << reqB.pwd() << std::endl;
    }

    return 0;
}
```

对于protoc中定义的**列表类型**

```cpp
int main2() {

// proto中的列表格式：repeated
//     message GetFriendListsResponse
// {
//     ResultCode result = 1;
//     repeated User friend_list = 2;  // 用repeated定义了一个列表类型
// }

    GetFriendListsResponse rsp;

    // ResultCode是GetFriendListsResponse的一个变量
    ResultCode *rc = rsp.mutable_result(); 
    rc->set_errcode(0);

    // 在friend_list列表里添加两行
    User *user1 = rsp.add_friend_list();
    user1->set_name("zhang san");
    user1->set_age(20);
    user1->set_sex(User::MAN);

    User *user2 = rsp.add_friend_list();
    user2->set_name("li si");
    user2->set_age(22);
    user2->set_sex(User::MAN);

    std::cout << rsp.friend_list_size() << std::endl;

    return 0;
}
```

#### protobuf使用

> 见example/callee/userservice.c

​	对于request和response这两个数据类型，都是继承的protobuf::Message，可以使用其提供的set_xxx、xxx()、SerializeToString序列化、ParseFromString反序列化等方法

<img src="E:\MarkDown\picture\image-20230608205849860.png" alt="image-20230608205849860" style="zoom: 50%;" />

​	对于下面这张流程图，caller调用方的请求消息序列化后通过muduo发送到calle，caller此时就堵塞在wait上，一直等到callee反序列化、调用本地方法、序列化、通过muduo发送到caller后，caller解除堵塞，进行反序列化得到结果

<img src="E:\MarkDown\picture\image-20230607212739784.png" alt="image-20230607212739784" style="zoom:50%;" />

​	对于proto文件中定义的service方法名，会生成xxx和xxx_Stub两个类。这里UserServiceRpc是我们在proto中定义的方法名，在xx.pb.cc和xx.pb.h中生成对应的两个类

![image-20230608210149491](E:\MarkDown\picture\image-20230608210149491.png)

​	对于我们在proto中定义的service类型的UserServiceRpc，生成了UserServiceRpc这个rpc服务提供者和UserServiceRpc_Stub这个rpc服务消费者两个类

​	在UserServiceRpc类中，定义了login和getfriendlist两个纯虚函数，在rpc服务发布端（rpc服务提供者）需要定义一个类，来继承UserServiceRpc，并重写这两个方法来调用对应的本地方法。这样在一个远端想调用rpc服务提供者里的方法的时候，先由rpc框架接受需要的参数，然后匹配到UserServiceRpc中我们重写了的方法，调用这个重写的函数，这个函数再将返回值给rpc框架，框架再发送回去。

​	重写的这个函数需要传入controller、request、Closure，结果保存到response

```cpp
/*
UserService原来是一个本地服务，提供了两个进程内的本地方法，Login和GetFriendLists
*/
class UserService : public fixbug::UserServiceRpc // 使用在rpc服务发布端（rpc服务提供者）
{
public:
    bool Login() {} // 本地的Login方法
    
	/*
    重写基类UserServiceRpc的虚函数 下面这些方法都是框架直接调用的
    1. caller   ===>   Login(LoginRequest)  => muduo =>   callee 
    2. callee   ===>    Login(LoginRequest)  => 交到下面重写的这个Login方法上了

    这样在一个远端想调用rpc服务提供者里的方法的时候，先由rpc框架接受，
    然后匹配到UserServiceRpc中我们重写了的方法，调用这个重写的函数，
    这个函数再将返回值给rpc框架，框架再发送回去。
    */
    void Login(::google::protobuf::RpcController* controller,
                       const ::fixbug::LoginRequest* request,
                       ::fixbug::LoginResponse* response,
                       ::google::protobuf::Closure* done)
    {
        // 框架给业务上报了请求参数LoginRequest，应用获取相应数据做本地业务
        std::string name = request->name();
        std::string pwd = request->pwd();

        // 调用本地方法
        bool login_result = Login(name, pwd); 

        // 把响应写入  包括错误码、错误消息、返回值
        fixbug::ResultCode *code = response->mutable_result();
        code->set_errcode(0);
        code->set_errmsg("");
        response->set_sucess(login_result);

        // 执行回调操作   执行响应对象数据的序列化和网络发送（都是由框架来完成的）
        done->Run();
    }
}
```



​	而在UserServiceRpc_Stub类中，Login和GetFriendLists这两个方法，都是调用了CallMethod方法。而UserServiceRpc_Stub的构造需要传入一个`RpcChannel`，而这个类是个抽象类，有个纯虚函数CallMethod。**这就意味着，我们要自己实现一个类继承RpcChannel，并重写其CallMethod方法**。在这个方法里，我们就比如可以进行序列化、远程调用请求等。

<img src="E:\MarkDown\picture\image-20230608212724093.png" alt="image-20230608212724093" style="zoom:67%;" />

## 服务存储格式

* RpcProvider服务发布类中通过std::unordered_map<std::string, ServiceInfo>存储服务名称及其对应的ServiceInfo
* 其中ServiceInfo类，保存了一个 Service* 指向服务对象的指针和 unordered_map<string, methoddescriptor*>服务方法的映射表；

## 环境配置

bin：可执行文件
build：项目编译文件
lib：项目库文件
src：源文件
test：测试代码
example：框架代码使用范例
CMakeLists.txt：顶层的cmake文件
README.md：项目自述文件
autobuild.sh：一键编译脚本

## 框架编写

`callee`是rpc服务的**提供者**，将本地服务发布在rpc

`caller`为**调用者**，使用这个rpc服务

#### 框架

**mprpcapplication.h**  `MprpcApplication`

* MprpcApplication，mprpc框架的基础类，负责框架的一些初始化操作。
* 读取main函数中传入的参数、通过MprpcConfig类加载配置文件等
* 这个类为单例模式的懒汉模式，即在需要的时候才通过函数创建一个实例

**mprpcconfig.h**

* MprpcConfig类，负责解析加载配置文件（服务器启动需要传入一些参数）设置这些参数。
* 因为要往main函数里加载配置文件，我们就直接创建一个类来负责这个工作。（对于我们这个项目的config文件放到了bin目录下，定义了rpc结点的ip地址和端口号）



**rpcprovider.h**  `RpcProvider`

* 框架提供的专门发布rpc服务的网络对象类，RpcProvider。

* 使用的时候需要创建一个RpcProvider类的rpc网络服务对象，通过调用`NotifyService`函数把UserService对象（提供服务的对象）发布到rpc节点上；通过调用`Run`函数启动一个rpc服务发布节点，Run以后，进程进入阻塞状态，等待远程的rpc调用请求。

* 其中RpcProvider中还集成了muduo网络库的eventloop、tcpserver等、实现了新的socket连接回调、已建立连接用户的读写事件回调

* 内部有个ServiceInfo类，保存了一个 Service* 服务对象和 unordered_map<string, methoddescriptor*>方法的映射表；

* 还有个unordered_map<string, ServiceInfo>服务对象和其服务方法的映射表

* NotifyService(Service*)

  * 供用户在rpc节点上发布rpc方法的一个函数接口，其功能是将服务对象和方法存到map中
  * 传入参数是基类Service指针(protobuf的方法类都是继承自service类)
  * 通过service->GetDescriptor()获取了服务对象的描述信息，进而获取服务的名字及服务对象service的方法的数量
  * 再依次获取每个服务方法的描述、将方法和服务都存到map中

* run函数

  * 通过MprpcApplication读取配置文件中的ip和port（就是rpc服务提供端的ip和port）
  * 创建TcpServer对象
  * 绑定连接回调和消息读写回调方法，即后面的onConnection和onMessage()
  * 通过ZkClient（封装的zk类），把当前rpc节点上要发布的服务全部注册到zk上面，让rpc client可以从zk上发现服务
    * zk.run()连接zkserver
    * 创建节点，存储ip和端口号
  * 启动muduo的网络服务（启动rpc服务节点，开始提供rpc远程网络调用服务）

* onConnection：新的socket连接回调

  * 如果和rpc client的连接断开了，就关闭文件描述符

* `onMessage`

  * 已建立连接用户的读写事件回调，通过muduo网络库的setMessageCallback接口绑定。如果远程有一个rpc服务的调用请求，那么OnMessage方法就会响应，**调用相应的服务对象的函数**

  * rpcheader.proto：
        在框架内部，RpcProvider和RpcConsumer需要协商好通信用的protobuf数据类型，比如我们需要区分 service_name method_name args。

    ​	所以需要定义proto的message类型，进行数据头（service_name和method_name）的序列化和反序列化，需要保存service_name、method_name和args_size（args_size是为了避免出现粘包问题）

    

  * 通过muduo提供的函数，读取远程rpc调用请求的字符流

    字符流格式：header_size(4个字节，用二进制，存取就比较方便) + header_str(包括service_name和method_name和args_size) + args_str

  * 读取字符流的前四个字节作为header_size，截取后面header_size个就是header_str，这就是我们需要反序列化的部分，反序列化并调用proto提供的函数，得到service_name、method_name和args_size。

  * 字符流最后剩下的部分就是args_str，对其反序列化得到我们的request

  

  * 有了service_name和method_name后，就能通过map得到service对象和method对象，进而生成rpc方法调用的请求request和响应response
  * 通过protobuf提供的NewCallback函数，绑定Closure的回调函数SendRpcResponse，返回一个Closure*
  * 调用service的CallMethod方法，传入method、request、response和Closure*，这就实现了根据远端rpc请求，调用当前rpc节点上发布的相应的方法。其中Closure是个抽象类，定义了一个纯虚函数run。


* SendRpcResponse
  * Closure的回调操作，用于序列化rpc的响应和令网络发送这个响应





接受一个rpc调用请求时，怎么知道要调用应用程序的哪个服务对象的哪个rpc方法？

![image-20230612233844495](E:\MarkDown\picture\image-20230612233844495.png)

​	比如对于一个UserService类，是我们本地的一个方法，要想发布成rpc，因此要继承UserServiceRpc这个通过proto文件生成的类。我们本地的UserService类去重写这些方法，比如对于Login方法，先从LoginRequest获取参数的值、再执行本地服务login并获取返回值，再将返回值写到LoginResponse，最后执行一个回调，将LoginResponse发送给rpc。



**由框架识别rpc请求，然后调用响应的哪个服务对象的哪个方法，比如UserServiceRpc对象的Login方法。**我们的框架接受rpc请求后，应该来自己判断调用啥方法。

​	这时，我们就需要生成一张表，记录服务对象和其发布的所有的服务方法。比如UserService中有Login和Register方法呀这种。而protobuf中，刚好提供了Service类来描述对象，Method类来描述方法，方便我们建这个表。

#### 框架提供给调用方的：

**mprpcchannel.h**

* 目的是定义一个MprpcChannel类来继承protobuf中的RpcChannel类，这样就能重载其CallMethod方法，所有通过stub代理对象调用的rpc方法，都会调用CallMethod，因此在这里统一做rpc方法调用的数据序列化和网络发送

* CallMethod
  * 序列化：获取service_name、method_name、args，并将其组织为header_size(4个字节) + header_str(包括service_name和method_name) + args_str 的格式
  * 使用tcp编程，send()发送rpc请求的字符串
    * 其中这里的ip和端口号，是通过zk获得的，即先创建ZkClient对象，调用其start方法连接到zkserver上，然后根据我们的service_name找到对应节点，节点的数据中就保存着ip地址和端口号
  * recv()接受rpc请求的响应值
  * 将响应反序列化为response

![image-20230616151504194](E:\MarkDown\picture\image-20230616151504194.png)

#### channel



需要重写callmethod方法，

​	此时存在一个问题，就是我们的caller调用方通过channel初始化stub对象、调用rpc方法，然后就读取response，这里默认了rpc请求一定是成功的，直接访问了response。我们的rpc方法调用都是转到了channel的callmethod里面，而序列化的时候、muduo传输错误的时候、反序列化失败的时候，reponse可能不存在或不正确，因此我们需要用RpcController存储控制信息。这个RpcController是我们重写的方法里面的形参，里面可以存失败与否以及一些错误信息。

​	因此，我们写了 mprpccontroller.h，继承RpcController类，重写其是否失败、设置错误信息、返回错误信息等几个。

​	然后在数据有问题时，就往这个对象里写入这些错误信息。在调用callmethod的时候把Controller传进去

## 应用示例

### 例一

要使用框架，我们需要编写proto、calle和caller对应的文件

比如我们要添加一个注册方法：

1. 在user.proto定义message类型的request和response，以及service类型的方法的描述
2. protoc user.proto --cpp_out=./ 重新生成proto文件对应的c++源码，此时在生成的UserServiceRpc类中就多了一个我们的注册方法
3. 此时我们要在 **callee** 中的继承类UserService中实现这个方法，实现我们需要的这个注册功能；并重写返回类型为void的这个方法：调用实现的这个方法，填写好response，并执行一个回调run()：将response序列化并通过网络发送

然后我们要使用这个方法：

1. 在 **caller** 中定义一个request，然后发起rpc调用请求，调用这个方法并等待返回结果

### 例二

> example目录下的friend

在举个例子，比如添加一个好友模块

1. 定义一个friend.proto
2. 在 `callee` 中写个friendservice.cc文件，定义一个类写本地方法
3. 令这个类继承proto生成的rpc类，并重写基类方法：取出request中的数据(通过proto提供的函数)、在这个基类方法中调用本地方法、写入到response中(通过proto提供的函数)、调用done->run();
4. 在main函数中，就调用框架的初始化操作、创建rpcprovider这个rpc网络服务对象，通过notify函数将我们这个friendservice类对象发布到rpc节点中、启动run()这个rpc服务发布节点，就开始等待远程的rpc调用请求了
5. 在 callee 中写好cmakelists.txt文件，里面就是set添加friendservice.cc文件、add_executable生成执行文件、target_link_libraries连接上mprpc和protobuf的库 这三条语句
6. 在 `caller` 中写个callfriendservice.cc调用文件
7. 在main函数中，就调用框架的初始化操作、通过channel初始化xxxrpc_stub对象、写好request请求、通过stub对象发起rpc方法的调用(就stub.xxx(xx)调用这个函数)、读取response中的结果
8. 同样也要写好cmakelists.txt文件

测试：

通过cmake编译一下（用那个按钮）。最后生成的是一个静态库，好处就是把muduo库的东西都放进去了，不用到时候再配置muduo的环境

./provider -i test.conf

./consumer -i test.conf

## ==zookeeper==

> cd /usr/local/zookeeper
>
> zk的默认端口号：2181
>
> conf: zk的配置文件
>
> bin: 客户端和服务器的代码程序

​	ZooKeeper 是一个分布式的，开放源码的分布式应用程序协同服务。 	
​	Zookeeper 提供了很多功能，在分布式中应用广泛，可以作为**服务注册中心**、全局分布式锁等。在我们项目里主要是做一个服务的动态注册和发现。所有提供rpc服务节点都注册在上面，然后根据要调用的名字查对应的ip和端口号，然后在对应主机上进行调用

### 为什么不选择 Redis 作为注册中心？

zookeeper临时节点自动宕机自动清除；

### 为什么要用zk

> 分布式系统的问题

* **服务的动态注册和发现**
  	如果客户端保存的服务提供者的列表是静态的，比如写死在配置文件中，如果服务的提供者发生了变化，比如有些机器下线了，或者又新增了实例，客户端根本不知道，想要得到最新的服务提供者的URL列表，必须手工更新配置文件，很不方便。
  
  * 问题: **客户端和服务提供者的紧耦合**
  
    <img src="E:\MarkDown\picture\image-20230617185138870.png" alt="image-20230617185138870" style="zoom: 37%;" />
  
  * 解决方案: 解除耦合，增加一个中间层——**注册中心**
      注册中心保存了能提供的服务的**名称**，以及**URL**(在我们项目里就是ip地址和端口号)。首先这些服务会在注册中心进行注册，当客户端来查询的时候，只需要给出名称，注册中心就会给出一个URL。所有的客户端在访问服务前，都需要向这个**注册中心**进行询问，以获得最新的地址。
    <img src="E:\MarkDown\picture\image-20230617185210952.png" alt="image-20230617185210952" style="zoom:40%;" />
    
  * 注册中心可以是树形结构，每个服务下面有若干节点，每个节点表示服务的实例。
    <img src="E:\MarkDown\picture\image-20230617185254413.png" alt="image-20230617185254413" style="zoom:50%;" />
  
  * 注册中心和各个服务实例直接建立Session，要求实例们定期发送心跳，一旦特定时间收不到心跳，则认为实例挂了，删除该实例。



> <img src="E:\MarkDown\picture\image-20230617185648978.png" alt="image-20230617185648978" style="zoom:80%;" />



### znode节点存储格式

> zk维护了一个文件系统，其中每个节点都称作znode，可以携带最多 `1M` 的数据，每个节点还可以创建孩子节点，数据模型是一棵树，由斜杠（/）分割的路径名唯一标识
>
> 我们可以将rpc服务的服务名及ip地址和端口号写在一个节点上

/就是根目录，node_1的地址是/node_1，node_1_1的地址是/node_1/node_1_1

<img src="E:\MarkDown\picture\image-20230616194030636.png" alt="image-20230616194030636" style="zoom:50%;" />

格式 ：

/service/method

> 注意，ZooKeeper 适用于存储和协同相关的关键数据，不适合用于大数据量存储。因为zk需要把所有的数据（它的 data tree）加载到内存中。这就决定了ZooKeeper 存储的数据量受内存的限制。一般的数据库系统例如 MySQL（使用 InnoDB 存储引擎的话）可以存储大于内存的数据，这是因为 InnoDB 是基于 B-Tree 的存储引擎。

### zk客户端常用命令

cd /usr/local/zookeeper/bin

启动zk服务：cd到其bin目录下，./zkServer.sh start

停止ZK服务：./zkServer.sh stop

启动zk客户端：./zkCli.sh

在客户端内就要使用zk提供的命令

ls: 和linux一样，如`ls /`命令查看根目录的节点，这里现在就zookeeper一个节点

get: 查询节点数据，比如`get /zookeeper`

create: 创建节点，如`create /s1 20`，创建了个s1节点，并保存20这个数据

set: 修改节点的值

delete: 删除节点



### cookie和session

#### **cookie**

​	cookie 的出现是因为 HTTP 是无状态的一种协议，换句话说，服务器记不住你，可能你每刷新一次网页，就要重新输入一次账号密码进行登录。而cookie 的作用就好比服务器给你贴个标签，然后你每次向服务器再发请求时，服务器就能够 cookie 认出你。

**工作原理**
（1）浏览器端第一次发送请求到服务器端
（2）服务器端创建Cookie，该Cookie中包含用户的信息，然后将该Cookie发送到浏览器端
（3）浏览器端再次访问服务器端时会携带服务器端创建的Cookie
（4）服务器端通过Cookie中携带的数据区分不同的用户
<img src="E:\MarkDown\picture\image-20230617163837103.png" alt="image-20230617163837103" style="zoom:57%;" />

**问题**

​	现在的很多网站功能很复杂，而且涉及很多的数据交互，比如说电商网站的购物车功能，信息量大，而且结构也比较复杂，无法通过简单的 cookie 机制传递这么多信息，而且要知道 cookie 字段是存储在 **HTTP header** 中的，就算能够承载这些信息，也会消耗很多的带宽，比较消耗网络资源。





#### **session(会话)**

​	session是服务器为了保存用户状态而创建的一个特殊的对象。简而言之，session就是一个对象，用于存储信息。 

​	session 可以配合 cookie 解决cookie不能存放大量数据的问题。比如说一个 cookie 存储这样一个变量 `sessionID=xxxx`，仅仅把这一个 cookie 传给服务器，然后服务器通过这个 ID 找到对应的 session，这个 session 是一个数据结构，里面存储着该用户的购物车等详细信息，服务器可以通过这些信息返回该用户的定制化网页，有效解决了追踪用户的问题。

​	session类似于一个Map，里面可以存放多个键值对，是以key-value进行存放的。key必须是一个字符串，value是一个对象。

<img src="E:\MarkDown\picture\image-20230617165026271.png" alt="image-20230617165026271" style="zoom:57%;" />

​	由于 session 存储在服务器中，肯定会消耗服务器的资源，所以 session 一般都会有一个过期时间，服务器一般会定期检查并删除过期的 session，如果后来该用户再次访问服务器，可能就会面临重新登录等等措施，然后服务器新建一个 session，将 session ID 通过 cookie 的形式传送给客户端。

**工作原理**

* 浏览器端第一次发送请求到服务器端，服务器端创建一个Session，同时会创建一个特殊的Cookie（name为JSESSIONID的固定值，value为session对象的ID），然后将该Cookie发送至浏览器端

* 浏览器端发送第N（N>1）次请求到服务器端,浏览器端访问服务器端时就会携带该name为JSESSIONID的Cookie对象

* 服务器端根据name为JSESSIONID的Cookie的value(sessionId)，去查询Session对象，从而区分不同用户。

  * name为JSESSIONID的Cookie不存在（关闭或更换浏览器），返回1中重新去创建Session与特殊的Cookie

  * name为JSESSIONID的Cookie存在，根据value中的SessionId去寻找session对象

  * value为SessionId不存在**（Session对象默认存活30分钟）**，返回1中重新去创建Session与特殊的Cookie

  * value为SessionId存在，返回session对象

<img src="E:\MarkDown\picture\image-20230617164011246.png" alt="image-20230617164011246" style="zoom:57%;" />

<img src="E:\MarkDown\picture\image-20230617164033928.png" alt="image-20230617164033928" style="zoom:50%;" />



#### 区别

* cookie和session都是用来跟踪浏览器用户身份的会话方式。
* cookie数据保存在客户端，session数据保存在服务端。

​	对于session，当登陆一个网站的时候，如果web服务器端使用的是session，那么所有的数据都保存在服务器上，客户端每次请求服务器的时候会发送当前会话sessionid，服务器根据当前sessionid判断相应的用户数据标志，以确定用户是否登陆或具有某种权限。由于数据是存储在服务器上面，所以你不能伪造。

​	对于cookie，如果浏览器使用的是cookie，那么所有数据都保存在浏览器端，比如你登陆以后，服务器设置了cookie用户名，那么当你再次请求服务器的时候，浏览器会将用户名一块发送给服务器，这些变量有一定的特殊标记。服务器会解释为cookie变量，所以只要不关闭浏览器，那么cookie变量一直是有效的，所以能够保证长时间不掉线。

* cookie作用于他所表示的path中(url中要包含path)，范围较小。session代表客户端和服务器的一次会话过程，web页面跳转时也可以共享数据，范围是本次会话，客户端关闭也不会消失。会持续到我们设置的session生命周期结束(默认30min)
* 使用session需要cookie的配合。cookie用来携带JSESSIONID
* cookie存放的数据量较小，session可以存储更多的信息。
* cookie由于存放在客服端，相对于session更不安全
* 由于session是存放于服务器的，当有很多客户端访问时，肯定会产生大量的session，这些session会对服务端的性能造成影响。





抽象地概括一下：**一个 cookie 可以认为是一个「变量」，形如 `name=value`，存储在浏览器；一个 session 可以理解为一种数据结构，多数情况是「映射」（键值对），存储在服务器上**。

注意，我说的是「一个」cookie 可以认为是一个变量，但是服务器可以一次设置多个 cookie，所以有时候说 cookie 是「一组」键值对儿，这也可以说得通。





### 会话（Session）

zk客户端与zk服务器之间的一个TCP长连接，通过这个长连接，客户端能够使用心跳检测与服务器保持有效的会话，也能向服务器发送请求并接收响应，还可接收服务器的Watcher事件通知。

Session的sessionTimeout，是会话超时时间，如果这段时间内，客户端未与服务器发生任何沟通（心跳或请求），服务器端会清除该session数据，客户端的TCP长连接将不可用，这种情况下，客户端需要重新实例化一个Zookeeper对象。

### 心跳

> 对于zk的心跳，通过conf 目录下的zoo.cfg进行修改
>
> ```c
> # 心跳检查的时间 2秒
> tickTime=2000
> ```
>
>
> 也可以在此设置ZK 服务器端的监听端口等参数

​	session中维持了一个心跳计数，比如每1s心跳计数就+1。注册中心要求上面的rpc节点每隔一段时间发送一个心跳消息，收到消息心跳计数就减1，如果超时（心跳计数超过阈值）就认为这个节点挂掉了。

​	zk client 会记录上一次发送数据的时间(lastSend)和上一次接收数据的时间(lastHeard)，zk client 给 server 发送心跳(ping)，这些心跳和其他命令一起发送给 zk server，如果 zk client 发现好长的时间没有接收到数据，认为超时，则断开与 server 的连接，并重连服务器。

* 如果是永久性节点，rpc节点超时未发送心跳，zk不会删除这个节点
* 如果是临时性节点，zk会自动删除

我们这里使用的是临时性节点，若节点挂掉，就不用这个信息了。否则请求会出错



### zk的watcher机制

> 观察者模式

​	通过zk提供的API，可以添加一个 watcher 监听某个节点的变化。在客户端维护了一个map表，Key是节点的名字，value就是节点携带的内容。

​	对一个节点添加watcher后，就会对其子节点进行监听。比如这个节点多了个子节点什么的，都由watcher主动的告诉客户端，调用其回调函数。是个事件回调机制

> 项目中的watcher

​	在ZkClient类的Start函数中绑定了个watcher，然后堵塞在信号量上。watcher函数进行ZkClient与zkserver是否相连的判断，相连后解除堵塞，完成与zk的连接

### ==zk的分布式锁==

Zookeeper实现分布式锁 https://zhuanlan.zhihu.com/p/363323742

### zookeeper集群节点宕机了怎么发现剔除的？

发现：watcher机制

剔除：临时节点

zookeeper心跳检测更新列表并利用watcher机制发给客户端

### Zookeeper的C编程接口

​	C接口的头文件在`/usr/local/include/zookeeper`中，需要包含这个`zookeeper.h`。在`/usr/local/lib`文件夹中有libzookeeper_mt.so和libzookeeper_st.so这俩多线程和单线程版本的库

原生ZkClient API存在的问题：
Zookeeper原生提供了C和Java的客户端编程接口，但是使用起来相对复杂，几个弱点：
1.不会自动发送心跳消息 <==== 这个后面发现是错误的，源码上会在1/3的Timeout时间发送ping心跳消息，是会自动发送心跳的
2.设置监听watcher只能是一次性的，每次触发后需要重复设置
3.znode节点只存储简单的byte字节数组，如果存储对象，需要自己转换对象生成字节数组  




### ZkClient编写

> 使用在rpcprovider.cc的run函数中

zookeeperutil

* global_watcher函数

  * 全局的一个回调函数，zkserver给zkclient的通知
  * 负责给信号量+1，唤醒主线程的wait

* ZkClient类，封装的zk客户端类，对zk提供的API进行封装

* Start()

  * zkclient启动连接zkserver。通过start函数连接到zkserver后就可以改动zk上的节点等操作

  * 先通过proto的api获取ip和端口号(zk的是2181)

  * 再通过zk的C的api `zookeeper_init` 初始化，即发起zk的连接

    * zookeeper_init中session的建立是异步的，直到收到ZOO_CONNECTED_STATE。而这个状态，我们在watcher回调里判定，如果是这个状态，表明session建立。则信号量+1，唤醒主线程

    * 咱链接库用的是zookeeper_mt多线程版本，这是因为zookeeper的API客户端程序提供了三个线程：

      API调用线程 

      网络I/O线程  pthread_create 底层poll （`zookeeper_init` 创建了子线程做网络IO）

      watcher回调线程 pthread_create（`zookeeper_init`的形参中传入了global_watcher这个回调函数，在独立的一个线程中运行

  * 将句柄绑定上一个信号量，然后wait堵塞，直到连接上zkserver，就会调用global_watcher来post信号量，就解除start()函数的堵塞

* Create(const char *path, const char *data, int datalen, int state=0);

  * 在zkserver上根据指定的path创建znode节点
  * path为znode节点的路径，data为节点的数据，datalen为数据长度，state为节点的状态，0为永久性节点。我们项目中用的是临时性节点，会自动被zkserver删除

* string GetData(const char *path);

  * 根据参数指定的znode节点路径，获取znode节点的值



ifconfig

## ==通信协议==

### TCP字节流

​	对于UDP，操作系统不会对消息进行拆分，每个UDP报文就是一个完整的用户消息，保存在队列里。

​	对于TCP，消息可能会被分组为多个报文，具体的划分取决于窗口大小、缓冲区大小等。因此TCP是面向字节流的协议，而不是面对报文的。

### 粘包

​	TCP通信过程中会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。所以需要对发送的数据包封装到一种通信协议里。

一般有三种分包的方式：

* 使用标准的应用层协议（比如：http、https）来封装要传输的不定长的数据包

* 固定长度的消息

  ​	这种是最简单方法，即每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。
  ​	但是这种方式灵活性不高，实际中很少用

* 特殊字符作为边界
      在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。比如 `HTTP`，请求方法和请求头都以回车和换行结尾，请求正文前也有一个回车和换行。
  <img src="E:\MarkDown\picture\image-20230618203541751.png" alt="image-20230618203541751" style="zoom:40%;" />
  
  * 如果刚好消息内容里有这个作为边界点的特殊字符，我们要对这个字符转义，避免被接收方当作消息的边界点而解析到无效的数据。
  * **效率低，需要一个字节一个字节接收，接收一个字节判断一次**，判断是不是那个特殊字符串
  
* **自定义消息结构**
      可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，有一个字段来说明紧随其后的数据有多大。这也是我们项目中使用的方法。

​	

​	在框架内部，RpcProvider 和 RpcConsumer 需要协商好通信用的protobuf数据类型，比如我们需要区分 service_name method_name args参数，要避免粘包问题。

​	所以需要定义proto的message类型，进行数据头（service_name和method_name）的序列化和反序列化，需要保存**service_name**、**method_name**和**args_size**

接收到的数据举例：

16UserServiceLoginzhang san123456  

header_size（4个字节，用二进制保存，存取就比较方便。这里header_size=16) + header_str（包括service_name和method_name，这里为UserServiceLogin) + args_str

> 我们自定义的数据结构是这样的：
> 	开头是固定4字节大小的 header_size，得到 header_size 后就知道了头部字段的大小。截取头部字段反序列化就得到我们的 service_name、method_name 和 args_size。有了args_size后，就能进行拆包，读取args_size长度就得到我们的args参数，对其进行反序列化得到我们的参数。
>
> 其中args_size是为了避免出现粘包问题。
>
> 接受到的数据前面四字节是header_size，这个数据右移4字节得到头部，再右移args_size，剩下的部分就是args_str了

## 日志

> 亮点：线程安全的queue，并进行了一点优化、宏

> ​	开源的中间件的话，比如kafka，是一种分布式的，基于发布 / 订阅的消息系统，一个分布式的消息队列，在分布式环境中提供一个异步的日志写入。

​	磁盘I/O操作是很慢的。因此通常是工作线程把日志写在缓冲区队列里。然后专门有一个线程负责磁盘IO操作，即从缓冲区队列中取出日志信息，写入到磁盘中。这样就不会把磁盘IO的消耗计入到框架的业务处理过程中。

![image-20230625235547890](E:\MarkDown\picture\image-20230625235547890.png)



**线程安全**

lockqueue.h

* 定义了一个LockQueue类，作为异步写日志的日志队列，提供push和pop功能
* 基于queue、条件变量、互斥锁。其中要注意多线程写入的线程安全，即任务线程和日志线程都会改变这个缓冲区队列，这里用到了锁和条件变量。一个优化点是当队列为空时，日志线程就不要再去抢这个锁了，因为没有意义，且会影响工作线程获取锁并写入的过程，会拖慢rpc业务的处理，具体操作是在pop时，判断任务队列是否为空，为空的话就wait。

```cpp
void Push(const T &data)
{
    std::lock_guard<std::mutex> lock(m_mutex);
    m_queue.push(data);
    m_condvariable.notify_one();
}

// 一个线程读日志queue，写日志文件
T Pop()
{
    std::unique_lock<std::mutex> lock(m_mutex);
    // while防止线程的虚假唤醒
    while (m_queue.empty())
    {
        // 日志队列为空，线程进入wait状态
        m_condvariable.wait(lock);
    }

    T data = m_queue.front();
    m_queue.pop();
    return data;
}
```

**Logger类**

logger.h

* 定义了一个单例模式的Logger类。这里使用的懒汉模式，需要的时候调用GetInstance函数才创建一个static对象实例

* 提供获取日志的单例、设置日志级别、写日志的功能

* 在Logger的构造函数中，启动写日志线程，日志线程循环执行：
  * 获取当前的日期：通过time函数得到当前日历时间（time_t），即从1970-01-01 00:00:00到现在的秒数。然后再通过localtime函数将其转换为tm的结构，就能取出当前时间的年月日等信息。
  
  * fopen创建log文件，获取队列中的信息并fputs将内容追加到文件中
  
    ```cpp
    FILE *pf = fopen(file_name, "a+"); // a+: 打开一个用于读取和追加的文件
    // const char *c_str(); // 将string转换为char类型
    fputs(msg.c_str(), pf); // 把字符串写入到指定的流 stream 中
    fclose(pf);
    ```
  
* SetLogLevel函数设置日志级别

  ```c++
  // 对于日志的级别，这里比较简单，就设置了普通信息和错误信息
  enum LogLevel {
      INFO,  // 打印一些重要的流程信息
      ERROR,
  };
  ```

* Log函数写日志， 把日志信息写入lockqueue缓冲区当中

**宏**

> 定义宏来完成写入INFO日志和ERROR日志的功能，在宏中进行log实例的获取、设置日志级别、写日志的操作

```cpp
// 定义宏 LOG_INFO("xxx %d %s", 20, "xxxx")
#define LOG_INFO(logmsgformat, ...) \
    // 为了避免出问题，一般采用do while的形式
    do \
    {  \
        Logger &logger = Logger::GetInstance(); \
        logger.SetLogLevel(INFO); \
        char c[1024] = {0}; \
        // ##__VA_ARGS__是可变参列表
        // 设将可变参数(...)按照 format 格式化成字符串，并将字符串复制到 str 中，size 为要写入的字符的最大数目，超过 size 会被截断，最多写入 size-1 个字符
        snprintf(c, 1024, logmsgformat, ##__VA_ARGS__); \
        logger.Log(c); \
    } while(0) \
```



使用：在NotifyService发布rpc方法的接口中，通过log记录了我们发布的服务名和方法名

## 编译脚本

先chmod 777 autobuild.sh给权限，然后执行./autobuild.sh

最后在lib目录下，存放libmprpc.a这个静态库和include中包含的头文件

autobuild.sh

```sh
#!/bin/bash
# 第一行的内容指定了shell脚本解释器的路径，而且这个指定路径只能放在文件的第一行。第一行写错或者不写时，系统会有一个默认的解释器进行解释。

set -e

# 删除build目录下所有文件
# 其中r表示将目录下所有文件依次删除，f表示直接删除无需逐一确认
# 其中pwd命令用于显示工作目录。
rm -rf `pwd`/build/*
# 加载到build目录中cmake和make
cd `pwd`/build &&
	cmake .. &&
	make
cd ..
# 将头文件拷贝到lib文件夹下（如果之后用库的话，只需要保留lib文件夹的头文件）
cp -r `pwd`/src/include `pwd`/lib
```

## 演示

> 启动zk的时候有时候会虚假启动，运行示例程序的时候就连不上zk服务器。先是改了端口测试不行，然后以为是服务器的安全组问题，结果也不行。
>
> 然后发现zk客户端也连不上，试了试zkCli.sh -server 127.0.0.1:2181也不行
>
> 之后用./zkServer.sh status看看zk是不是真的启动了。如果没有启动的话，看看cd /usr/local/zookeeper/bin/zookeeper.out这个文件里报的什么错，发现是java的问题。但java安装了，且环境变量也没有问题。
>
> 最后解决方法是启动zk前先执行：source /etc/profile，即通过source命令刷新环境变量

cd /usr/local/zookeeper/bin

启动zk服务：cd到其bin目录下，./zkServer.sh start

cd /root/linux/myrpc/bin

启动zk服务提供者

./provider -i test.conf

启动zk服务的使用者

./consumer -i test.conf

停止ZK服务：./zkServer.sh stop

